{"cells":[{"cell_type":"markdown","id":"099a9523","metadata":{"id":"099a9523"},"source":["# Multiclass classification\n","\n","When solving a classification problem, many existing machine learning models allow only two classes to be separated, usually referred to as \"positive\" and \"negative\". Positive patterns are commonly those related to what is to be detected, such as disease, alarm, or a type of object in an image. Negative patterns are often characterised by the absence of this characteristic that positive patterns have. To develop an ANN that classifies into two classes, a single neuron is needed in the output layer, with a logarithmic (or similar) sigmoidal transfer function, such that the output of the ANN will be between 0 and 1, and can be interpreted as the ANN's certainty in classifying a pattern as \"positive\". The classification into \"negative\" or \"positive\" is done in a simple way, by applying a threshold which is typically 0.5, although this can be changed.\n","\n","However, there are many occasions when a system that is able to classify into more than two classes is desired. A simple example is a system that wants to classify an image according to whether a dog, cat or mouse, or some other type of animal is observed. In this case, you want to develop a 4-class classification system: \"dog\"/\"cat\"/\"mouse\"/\"other\". If an ANN to distinguish between these 3 animals is required, an output neuron for each class is needed, including the \"other\" class (4 output neurons in total).\n","\n","In the multiclass classification scheme, as has been done in previous assignments, an encoding called one-hot-encoding is generally used, which is based on obtaining a boolean value for each pattern and each class, in such a way that each boolean value will be equal to 1 if that pattern belongs to that class, and 0 otherwise. When training an ANN with this scheme, each output neuron can be understood as a model specialised in classifying in a given class. In this type of networks, a linear transfer function is usually used in the output layer, whereby negative outputs indicate that a neuron does not classify the pattern into that class (i.e. from the point of view of that class it classifies it as \"negative\"), and positive outputs indicate that a neuron classifies the pattern as that class (i.e. from the point of view of that class it classifies it as \"positive\"). The absolute value of a neuron's output indicates that neuron's confidence in the classification. Finally, the softmax function receives these classification values and transforms them in such a way that they are between 0 and 1, and add up to 1, interpreted as the probability of belonging to each class. The pattern will be classified into the class whose output value is the highest. The softmax function is defined as follows:\n","\n","$$\n","softmax(y^i) = \\frac{e^{y^i}}{\\sum_j{e^{y^j}}}\n","$$\n","\n","where $y^i$ is the output of the $i$-th neuron. For example, in a 3-class classification problem, if the outputs from the 3 neurons are `[2, 1, 0.2]`, they would classify the inputs as belonging to their respective classes, although the first one with much greater certainty. After applying the softmax function, the respective probabilities will be `[0.65, 0.24, 0.11]`, so the pattern will be classified as the first class.\n","\n","In this way, the softmax function converts the real values produced by the output neurons into probability values, so that the more negative a value is (the more certainty of not belonging to that class), the closer it is to 0, and the more positive a value is (the more certainty of belonging to that class), the closer it is to 1. As indicated above, the sum of output probabilities will be equal to 1. Because of this fact, a fourth special class \"other\" is needed in the example above and in any other example where a pattern may not belong to any of the predefined classes.\n","\n","### Question 4.2\n","\n","> ❓ Why is this extra class necessary when using the softmax function?"]},{"cell_type":"markdown","id":"ad46fa6d","metadata":{"id":"ad46fa6d"},"source":["When using the softmax function, cause we are converting the values produced by the neurons into probabilities, acording to basic probability theory, we have to have a class for the pattern not belonging to any of the class defined, so the probability of any of the pattern belonging to any of the possible class is 1. In conclussion, for it to be coherent with the probability theory.\n","\n","Tip: write in Julia `softmax([-1, -1, -0.2])`, and interpret the inputs (what does the vector `[-1, -1, -0.2]` represent and how is it interpreted?) and outputs of the function (how much do the values add up to? what does each say?). To use this function, import it from the Flux library."]},{"cell_type":"code","source":["import Pkg; Pkg.add(\"Flux\");"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5yzqgjKyHp-_","executionInfo":{"status":"ok","timestamp":1759948991356,"user_tz":-120,"elapsed":241397,"user":{"displayName":"José Manuel Bueno","userId":"13231755112106791400"}},"outputId":"b0a8d228-4a9c-4198-9ded-423513062e2e"},"id":"5yzqgjKyHp-_","execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General.toml`\n","\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m PrettyPrint ───────── v0.2.0\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m RealDot ───────────── v0.1.0\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ZygoteRules ───────── v0.2.7\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ContextVariablesX ─── v0.1.3\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ShowCases ─────────── v0.1.0\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Accessors ─────────── v0.1.42\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m InitialValues ─────── v0.3.1\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m MLUtils ───────────── v0.4.8\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DefineSingletons ──── v0.1.2\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m FLoopsBase ────────── v0.1.1\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m MLCore ────────────── v1.0.0\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ProgressLogging ───── v0.1.5\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m MicroCollections ──── v0.2.0\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m IRTools ───────────── v0.4.15\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m OneHotArrays ──────── v0.2.10\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Zygote ────────────── v0.7.10\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m CompositionsBase ──── v0.1.2\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m JuliaVariables ────── v0.2.4\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m NameResolution ────── v0.1.5\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m BangBang ──────────── v0.4.4\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m SparseInverseSubset ─ v0.1.2\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m FLoops ────────────── v0.2.2\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ChainRules ────────── v1.72.6\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m MLStyle ───────────── v0.4.17\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Flux ──────────────── v0.16.5\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m SplittablesBase ───── v0.1.15\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Transducers ───────── v0.4.85\n","\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Baselet ───────────── v0.1.1\n","\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.11/Project.toml`\n","  \u001b[90m[587475ba] \u001b[39m\u001b[92m+ Flux v0.16.5\u001b[39m\n","\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.11/Manifest.toml`\n","  \u001b[90m[7d9f7c33] \u001b[39m\u001b[92m+ Accessors v0.1.42\u001b[39m\n","  \u001b[90m[198e06fe] \u001b[39m\u001b[92m+ BangBang v0.4.4\u001b[39m\n","  \u001b[90m[9718e550] \u001b[39m\u001b[92m+ Baselet v0.1.1\u001b[39m\n","  \u001b[90m[082447d4] \u001b[39m\u001b[92m+ ChainRules v1.72.6\u001b[39m\n","  \u001b[90m[a33af91c] \u001b[39m\u001b[92m+ CompositionsBase v0.1.2\u001b[39m\n","  \u001b[90m[6add18c4] \u001b[39m\u001b[92m+ ContextVariablesX v0.1.3\u001b[39m\n","  \u001b[90m[244e2a9f] \u001b[39m\u001b[92m+ DefineSingletons v0.1.2\u001b[39m\n","  \u001b[90m[cc61a311] \u001b[39m\u001b[92m+ FLoops v0.2.2\u001b[39m\n","  \u001b[90m[b9860ae5] \u001b[39m\u001b[92m+ FLoopsBase v0.1.1\u001b[39m\n","  \u001b[90m[587475ba] \u001b[39m\u001b[92m+ Flux v0.16.5\u001b[39m\n","  \u001b[90m[7869d1d1] \u001b[39m\u001b[92m+ IRTools v0.4.15\u001b[39m\n","  \u001b[90m[22cec73e] \u001b[39m\u001b[92m+ InitialValues v0.3.1\u001b[39m\n","  \u001b[90m[b14d175d] \u001b[39m\u001b[92m+ JuliaVariables v0.2.4\u001b[39m\n","  \u001b[90m[c2834f40] \u001b[39m\u001b[92m+ MLCore v1.0.0\u001b[39m\n","  \u001b[90m[d8e11817] \u001b[39m\u001b[92m+ MLStyle v0.4.17\u001b[39m\n","  \u001b[90m[f1d291b0] \u001b[39m\u001b[92m+ MLUtils v0.4.8\u001b[39m\n","  \u001b[90m[128add7d] \u001b[39m\u001b[92m+ MicroCollections v0.2.0\u001b[39m\n","  \u001b[90m[71a1bf82] \u001b[39m\u001b[92m+ NameResolution v0.1.5\u001b[39m\n","  \u001b[90m[0b1bfda6] \u001b[39m\u001b[92m+ OneHotArrays v0.2.10\u001b[39m\n","  \u001b[90m[8162dcfd] \u001b[39m\u001b[92m+ PrettyPrint v0.2.0\u001b[39m\n","  \u001b[90m[33c8b6b6] \u001b[39m\u001b[92m+ ProgressLogging v0.1.5\u001b[39m\n","  \u001b[90m[c1ae055f] \u001b[39m\u001b[92m+ RealDot v0.1.0\u001b[39m\n","  \u001b[90m[605ecd9f] \u001b[39m\u001b[92m+ ShowCases v0.1.0\u001b[39m\n","  \u001b[90m[dc90abb0] \u001b[39m\u001b[92m+ SparseInverseSubset v0.1.2\u001b[39m\n","  \u001b[90m[171d559e] \u001b[39m\u001b[92m+ SplittablesBase v0.1.15\u001b[39m\n","  \u001b[90m[28d57a85] \u001b[39m\u001b[92m+ Transducers v0.4.85\u001b[39m\n","  \u001b[90m[e88e6eb3] \u001b[39m\u001b[92m+ Zygote v0.7.10\u001b[39m\n","  \u001b[90m[700de1a5] \u001b[39m\u001b[92m+ ZygoteRules v0.2.7\u001b[39m\n","\u001b[92m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n","   5382.5 ms\u001b[32m  ✓ \u001b[39m\u001b[90mInitialValues\u001b[39m\n","   4731.2 ms\u001b[32m  ✓ \u001b[39m\u001b[90mPrettyPrint\u001b[39m\n","   4725.6 ms\u001b[32m  ✓ \u001b[39m\u001b[90mShowCases\u001b[39m\n","    842.8 ms\u001b[32m  ✓ \u001b[39m\u001b[90mRealDot\u001b[39m\n","    977.2 ms\u001b[32m  ✓ \u001b[39m\u001b[90mCompositionsBase\u001b[39m\n","   1123.5 ms\u001b[32m  ✓ \u001b[39m\u001b[90mDefineSingletons\u001b[39m\n","   1323.2 ms\u001b[32m  ✓ \u001b[39m\u001b[90mProgressLogging\u001b[39m\n","   1740.8 ms\u001b[32m  ✓ \u001b[39m\u001b[90mSparseInverseSubset\u001b[39m\n","   3407.1 ms\u001b[32m  ✓ \u001b[39m\u001b[90mBaselet\u001b[39m\n","   2122.5 ms\u001b[32m  ✓ \u001b[39m\u001b[90mContextVariablesX\u001b[39m\n","   3046.0 ms\u001b[32m  ✓ \u001b[39m\u001b[90mSplittablesBase\u001b[39m\n","   6331.5 ms\u001b[32m  ✓ \u001b[39m\u001b[90mIRTools\u001b[39m\n","   2214.4 ms\u001b[32m  ✓ \u001b[39m\u001b[90mZygoteRules\u001b[39m\n","   2980.9 ms\u001b[32m  ✓ \u001b[39m\u001b[90mMLCore\u001b[39m\n","   1269.2 ms\u001b[32m  ✓ \u001b[39m\u001b[90mNameResolution\u001b[39m\n","   1370.5 ms\u001b[32m  ✓ \u001b[39m\u001b[90mCompositionsBase → CompositionsBaseInverseFunctionsExt\u001b[39m\n","   2909.7 ms\u001b[32m  ✓ \u001b[39m\u001b[90mOneHotArrays\u001b[39m\n","   1806.6 ms\u001b[32m  ✓ \u001b[39m\u001b[90mFLoopsBase\u001b[39m\n","   9429.8 ms\u001b[32m  ✓ \u001b[39m\u001b[90mAccessors\u001b[39m\n","   2343.7 ms\u001b[32m  ✓ \u001b[39m\u001b[90mMLDataDevices → MLDataDevicesOneHotArraysExt\u001b[39m\n","  19294.4 ms\u001b[32m  ✓ \u001b[39m\u001b[90mChainRules\u001b[39m\n","   1245.4 ms\u001b[32m  ✓ \u001b[39m\u001b[90mAccessors → StructArraysExt\u001b[39m\n","   1338.2 ms\u001b[32m  ✓ \u001b[39m\u001b[90mAccessors → LinearAlgebraExt\u001b[39m\n","   1692.5 ms\u001b[32m  ✓ \u001b[39m\u001b[90mAccessors → TestExt\u001b[39m\n","   1553.2 ms\u001b[32m  ✓ \u001b[39m\u001b[90mAccessors → IntervalSetsExt\u001b[39m\n","   1874.9 ms\u001b[32m  ✓ \u001b[39m\u001b[90mAccessors → UnitfulExt\u001b[39m\n","   2371.4 ms\u001b[32m  ✓ \u001b[39m\u001b[90mAccessors → StaticArraysExt\u001b[39m\n","  17099.7 ms\u001b[32m  ✓ \u001b[39mReactant → ReactantOneHotArraysExt\n","   2313.8 ms\u001b[32m  ✓ \u001b[39m\u001b[90mArrayInterface → ArrayInterfaceChainRulesExt\u001b[39m\n","   2359.5 ms\u001b[32m  ✓ \u001b[39m\u001b[90mMLDataDevices → MLDataDevicesChainRulesExt\u001b[39m\n","   2515.4 ms\u001b[32m  ✓ \u001b[39m\u001b[90mBangBang\u001b[39m\n","   2165.4 ms\u001b[32m  ✓ \u001b[39m\u001b[90mBangBang → BangBangStaticArraysExt\u001b[39m\n","   8257.1 ms\u001b[32m  ✓ \u001b[39m\u001b[90mBangBang → BangBangDataFramesExt\u001b[39m\n","   1540.5 ms\u001b[32m  ✓ \u001b[39m\u001b[90mBangBang → BangBangChainRulesCoreExt\u001b[39m\n","   1666.7 ms\u001b[32m  ✓ \u001b[39m\u001b[90mBangBang → BangBangStructArraysExt\u001b[39m\n","   1636.4 ms\u001b[32m  ✓ \u001b[39m\u001b[90mBangBang → BangBangTablesExt\u001b[39m\n","   2633.1 ms\u001b[32m  ✓ \u001b[39m\u001b[90mMicroCollections\u001b[39m\n","   8076.1 ms\u001b[32m  ✓ \u001b[39m\u001b[90mTransducers\u001b[39m\n","   7067.2 ms\u001b[32m  ✓ \u001b[39m\u001b[90mTransducers → TransducersDataFramesExt\u001b[39m\n","   2182.1 ms\u001b[32m  ✓ \u001b[39m\u001b[90mTransducers → TransducersAdaptExt\u001b[39m\n","  90263.9 ms\u001b[32m  ✓ \u001b[39m\u001b[90mMLStyle\u001b[39m\n","   5765.6 ms\u001b[32m  ✓ \u001b[39m\u001b[90mJuliaVariables\u001b[39m\n","   8154.0 ms\u001b[32m  ✓ \u001b[39m\u001b[90mFLoops\u001b[39m\n","  14263.8 ms\u001b[32m  ✓ \u001b[39m\u001b[90mMLUtils\u001b[39m\n","   5652.4 ms\u001b[32m  ✓ \u001b[39m\u001b[90mMLDataDevices → MLDataDevicesMLUtilsExt\u001b[39m\n","   7277.8 ms\u001b[32m  ✓ \u001b[39mLux → LuxMLUtilsExt\n"," 111362.4 ms\u001b[32m  ✓ \u001b[39m\u001b[90mZygote\u001b[39m\n","   5383.5 ms\u001b[32m  ✓ \u001b[39m\u001b[90mZygote → ZygoteColorsExt\u001b[39m\n","   5393.4 ms\u001b[32m  ✓ \u001b[39m\u001b[90mMLDataDevices → MLDataDevicesZygoteExt\u001b[39m\n","   8439.3 ms\u001b[32m  ✓ \u001b[39mLux → LuxZygoteExt\n","  14411.0 ms\u001b[32m  ✓ \u001b[39mReactant → ReactantZygoteExt\n","  15775.9 ms\u001b[32m  ✓ \u001b[39mFlux\n","    899.0 ms\u001b[32m  ✓ \u001b[39mFlux → FluxCUDAExt\n","   6179.2 ms\u001b[32m  ✓ \u001b[39mLux → LuxFluxExt\n","  23987.5 ms\u001b[32m  ✓ \u001b[39mFlux → FluxEnzymeExt\n","  55 dependencies successfully precompiled in 216 seconds. 487 already precompiled.\n"]}]},{"cell_type":"code","source":["using Flux\n","softmax([-1, -1, -0.2])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":315},"id":"z6ih9whpHfmt","executionInfo":{"status":"error","timestamp":1759948731632,"user_tz":-120,"elapsed":86,"user":{"displayName":"José Manuel Bueno","userId":"13231755112106791400"}},"outputId":"707e4407-8e29-4be6-ba17-b7b353d29b47"},"id":"z6ih9whpHfmt","execution_count":1,"outputs":[{"output_type":"error","ename":"LoadError","evalue":"ArgumentError: Package Flux not found in current path.\n- Run `import Pkg; Pkg.add(\"Flux\")` to install the Flux package.","traceback":["ArgumentError: Package Flux not found in current path.\n- Run `import Pkg; Pkg.add(\"Flux\")` to install the Flux package.","","Stacktrace:"," [1] \u001b[0m\u001b[1mmacro expansion\u001b[22m","\u001b[90m   @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:2296\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m"," [2] \u001b[0m\u001b[1mmacro expansion\u001b[22m","\u001b[90m   @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mlock.jl:273\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m"," [3] \u001b[0m\u001b[1m__require\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90minto\u001b[39m::\u001b[0mModule, \u001b[90mmod\u001b[39m::\u001b[0mSymbol\u001b[0m\u001b[1m)\u001b[22m","\u001b[90m   @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:2271\u001b[24m\u001b[39m"," [4] \u001b[0m\u001b[1m#invoke_in_world#3\u001b[22m","\u001b[90m   @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4messentials.jl:1089\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m"," [5] \u001b[0m\u001b[1minvoke_in_world\u001b[22m","\u001b[90m   @\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4messentials.jl:1086\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m"," [6] \u001b[0m\u001b[1mrequire\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90minto\u001b[39m::\u001b[0mModule, \u001b[90mmod\u001b[39m::\u001b[0mSymbol\u001b[0m\u001b[1m)\u001b[22m","\u001b[90m   @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:2260\u001b[24m\u001b[39m"]}]},{"cell_type":"markdown","id":"749072c5","metadata":{"id":"749072c5"},"source":["### Question 4.3\n","\n","> ❓ Might it not be necessary to create the additional class? What modification would have to be made to the ANN? How would the output be interpreted? How would the output class be generated based on the outputs of the output neurons?"]},{"cell_type":"markdown","id":"8fd54309","metadata":{"id":"8fd54309"},"source":["What we could do it's replacing softmax by an indivual activation per each output neuron (with linear functions, or sigmoidal functions, for example).\n","\n","Then, we could predict the class choosing the neuron with the highest activation over a threshold. It is true also that in that case we would loose, so to say, the probabilistic insight, not forcing the probabilities to sum one, providing that we don't add an additional class for the cases exposes at the text above.\n"]},{"cell_type":"markdown","id":"b7bb4d94","metadata":{"id":"b7bb4d94"},"source":["### Question 4.4\n","\n","> ❓ In general, how does the output of a model have to be in order not to need this fourth class?"]},{"cell_type":"markdown","id":"4147d60f","metadata":{"id":"4147d60f"},"source":["As we said, if we don't interpret the outputs as a probability distribution, there is no need of an additional class. As we stated, that can be the case if we apply an individual activation per output neuron and then compare the magnitudes, or compare then to a certain threshold."]},{"cell_type":"markdown","id":"cf440c48","metadata":{"id":"cf440c48"},"source":["### Question 4.5\n","\n","> ❓ Does a kNN model need this fourth class?"]},{"cell_type":"markdown","id":"dc3524cd","metadata":{"id":"dc3524cd"},"source":["In that case, we would not need an additional class, cause the kNN model compares the pattern with the labelled samples, and then choose the class with a biggest frequency.\n","\n","In case we add another class, it would be considered as another label, but it is not strictly necessary for the algorithm to be functional, cause there is no normalization applied, like softmax.\n"]},{"cell_type":"markdown","id":"e0ac9061","metadata":{"id":"e0ac9061"},"source":["### Question 4.6\n","\n","> ❓ How many classes would be necessary if an ANN wanted to recognise those 3 types of animals, and, if it is not one of them, to say whether it is an animal or not? What if the model is a kNN?"]},{"cell_type":"markdown","id":"89ab0b18","metadata":{"id":"89ab0b18"},"source":["\n","In that case (at least intuitively reasoning) we would need 5 classes, one for each of the three animals we want to recognise, a fourth one for getting and recognising animals, but which are not in between the first three, and the fifth for the elements which are suppossed to be recognised as not animals. Having that five classes and applying softmax we can keep the sum of probabilities as 1.\n","\n","In case of a kNN, the algorithm can only vote in between classes on the training set. That means that, if we really wanna do, not just these 3 animals recognition, but also say, on another case, if it is an animal or not, we have to keep these five classes previously mentioned. That is because we need labels for each of the cases proposed.\n"]},{"cell_type":"markdown","id":"3c98bc2c","metadata":{"id":"3c98bc2c"},"source":["Therefore, the \"positive\"/\"negative\" scheme no longer applies if more than two classes are required. The problem in these cases is that many of the machine learning models are only capable of separating two classes, so theoretically they could not be used. An example of such systems are Support Vector Machines (SVM), which are discussed in more detail in the theory class. Modifications have been made to the formulation of this model to allow multi-class classifications; however, in practice they are not commonly used, and instead a strategy that allows binary SVMs to be used to classify into multiple classes is often employed.\n","\n","There are two main strategies for converting multi-class problems into binary classification problems. These strategies are called \"one-against-one\" or \"one-against-all\". Both are explained in theory class, but since \"one-against-all\" is much more widely used, this strategy will be used in the following.\n","\n","The \"one-against-all\" strategy is based on generating L binary classifiers for a classification problem of L classes, one per class. In the l-th problem, class l must be separated from the rest, i.e., the patterns belonging to that class will be considered \"positive\", and those not belonging to it will be considered \"negative\". Continuing with the previous example of animals, 3 different classification problems would have to be solved: one to classify \"dog\"/\"not dog\", one to classify \"cat\"/\"not cat\", and one to classify \"mouse\"/\"not mouse\". Three classifiers would therefore be trained with the same inputs but with different desired outputs for each problem.\n","\n","### Question 4.7\n","\n","> ❓ In the previously described problem, 4 classes were used for these 3 animals, including the class \"other\". Why not train a classifier for this class in a \"one-against-all\" scheme?"]},{"cell_type":"markdown","id":"6b90226b","metadata":{"id":"6b90226b"},"source":["That would not have so much sense, cause the function of an additional class on a ANN with softmax, is recognizing or getting patterns which does not belong to any of the other three classes.\n","\n","But if we apply an One vs All squema, and we train a binary classifier for each class, training a binary classifier for a \"rest\" class would not give so much additional information. We think it is better to stablish a certain threshold below which we would consider the pattern as not belonging to any of the three considered classes."]},{"cell_type":"markdown","id":"72bc8ae7","metadata":{"id":"72bc8ae7"},"source":["Once the binary classifiers are trained, any given pattern is fed into all the classifiers and, depending on the output, a decision is made. If only one of the systems has positive output, or none of the three classifies it as positive, the decision is clear. However, sometimes more than one classifier will give a positive output for the same pattern. Fortunately, many classifiers give information about the level of certainty or confidence they have that the pattern is classified as \"positive\". If more than one binary model classifies the pattern as positive, it will be assigned to the class corresponding to the classifier that has a higher certainty in its classification."]},{"cell_type":"markdown","id":"48231191","metadata":{"id":"48231191"},"source":["### Question 4.8\n","\n","> ❓ Would it be possible to use the outputs of those 3 classifiers as the input of the softmax function? What would be the consequences?"]},{"cell_type":"markdown","id":"5275a1a7","metadata":{"id":"5275a1a7"},"source":["We could apply softmax, and then transform each of the ouput of each binary classifier into a probability so all of them would sum 1.\n","\n","It is true that, in that case, given that each classifier was trained applying that One vs All Squema, we would then loose this binary interpretation on each of the systems.\n","\n","For example, in the case that some of the classifiers conclude that the pattern does not belong to the class considered, we could get untruthful decission, so to say.\n","\n","Each output of each binary classifier should not be considered with the rest as a probability distribution, in conclussion."]},{"cell_type":"markdown","id":"d515fedd","metadata":{"id":"d515fedd"},"source":["### Question 4.9\n","\n","> ❓ In general, when there are L classes and a pattern may not belong to any of them, what is the impact of using the softmax function on the outputs? In which cases could it be used? Why?"]},{"cell_type":"markdown","id":"ca2e084f","metadata":{"id":"ca2e084f"},"source":["In case we have L classes, and a pattern does not belong to any of then, applying softmax would mean distributing all of the probability in between those L classes.\n","\n","That would imply that, for sure, a class would be selected among those L classes, for a pattern, even if it does not belong to any of those.\n","\n","We think we could just apply softmax on that case if we assume that each of the patterns received would strictly belong to any of the L classes. If not, it would be better to consider an additional class, or not apply softmax or just apply an activation function to each of the output neurons, maybe, with a certain threshold below which, as we said, the pattern could be rejected by any of the neurons."]},{"cell_type":"markdown","id":"47b305cf","metadata":{"id":"47b305cf"},"source":["### Question 4.10\n","\n","> ❓ The softmax function is useful to get a loss value to train the ANN. However, if it were not used in the animal example above, would the fourth class \"other\" be necessary?"]},{"cell_type":"markdown","id":"98db9733","metadata":{"id":"98db9733"},"source":["In case we apply softmax, and then a loss function like cross-entropy, then yes, the fourth class would be necessary, given that, as we stated before, that softmax distributes all of the probability in between the existing classes."]},{"cell_type":"markdown","id":"62d451a0","metadata":{"id":"62d451a0"},"source":["Finally, it is necessary to consider a different scenario when assigning patterns to classes. So far, and in most situations, the classes considered are mutually exclusive, i.e. in the example above, an animal is either a dog, a cat, a mouse, or none of the 3, but it cannot be of several classes at the same time. This is the most common case, but occasionally a problem will have classes that are not mutually exclusive. For example, when classifying animal sounds according to the animal that makes them, it may happen that several animals are mixed in one sound. In these cases, the use of a linear transfer function in the last layer together with the softmax function would not work, since, naturally, the sum of the probabilities of belonging to the classes may be greater than 1 (it may belong to several classes at the same time). For these cases, the scheme that can be used to train ANNs is to use logarithmic sigmoidal transfer functions in the last layer (instead of linear), which give an output between 0 and 1, and not to perform transformation using the softmax function. In this way, the final output of each output neuron is independent of the rest of the output neurons, and more than one can take values close to 1. The output of each neuron would again be interpreted as the probability of belonging to that class, but in this case the sum of the probabilities does not have to be 1 (they are independent). Not applying the softmax function has two advantages: the first, already mentioned, is that it allows classification into non-mutually exclusive classes; the second is that an additional class (\"other\" in the example above) is no longer needed for cases where a set of inputs may not belong to any of the given classes.\n","\n","### Question 4.11\n","\n","> ❓ Why is this extra class no longer needed?"]},{"cell_type":"markdown","id":"12e98cd7","metadata":{"id":"12e98cd7"},"source":["As we have on the paragraph before, if we replace softmax by sigmoidal transfer function at the last layer, each neuron would be like an independent detector for each class.\n","\n","As said, in case the ouput of each neuron would be close to 0, we could then assume than the noise (in this case) would not belong to any of the classes.\n","\n","On the contrary, many if the neurons could output values close to 1, assuming then the possibility of the pattern belonging to several classes at the same time, equivalently, assuming that the classes are not disjoint."]},{"cell_type":"markdown","id":"c9523cc3","metadata":{"id":"c9523cc3"},"source":["Given a set of inputs, as always, it is classified into the class whose output neuron has shown the highest confidence. This scheme of non-mutually exclusive outputs is similar to the \"one-against-all\" scheme, in which one classifier per class is trained in parallel. The classifiers are independent and the final class is that of the classifier that has the highest certainty of belonging to that class. If all classifiers return \"negative\" as a classification and there is no possibility of not belonging to any class, the classifier with the lowest certainty of being negative is classified in the corresponding class. If all classifiers return \"negative\" as a classification and there is a possibility of non-class membership, it is simply classified as \"other\".\n","\n","The following table shows a summary of the different scenarios when using an ANN to solve a classification problem. Note that in the case of binary classification, the possibility that a set of entries do not belong to any class is not considered, since in this case we would be in multi-class classification."]},{"cell_type":"markdown","id":"ab6a06f0-9752-4325-9af2-3cfbc60c27f5","metadata":{"id":"ab6a06f0-9752-4325-9af2-3cfbc60c27f5"},"source":["\n","\n","|                        |                       | Is extra<br>class added<br>if it can<br>not belong<br>to any of<br>the given? | Transfer<br>function<br>of the<br>output | Output<br>range<br>of the<br>neuron | Function<br>applied<br>to the<br>outputs<br>(additional layer) | Final<br>output<br>range | Which classes<br>it classifies<br>into? |\n","|------------------------|-----------------------|------------------------------------------|-----------------------------------------|-----------------------------------|----------------------------------------------------|------------------------|------------------------------------|\n","| **Binary<br>classification** |                       | –                                        | Logarithmic<br>sigmoid                  | [0,1]                             | –                                                  | [0,1]                 | \"negative\"/\"positive\" if the output<br>is greater or equal to a threshold<br>(0.5 for outputs in [0,1]) |\n","| **Multiclass**         | Mutually<br>exclusive<br>classes | Yes                                      | Linear                                  | [-inf, +inf]                      | softmax                                            | [0,1]                 | Class whose output is<br>closer to 1                 |\n","|                        | Not<br>mutually<br>exclusive<br>classes | No                                       | Logarithmic<br>sigmoid                  | [0,1]                             | –                                                  | [0,1]                 | Classes whose output<br>is greater or equal<br>to a threshold (0.5 for outputs<br>in [0,1]); can be several or none |\n"]},{"cell_type":"markdown","id":"8bf2a1a7-74ed-4024-a461-a2cb03c4e615","metadata":{"id":"8bf2a1a7-74ed-4024-a461-a2cb03c4e615"},"source":["In the case of using a \"one-against-all\" strategy, this would be similar to the last row, except that the interval would not necessarily be `[0, 1]`, but would be conditioned by the model used, and therefore the threshold as well. For example, the outputs of a SVM range from $-\\infty$ to $+\\infty$, so the typical threshold is set to 0.\n","\n","Another factor to consider when dealing with multiclass problems is the performance metric. Most of the metrics studied (PPV, sensitivity, etc.) correspond to binary classification problems. When the number of classes is greater than 2, these metrics can still be used; however, their use is slightly different.\n","\n","When the number of classes is greater than two, the PPV, NPV, sensitivity and specificity metrics can be calculated separately for each class. Thus, from the point of view of a particular class, that class will be referred to as the positive class and the rest of classes will be put together in the negative class. In this way, from the exclusive point of view of that class, TP, TN, FP and FN can be calculated, and from them the sensitivity, specificity, PPV and NPV values for that particular class, and finally the F-score value. This way of treating classes separately is similar to the development of several classifiers in the \"one-against-all\" strategy (in the case of training binary classifiers that do not allow multi-class classification). Once these values have been calculated, they can be combined into a single value that will be used to evaluate the performance of the classifier. In this regard, there are 3 strategies: macro, weighted, and micro. We will use only the first two:\n","\n","- **Macro**. In this strategy, those metrics such as the PPV or the F-score are calculated as the arithmetic mean of the metrics of each class. As it is an arithmetic average, it does not consider the possible imbalance between classes.\n","- **Weighted**. In this stratey, the metrics corresponding to each class are averaged, weighting them with the number of patterns that belong (desired output) to each class. It is therefore suitable when classes are unbalanced.\n","- **Micro**. TP, FN, and FP are calculated globally. When the classes are not mutually exclusive, the micro-PPV or micro-F-score is equal to the accuracy value. Therefore, this metric is useful when there are mutually exclusive classes."]},{"cell_type":"markdown","id":"d7823979-84d1-4164-90b0-e1898431b7e7","metadata":{"id":"d7823979-84d1-4164-90b0-e1898431b7e7"},"source":["In this assignment, you are asked to:"]},{"cell_type":"markdown","id":"717731a5-7463-4886-b794-4b75dd36f596","metadata":{"id":"717731a5-7463-4886-b794-4b75dd36f596"},"source":["1. Develop a function called `confusionMatrix` (same name as in the previous assignment) that returns the values of the metrics adapted to the condition of having more than two classes. To do so, include an additional parameter that allows to calculate them in the *macro* and *weighted* forms.\n","\n","    This function should receive two matrices: model outputs (`outputs`) and desired outputs (`targets`), both of Boolean elements and dimension 2, with each pattern in a row and each class in a column. The first thing this function should do is to check that the number of columns of both matrices is equal and is different from 2. In case they have only one column, these columns are taken as vectors and the confusionMatrix function developed in the previous assignment is called.\n","    \n","    ### Question 4.12\n","    \n","    > ❓ Why are two-column matrices invalid?\n","    \n","    `Answer here`\n","    \n","    If both matrices have more than 2 columns, the following steps can be followed:\n","    \n","    - Reserve memory for the sensitivity, specificity, PPV, NPV and F-score vectors, with one value per class, initially equal to 0. To do this, the `zeros` function can be used.\n","    \n","    - Iterate for each class, and, if there are patterns in that class, make a call to the `confusionMatrix` function of the previous assignment, passing as vectors the columns corresponding to the class of that iteration of the outputs and targets matrices. Assign the result to the corresponding element of the sensitivity, specificity, PPV, NPV and $F_1$ vectors.\n","    - Reserve memory for the confusion matrix.\n","    - Perform a nested loop where both loops iterate over the class indices to fill in each cell of the confusion matrix.\n","  ### Question 4.13\n","  > 💡 In reality, it is not necessary to reserve memory and use nested loops.  \n","  > This can be implemented in a more elegant and efficient way using a **comprehension**, in a single line of code.  \n","  > As a hint, it may help to first write the nested loop with a one-line body, and then try to convert it into a comprehension."]},{"cell_type":"code","execution_count":null,"id":"45bac10d-8aa5-4f0e-8793-d7b473348fe6","metadata":{"id":"45bac10d-8aa5-4f0e-8793-d7b473348fe6"},"outputs":[],"source":["`Answer here`"]},{"cell_type":"markdown","id":"9a086c34-8607-440c-b89b-61096691cefd","metadata":{"id":"9a086c34-8607-440c-b89b-61096691cefd"},"source":["\n","* Aggregate the values of sensitivity, specificity, PPV, NPV, and $F_1$-score for each class into a single value according to the *macro* or *weighed* strategy, as specified in the input argument.\n","> ⚠️ When using the **weighted** strategy, it is necessary to compute how many instances belong to each class in order to calculate the weighted average.  \n","> This can be done with `sum(targets, dims=1)`.  \n","> However, this call returns a matrix with a single row (and one column per class).  \n","> If you try to compute the element-wise product of this result with one of the metric vectors (e.g., the sensitivity vector), the result will be a matrix, because the metric vector is interpreted as a column — and multiplying a row by a column gives a full matrix.\n","> ✅ This can be resolved in several ways, the simplest being to **flatten** the matrix of instance counts into a vector using: `vec(sum(targets, dims=1))`\n","    \n","* Finally, calculate the accuracy value with the `accuracy` function developed in a previous assignment, and calculate the error rate from this value."]},{"cell_type":"code","execution_count":5,"id":"dde65a04","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dde65a04","executionInfo":{"status":"ok","timestamp":1759955901019,"user_tz":-120,"elapsed":33,"user":{"displayName":"José Manuel Bueno","userId":"13231755112106791400"}},"outputId":"673c16c4-8b7b-4cb4-da06-4ec510818c2d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["confusionMatrix (generic function with 1 method)"]},"metadata":{},"execution_count":5}],"source":["function confusionMatrix(outputs::AbstractArray{Bool,2}, targets::AbstractArray{Bool,2}; weighted::Bool=true)\n","\n","\n","    @assert size(outputs) == size(targets) \"Outputs and targets have to have the same size\"\n","    num_classes = size(targets, 2)\n","\n","    if num_classes == 1\n","        return confusionMatrix(vec(outputs), vec(targets))\n","    end\n","\n","    sensitivities = zeros(Float64, num_classes)\n","    specificities = zeros(Float64, num_classes)\n","    ppvs = zeros(Float64, num_classes)\n","    npvs = zeros(Float64, num_classes)\n","    f1s = zeros(Float64, num_classes)\n","\n","    class_counts = vec(sum(targets; dims = 1))\n","\n","    for c in 1:num_classes\n","        if class_counts[c] > 0\n","            acc, err, sens, spec, ppv, npv, f1, _ =\n","                confusionMatrix(outputs[:, c], targets[:, c])\n","            sensitivities[c] = sens\n","            specificities[c] = spec\n","            ppvs[c] = ppv\n","            npvs[c] = npv\n","            f1s[c] = f1\n","        end\n","    end\n","\n","    confusion = Array{Int64}(undef, num_classes, num_classes)\n","    for r in 1:num_classes, c in 1:num_classes\n","        confusion[r, c] = count(outputs[:, r] .& targets[:, c])\n","    end\n","\n","    weights = weighted ? class_counts : ones(Float64, num_classes)\n","    weight_sum = sum(weights)\n","    weights = weight_sum == 0 ? fill(1.0 / num_classes, num_classes) : weights ./ weight_sum\n","\n","    macro_sens = sum(weights .* sensitivities)\n","    macro_spec = sum(weights .* specificities)\n","    macro_ppv = sum(weights .* ppvs)\n","    macro_npv = sum(weights .* npvs)\n","    macro_f1 = sum(weights .* f1s)\n","\n","    acc = accuracy(outputs, targets)\n","    err = 1 - acc\n","\n","    return (acc, err, macro_sens, macro_spec, macro_ppv, macro_npv, macro_f1, confusion)\n","end"]},{"cell_type":"markdown","id":"1626864d-d26f-4574-9769-3a33d28e9a4d","metadata":{"id":"1626864d-d26f-4574-9769-3a33d28e9a4d"},"source":["This function should return a tuple with the same values specified in the previous exercise, in the same order: **accuracy, error rate, sensitivity, specificity, PPV, NPV, F1-score, and the confusion matrix.** As mentioned before, the use of loops is allowed for implementing this function.\n","\n","\n","3. Develop another function called `confusionMatrix` in which the first parameter `outputs` is of type `AbstractArray{<:Real,2}`, and `targets` is of type `AbstractArray{Bool,2}` (the same as before). What this function should do is to convert the first parameter to an array of boolean values (using the function `classifyOutputs`) and call the previous function. Additionally, this function may receive two optional arguments: `threshold` and `weighted`.  \n","These values will be used when calling the respective underlying functions that this one builds upon.\n","\n","  ### Question 4.14\n","> ❓ Within this function, in which calls will each of these two optional parameters be used?"]},{"cell_type":"markdown","id":"c1217bc8-7bce-447c-9ab4-c2fe91069fce","metadata":{"id":"c1217bc8-7bce-447c-9ab4-c2fe91069fce"},"source":["The threshold parameter will be used when the matrices have just one column, that means, on the binary problem. This is applied on the function classifyOutputs.\n","\n","The weighted parameter is applied on the boolean version of the function confusionMatrix, the previous one, for determining if the metrics will be calculated as a weighted average or not."]},{"cell_type":"markdown","id":"5ea0261b-1f93-41a2-8d70-646911358cdb","metadata":{"id":"5ea0261b-1f93-41a2-8d70-646911358cdb"},"source":["> ❓ In which cases is the `threshold` parameter required?"]},{"cell_type":"markdown","id":"187c0d9b-49e2-4924-a3ea-a4cd7ed65f30","metadata":{"id":"187c0d9b-49e2-4924-a3ea-a4cd7ed65f30"},"source":["Just when the output matrix has just one column, has stated."]},{"cell_type":"markdown","id":"b1c440ef-c6e0-4081-84f7-9084e41bfa62","metadata":{"id":"b1c440ef-c6e0-4081-84f7-9084e41bfa62"},"source":["Since this function builds upon the previous one, it **should not contain any loops**."]},{"cell_type":"code","execution_count":6,"id":"4bef0d58","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4bef0d58","executionInfo":{"status":"ok","timestamp":1759956338694,"user_tz":-120,"elapsed":4423,"user":{"displayName":"José Manuel Bueno","userId":"13231755112106791400"}},"outputId":"c7a29994-8a65-42fc-8e7d-73bb443305b0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["confusionMatrix (generic function with 2 methods)"]},"metadata":{},"execution_count":6}],"source":["using Downloads\n","using DelimitedFiles\n","\n","github_url = \"https://raw.githubusercontent.com/jmbuenot/ML1-Master-MIA/main/utils_ML1.jl\"\n","local_path = joinpath(pwd(), \"utils_ML1.jl\")\n","Downloads.download(github_url, local_path)\n","\n","include(local_path)\n","using .UtilsML1: classifyOutputs\n","function confusionMatrix(outputs::AbstractArray{<:Real,2},targets::AbstractArray{Bool,2};threshold::Real = 0.5,weighted::Bool = true)\n","    classified = classifyOutputs(outputs; threshold = threshold)\n","    return confusionMatrix(classified, targets; weighted = weighted)\n","end"]},{"cell_type":"markdown","id":"d146495f-1433-4d82-abae-9e0d1de169b1","metadata":{"id":"d146495f-1433-4d82-abae-9e0d1de169b1"},"source":["4. Override this function once again by developing another function of the same name that performs the same task, but this time taking as inputs two vectors (`targets` and `outputs`) of the same length, whose elements are of any type (i.e., they are of type `AbstractArray{<:Any}`), plus the additional parameter that allows to aggregate the metrics through the *macro* and *weighted* strategies. The vectors `outputs` and `targets` contain the predicted and true labels for each instance, respectively. Therefore, both vectors must have the same length. The vector `classes` contains all possible class labels, and will usually have a different length than `outputs` and `targets`. The elements of these vectors represent labels and may take various forms — such as real numbers, integers, strings, or symbols. For this reason, the type `Any` is used. For example, a valid `classes` vector could be:\n","   ```julia\n","       [\"dog\", \"cat\", 3, :green]\n","    ```\n","    Obviously, both the predicted labels (`outputs` vector) and the true labels (`targets` vector) must be included in the `classes` vector.\n","\n","  ### Question 4.15\n"," > ❓ Write this check without using any loops. You may find the functions `all`, `in`, and `unique` useful. (A solution is provided at the end of the notebook.)"]},{"cell_type":"markdown","id":"9d4426cd-265a-41a3-8485-76dac37d9fcb","metadata":{"id":"9d4426cd-265a-41a3-8485-76dac37d9fcb"},"source":["\n","We could check that the predicted and true labels belong to the set of classes is to evaluate all(in(clase, classes_vec) for clase in outputs) and the same for targets, after evaluating length(unique(classes_vec)) == length(classes_vec) for verifying that there are not duplicated classes.\n"]},{"cell_type":"markdown","id":"2712b539-52c7-470c-bf8b-3938c15ba213","metadata":{"id":"2712b539-52c7-470c-bf8b-3938c15ba213"},"source":["This function can be implemented by calling a previous version of `confusionMatrix`, specifically the one that receives **boolean matrices (2D)** along with an optional argument.\n","\n","To prepare for this, both `outputs` and `targets` should be **one-hot encoded** by calling the `oneHotEncoding` function, passing the `classes` vector as input.\n","\n","Once both matrices are encoded, you can call the corresponding `confusionMatrix` function.\n","⚠️ It is essential to build the `classes` vector **before** the calls to `oneHotEncoding`, and to use the **same vector** in both calls.  "]},{"cell_type":"markdown","id":"644176f3-88d5-421a-a013-cc03287ce06d","metadata":{"id":"644176f3-88d5-421a-a013-cc03287ce06d"},"source":["   ### Question 4.16\n","  > ❓ What could happen if this is not done?"]},{"cell_type":"markdown","id":"8c87ce40-36d9-4302-9102-bbe3adc696b1","metadata":{"id":"8c87ce40-36d9-4302-9102-bbe3adc696b1"},"source":["If this is not done, it means, if we not check that all of the labels appear as one of the provided classes, then we could not have aligned columns when doing oneHotEncoding, so we could have a not proper matrix for computing the metrics or the confussion matrix, so these last computations would loose its function."]},{"cell_type":"code","execution_count":7,"id":"117a3eb2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"117a3eb2","executionInfo":{"status":"ok","timestamp":1759957571519,"user_tz":-120,"elapsed":50,"user":{"displayName":"José Manuel Bueno","userId":"13231755112106791400"}},"outputId":"894f6ebb-c576-42b7-e59b-9fbf2894d513"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["confusionMatrix (generic function with 3 methods)"]},"metadata":{},"execution_count":7}],"source":["using .UtilsML1: oneHotEncoding\n","function confusionMatrix(outputs::AbstractArray{<:Any,1},targets::AbstractArray{<:Any,1},classes::AbstractArray{<:Any,1};weighted::Bool = true)\n","    @assert length(outputs) == length(targets) \"Outputs and targets have to have the same length\"\n","\n","    classes_vec = collect(classes)\n","    @assert !isempty(classes_vec) \"At least we require one class\"\n","    @assert length(unique(classes_vec)) == length(classes_vec) \"No duplicate classes\"\n","    @assert all(in(value, classes_vec) for value in outputs) \"outputs have labels out of the classes considered\"\n","    @assert all(in(value, classes_vec) for value in targets) \"'targets have labels out of the classes considered\"\n","\n","    if length(classes_vec) == 1\n","        cls = classes_vec[1]\n","        encoded_outputs = reshape(outputs .== cls, :, 1)\n","        encoded_targets = reshape(targets .== cls, :, 1)\n","    else\n","        encoded_outputs = oneHotEncoding(outputs, classes_vec)\n","        encoded_targets = oneHotEncoding(targets, classes_vec)\n","    end\n","\n","    return confusionMatrix(encoded_outputs, encoded_targets; weighted = weighted)\n","end"]},{"cell_type":"markdown","id":"3da54da5-0a4e-4f57-8057-ff49ea54041e","metadata":{"id":"3da54da5-0a4e-4f57-8057-ff49ea54041e"},"source":["Since this function builds on a previous one, it must **return the same outputs** and **must not use loops**."]},{"cell_type":"markdown","id":"57916e51-9751-4dc9-a26a-f78daa475217","metadata":{"id":"57916e51-9751-4dc9-a26a-f78daa475217"},"source":["5. You must now create a final version of `confusionMatrix`, similar to the one described above, but **without receiving the `classes` vector** explicitly. Instead, `classes` should be constructed from `outputs` and `targets` using:\n","   ```julia\n","          classes = unique(vcat(targets, outputs))\n","   ```\n","  Then, call the previous version of `confusionMatrix`, passing `outputs`, `targets`, `classes`, and the optional argument.\n","\n","> ⚠️ Use this version with caution.\n","> If the data has been partitioned (e.g., for cross-validation), some classes might not appear in `outputs` or `targets`, potentially affecting results."]},{"cell_type":"code","execution_count":9,"id":"0fbff7d6-93c9-41f4-b34a-7f0bffd420d4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0fbff7d6-93c9-41f4-b34a-7f0bffd420d4","executionInfo":{"status":"ok","timestamp":1759958602745,"user_tz":-120,"elapsed":52,"user":{"displayName":"José Manuel Bueno","userId":"13231755112106791400"}},"outputId":"3eb67b0d-c76a-4599-bd2b-56554a0fee75"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["confusionMatrix (generic function with 4 methods)"]},"metadata":{},"execution_count":9}],"source":["function confusionMatrix(outputs::AbstractArray{<:Any,1},targets::AbstractArray{<:Any,1};weighted::Bool = true)\n","    classes = unique(vcat(targets, outputs))\n","    return confusionMatrix(outputs, targets, classes; weighted = weighted)\n","end"]},{"cell_type":"markdown","id":"63d120a8-43d7-4662-ab39-2eaaf9b75331","metadata":{"id":"63d120a8-43d7-4662-ab39-2eaaf9b75331"},"source":["As before, this function must return the same outputs and should not use loops.\n","\n","## printConfusionMatrix (multiclass versions)\n","As in the previous notebook, implement four versions of the `printConfusionMatrix` function — now for the multiclass case, where both `outputs` and `targets` may be matrices or vectors, depending on the context."]},{"cell_type":"code","execution_count":14,"id":"4f48e7f0-f8d4-4930-9749-9d35144b46e7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4f48e7f0-f8d4-4930-9749-9d35144b46e7","executionInfo":{"status":"ok","timestamp":1759959293091,"user_tz":-120,"elapsed":42,"user":{"displayName":"José Manuel Bueno","userId":"13231755112106791400"}},"outputId":"157082e0-c425-4029-ad44-6efb855341e1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["printConfusionMatrix (generic function with 1 method)"]},"metadata":{},"execution_count":14}],"source":["function printConfusionMatrix(outputs::AbstractArray{Bool,2},targets::AbstractArray{Bool,2};weighted::Bool = true)\n","    metrics = confusionMatrix(outputs, targets; weighted = weighted)\n","    aggregation = weighted ? \"weighted\" : \"macro\"\n","    _print_confusion_summary(metrics; aggregation = aggregation)\n","    return nothing\n","end"]},{"cell_type":"code","execution_count":15,"id":"7a595e73-603b-44b6-98c7-55331a66cc34","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7a595e73-603b-44b6-98c7-55331a66cc34","executionInfo":{"status":"ok","timestamp":1759959356108,"user_tz":-120,"elapsed":60,"user":{"displayName":"José Manuel Bueno","userId":"13231755112106791400"}},"outputId":"11c90ede-cecc-4ae4-f39b-69be682103cf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["printConfusionMatrix (generic function with 2 methods)"]},"metadata":{},"execution_count":15}],"source":["function printConfusionMatrix(outputs::AbstractArray{<:Real,2},targets::AbstractArray{Bool,2};weighted::Bool = true)\n","    metrics = confusionMatrix(outputs, targets; weighted = weighted)\n","    aggregation = weighted ? \"weighted\" : \"macro\"\n","    _print_confusion_summary(metrics; aggregation = aggregation)\n","    return nothing\n","end"]},{"cell_type":"code","execution_count":17,"id":"8cff04ed-4158-4d66-820a-2720bb25e1ee","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8cff04ed-4158-4d66-820a-2720bb25e1ee","executionInfo":{"status":"ok","timestamp":1759959389108,"user_tz":-120,"elapsed":19,"user":{"displayName":"José Manuel Bueno","userId":"13231755112106791400"}},"outputId":"dbcc4cdd-beb5-4898-f7e8-90f909f7bcf5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["printConfusionMatrix (generic function with 3 methods)"]},"metadata":{},"execution_count":17}],"source":["function printConfusionMatrix(outputs::AbstractArray{<:Any,1},targets::AbstractArray{<:Any,1},classes::AbstractArray{<:Any,1};weighted::Bool = true)\n","    metrics = confusionMatrix(outputs, targets, classes; weighted = weighted)\n","    aggregation = weighted ? \"weighted\" : \"macro\"\n","    _print_confusion_summary(metrics; aggregation = aggregation)\n","    return nothing\n","end"]},{"cell_type":"code","execution_count":18,"id":"d6bd6590-a074-4a05-a3ca-a789e642b3ef","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d6bd6590-a074-4a05-a3ca-a789e642b3ef","executionInfo":{"status":"ok","timestamp":1759959426939,"user_tz":-120,"elapsed":42,"user":{"displayName":"José Manuel Bueno","userId":"13231755112106791400"}},"outputId":"144d67d0-09b9-4ee8-d667-229290dd709d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["printConfusionMatrix (generic function with 4 methods)"]},"metadata":{},"execution_count":18}],"source":["function printConfusionMatrix(outputs::AbstractArray{<:Any,1},targets::AbstractArray{<:Any,1};weighted::Bool = true)\n","    metrics = confusionMatrix(outputs, targets; weighted = weighted)\n","    aggregation = weighted ? \"weighted\" : \"macro\"\n","    _print_confusion_summary(metrics; aggregation = aggregation)\n","    return nothing\n","end"]},{"cell_type":"markdown","id":"25c99fd3-7ba5-4044-babe-f91f396564ba","metadata":{"id":"25c99fd3-7ba5-4044-babe-f91f396564ba"},"source":["All four versions must also accept an optional argument to choose whether metrics are computed using the macro or weighted strategy.\n","\n","> 🛠 These four functions will not be graded, but they will be useful for Exercise 2."]},{"cell_type":"markdown","id":"adf09566","metadata":{"id":"adf09566"},"source":["### Learn Julia\n","\n","The defensive programming line to ensure that all classes of the `output` vector are included in the desired output vector is as follows:\n","\n","```Julia\n","@assert(all([in(output, unique(targets)) for output in outputs]))\n","```"]},{"cell_type":"code","execution_count":null,"id":"00268f0a","metadata":{"id":"00268f0a"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Julia","name":"julia"},"language_info":{"name":"julia"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}