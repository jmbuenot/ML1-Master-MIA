{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d977b995-8f5e-4031-b67f-acbd83c49a53",
   "metadata": {},
   "source": [
    "# The ScikitLearn.jl library\n",
    "\n",
    "The Scikit-learn library is an open-source machine learning library developed for the Python programming language, the first version of which dates back to 2010. It implements many machine learning models, related to classification, regression, clustering or dimensionality reduction. These models include Support Vector Machines (SVM), decision trees, random forests, or k-means. It is currently one of the most widely used libraries in the field of machine learning, due to the large number of functionalities it offers as well as its ease of use, since it provides a uniform interface for training and using models. The documentation for this library is available at https://scikit-learn.org/stable/.\n",
    "\n",
    "For Julia, the ScikitLearn.jl library implements this interface and the algorithms contained in the scikit-learn library, supporting both Julia's own models and those of the scikit-learn library. The latter is done by means of the PyCall.jl library, which allows code written in Python to be executed from Julia in a transparent way for the user, who only needs to have ScikitLearn.jl installed. Documentation for this library can be found at https://scikitlearnjl.readthedocs.io/en/latest/.\n",
    "\n",
    "However, recently, some incompatibilities have been reported with some versions of the SSL library. To avoid potential compatibility issues between Julia, PyCall, and ScikitLearn, we will use a different library for this exercise.\n",
    "The library we will use is MLJ (Machine Learning in Julia), which is not strictly a library but rather a framework that allows the use of various related libraries through a common interface.\n",
    "As a result, the function names used to create and train models remain the same regardless of the specific models being used.\n",
    "In the practical sessions of this course, in addition to ANNs, we will use the following models, available within the MLJ framework:\n",
    "\n",
    "- Support Vector Machines (SVM)\n",
    "- Decision trees\n",
    "- kNN\n",
    "\n",
    "In order to use these models, it is first necessary to install and import the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74f7a80f-4883-46f1-be06-db52e981c19a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `/opt/julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `/opt/julia/environments/v1.9/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `/opt/julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `/opt/julia/environments/v1.9/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "    import Pkg;\n",
    "    Pkg.add(\"MLJ\")\n",
    "    Pkg.add([\n",
    "      \"LIBSVM\", \"MLJLIBSVMInterface\",\n",
    "      \"DecisionTree\", \"MLJDecisionTreeInterface\",\n",
    "      \"NearestNeighborModels\", \"CategoricalArrays\"\n",
    "    ])\n",
    "    using MLJ;\n",
    "    using LIBSVM, DecisionTree, Random, MLJLIBSVMInterface, MLJDecisionTreeInterface, NearestNeighborModels, DelimitedFiles, CategoricalArrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f4b6f9-e0c7-4e04-a4d8-21d63495a95c",
   "metadata": {},
   "source": [
    "Similarly, it is necessary to install the packages that contain the specific learning algorithms (e.g., LIBSVM, NearestNeighborModels, DecisionTree) as well as the packages that provide the interfaces between these algorithms and the MLJ framework (MLJLIBSVMInterface, MLJDecisionTreeInterface).\n",
    "To import the models to be used, we can rely on the `MLJ.@load` macro. For example, the following lines import the three models mentioned above, which will be used in this course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a935a0dc-d5b2-4880-b692-e338679d8346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `/opt/julia/environments/v1.9/Project.toml`\n",
      "  \u001b[90m[336ed68f] \u001b[39mCSV v0.10.15\n",
      "\u001b[33m⌅\u001b[39m \u001b[90m[324d7699] \u001b[39mCategoricalArrays v0.10.8\n",
      "\u001b[33m⌅\u001b[39m \u001b[90m[a93c6f00] \u001b[39mDataFrames v1.7.1\n",
      "  \u001b[90m[7806a523] \u001b[39mDecisionTree v0.12.4\n",
      "  \u001b[90m[8bb1440f] \u001b[39mDelimitedFiles v1.9.1\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[5789e2e9] \u001b[39mFileIO v1.17.0\n",
      "\u001b[33m⌅\u001b[39m \u001b[90m[587475ba] \u001b[39mFlux v0.14.21\n",
      "\u001b[33m⌅\u001b[39m \u001b[90m[f67ccb44] \u001b[39mHDF5 v0.16.16\n",
      "\u001b[33m⌅\u001b[39m \u001b[90m[7073ff75] \u001b[39mIJulia v1.26.0\n",
      "  \u001b[90m[916415d5] \u001b[39mImages v0.26.2\n",
      "  \u001b[90m[033835bb] \u001b[39mJLD2 v0.6.2\n",
      "  \u001b[90m[b1bec4e5] \u001b[39mLIBSVM v0.8.1\n",
      "  \u001b[90m[23992714] \u001b[39mMAT v0.10.7\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[add582a8] \u001b[39mMLJ v0.20.0\n",
      "  \u001b[90m[c6f25543] \u001b[39mMLJDecisionTreeInterface v0.4.2\n",
      "  \u001b[90m[61c7150f] \u001b[39mMLJLIBSVMInterface v0.2.1\n",
      "  \u001b[90m[6ee0df7b] \u001b[39mMLJLinearModels v0.10.1\n",
      "  \u001b[90m[6f286f6a] \u001b[39mMultivariateStats v0.10.3\n",
      "  \u001b[90m[9bbee03b] \u001b[39mNaiveBayes v0.5.6\n",
      "  \u001b[90m[636a865e] \u001b[39mNearestNeighborModels v0.2.3\n",
      "\u001b[33m⌅\u001b[39m \u001b[90m[91a5bcdd] \u001b[39mPlots v1.40.20\n",
      "\u001b[33m⌅\u001b[39m \u001b[90m[c3e4b0f8] \u001b[39mPluto v0.19.47\n",
      "  \u001b[90m[3646fa90] \u001b[39mScikitLearn v0.7.0\n",
      "  \u001b[90m[f3b207a7] \u001b[39mStatsPlots v0.15.8\n",
      "  \u001b[90m[24678dba] \u001b[39mTSne v1.3.0\n",
      "  \u001b[90m[bd369af6] \u001b[39mTables v1.12.1\n",
      "  \u001b[90m[fdbf4ff8] \u001b[39mXLSX v0.10.4\n",
      "  \u001b[90m[10745b16] \u001b[39mStatistics v1.9.0\n",
      "\u001b[36m\u001b[1mInfo\u001b[22m\u001b[39m Packages marked with \u001b[32m⌃\u001b[39m and \u001b[33m⌅\u001b[39m have new versions available, but those with \u001b[33m⌅\u001b[39m are restricted by compatibility constraints from upgrading. To see why use `status --outdated`\n"
     ]
    }
   ],
   "source": [
    "SVMClassifier = MLJ.@load SVC pkg=LIBSVM verbosity=0\n",
    "kNNClassifier = MLJ.@load KNNClassifier pkg=NearestNeighborModels verbosity=0\n",
    "DTClassifier = MLJ.@load DecisionTreeClassifier pkg=DecisionTree verbosity=0\n",
    "\n",
    "Pkg.status()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0066a56-b180-4e1e-9ac0-153915e072d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "As can be seen, each model is loaded from a different package. The `verbosity=0` option is simply used to suppress the output message that would otherwise be printed during the import.\n",
    "This way, we define three functions to create each one of the three models. Each function receives as arguments the specific hyperparameters for the corresponding model.\n",
    "Below are three examples, one for each type of model that will be used in these course exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "286fcfa5-2185-4bd4-a80e-d75d23f1e9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(\n",
       "  kernel = LIBSVM.Kernel.RadialBasis, \n",
       "  gamma = 2.0, \n",
       "  cost = 1.0, \n",
       "  cachesize = 200.0, \n",
       "  degree = 3, \n",
       "  coef0 = 0.0, \n",
       "  tolerance = 0.001, \n",
       "  shrinking = true)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SVMClassifier(kernel=LIBSVM.Kernel.RadialBasis, cost=1.0, gamma=2.0, degree=Int32(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9004a1f-2ebb-4f1d-8142-ab113f0b3e9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(\n",
       "  max_depth = 4, \n",
       "  min_samples_leaf = 1, \n",
       "  min_samples_split = 2, \n",
       "  min_purity_increase = 0.0, \n",
       "  n_subfeatures = 0, \n",
       "  post_prune = false, \n",
       "  merge_purity_threshold = 1.0, \n",
       "  display_depth = 5, \n",
       "  feature_importance = :impurity, \n",
       "  rng = MersenneTwister(1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DTClassifier(max_depth=4, rng=Random.MersenneTwister(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f6f400f-9e7f-400f-b30d-3582be8f145d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNNClassifier(\n",
       "  K = 3, \n",
       "  algorithm = :kdtree, \n",
       "  metric = Euclidean(0.0), \n",
       "  leafsize = 10, \n",
       "  reorder = true, \n",
       "  weights = Uniform())"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = kNNClassifier(K=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f622c21e-8ad3-4152-a9c1-5f939ccc644f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "When creating a kNN model, the main hyperparameter is `K`, which defines the number of neighbors.\n",
    "For decision trees, the main hyperparameter is `max_depth`, which sets the maximum depth of the tree.\n",
    "In the case of decision trees, as shown earlier, there is also a parameter called `rng`. This parameter controls the randomness involved in a specific part of the model construction process.\n",
    "\n",
    "Specifically, for decision trees, this randomness occurs during the selection of features used to split a node. The `DecisionTree` library uses a random number generator (RNG) for this step, which is updated with each call. As a result, different calls to the function (along with subsequent calls to `fit!`) may produce different models, even with the same data.\n",
    "\n",
    "To control this randomness and make the process deterministic, it is advisable to provide a fixed integer value as the RNG seed, as shown in the previous example.\n",
    "This ensures that creating a model with a given input-output dataset and a defined set of hyperparameters becomes a reproducible process.\n",
    "\n",
    "In general, it is preferable to control randomness across the entire model development workflow (e.g., cross-validation, training/test splits) by setting a global random seed at the beginning.\n",
    "However, for the purposes of these exercises, we will use the `rng` keyword specifically for the decision tree model.\n",
    "\n",
    "SVMs have a more complex set of hyperparameters, which depend on the kernel function being used.  \n",
    "First, the hyperparameter `C` controls the trade-off between the margin width and classification error. Lower values allow for more misclassifications (more tolerance), while higher values fit the model more tightly to the data.  \n",
    "In MLJ, this parameter is passed using the keyword `cost` when calling `SVMClassifier`.\n",
    "\n",
    "Additionally, it is necessary to specify which kernel to use. This is done using the keyword `kernel`, which can take one of the following values provided by the `LIBSVM` library:\n",
    "\n",
    "- `LIBSVM.Kernel.Linear`\n",
    "- `LIBSVM.Kernel.RadialBasis`\n",
    "- `LIBSVM.Kernel.Sigmoid`\n",
    "- `LIBSVM.Kernel.Polynomial`\n",
    "\n",
    "Depending on the kernel selected, different additional hyperparameters are used:\n",
    "\n",
    "- **Linear kernel**: Only requires `C` (via `cost`).\n",
    "- **RBF (Radial Basis Function) kernel**: In addition to `C`, it uses `gamma`, which controls the influence of each support vector.\n",
    "- **Sigmoid kernel**: Uses `C`, `gamma`, and `coef0`. This kernel behaves similarly to a neural network, where `gamma` and `coef0` influence the shape of the decision function.\n",
    "- **Polynomial kernel**: Uses `C`, `degree` (the degree of the polynomial), `gamma`, and `coef0`.\n",
    "\n",
    "Typical values for these hyperparameters include:  \n",
    "`0.001`, `0.1`, `1`, `10`, `100`, `1000`.\n",
    "\n",
    "The following table summarises the different hyperparameters, the kernels that use them, and typical values they may take.\n",
    "\n",
    "Note that when calling the `SVMClassifier` function, the hyperparameters use the same names as listed here, except for `C`, which must be passed as `cost`, as shown in the previous example.\n",
    "\n",
    "It is also important that the arguments passed to the function have the correct type, as required by the `LIBSVM` library. Otherwise, an error may occur.\n",
    "\n",
    "To prevent this, it is recommended to explicitly cast each hyperparameter to the appropriate type when calling the function.\n",
    "\n",
    "| Hyperparameter | Applicable Kernels                 | Typical Values                     | Required Type in LIBSVM |\n",
    "|----------------|------------------------------------|------------------------------------|--------------------------|\n",
    "| `cost` (`C`)   | Linear, RBF, Sigmoid, Polynomial   | 0.001, 0.1, 1, 10, 100, 1000       | `Float64`               |\n",
    "| `gamma`        | RBF, Sigmoid, Polynomial           | 0.1, 0.01, 0.001, 0.0001           | `Float64`               |\n",
    "| `coef0`        | Sigmoid, Polynomial                | 0, 1, 5, 10                        | `Int32`                 |\n",
    "| `degree`       | Polynomial                         | 2, 3, 4, 5                         | `Float64`               |\n",
    "\n",
    "Although the basic SVM model is inherently binary, the implementation provided in MLJ already supports multi-class classification.  \n",
    "Therefore, it is not necessary to manually apply a one-vs-all strategy for multi-class problems.\n",
    "\n",
    "Once a model has been created, it must be wrapped in a `machine` object. This object acts as a container that associates the model with the data and handles both training and prediction.  \n",
    "It is a core concept in MLJ and simplifies model workflows by centralizing model fitting (`fit!`) and prediction (`predict`) logic.\n",
    "\n",
    "A `machine` has three main components:\n",
    "\n",
    "- **Model**: Specifies the algorithm to be used. It has been created earlier, without any data or learned state.\n",
    "- **Data**: Provides the input features and target labels (if supervised).\n",
    "- **Internal State**: Stores learned parameters after the model is trained.\n",
    "\n",
    "To create a `machine`, you can use the `machine` function, passing in the model, the input features, and the target labels.  \n",
    "Note that the input data must not be plain arrays. Instead, it should be converted to a supported table format such as `Tables.table`, `DataFrame`, or a `NamedTuple`.  \n",
    "If your data is currently stored as arrays, as is the case in these exercises, the following line shows how to construct the machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73203665-5261-45d4-8ce4-59ce0ffdd53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data from previous notebooks\n",
    "if !isdefined(Main, :UtilsML1)\n",
    "    include(\"utils_ML1.jl\")\n",
    "end\n",
    "using .UtilsML1\n",
    "using Flux  # For activation functions like σ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d2ab75-c752-4497-ba35-22d028e61179",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = SVMClassifier(kernel=LIBSVM.Kernel.RadialBasis, cost=1.0, gamma=2.0, degree=Int32(3))\n",
    "# create the machine object\n",
    "mach = machine(model, MLJ.table(trainingInputs), categorical(trainingTargets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6435ff-3ca4-4e90-98f0-e9e7ac9d79d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "As shown, the input matrix is converted into a table, and the target vector is converted into a categorical array, since this exercise involves classification problems.  \n",
    "\n",
    "It is important to note that the variable `targets` (the output labels) should be a **vector**, not a matrix. Each element in the vector corresponds to the label of one input sample and can be of any type (e.g., integer, string, etc.).\n",
    "\n",
    "Although some models may support one-hot encoded labels, others do not. Therefore, in these exercises, we will use a vector of labels, with one label per instance, rather than one-hot encoding (which is typically used in neural networks).\n",
    "\n",
    "To prevent compatibility issues with certain model implementations, we convert all label values to `String` before passing them to the model.\n",
    "\n",
    "Once the machine object has been created, the model can be trained using the `fit!` function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6aa2ad-7df8-4d8c-af9b-0a60473a511d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "MLJ.fit!(mach, verbosity=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be84f61-ec1e-4d43-9643-3adb74f2c94e",
   "metadata": {},
   "source": [
    "This function only requires the `machine` object as an argument, since the training data has already been bound to it.\n",
    "The optional argument `verbosity=0` is used to suppress output messages during training.\n",
    "\n",
    "### Question 6.1\n",
    "\n",
    "> ❓ What does the fact that the name of this function ends in bang (!) indicate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b8524",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "It indicates that this function not only returns some result, but changes its arguments. It called \"mutating function\". In this case the _fit_ function changes mach object passed as an argumen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5151f1",
   "metadata": {},
   "source": [
    "Contrary to the Flux library, where it was necessary to write the ANN training loop, in this library the loop is already implemented, and it is called automatically when the `fit!` function is executed. Therefore, it is not necessary to write the code for the training loop.\n",
    "\n",
    "An important aspect to consider is the layout of the data to be used.  \n",
    "\n",
    "As shown in previous exercises, when training an Artificial Neural Network (ANN), the input samples (patterns) are arranged in **columns**, and each **row** in the input matrix represents a feature.\n",
    "\n",
    "However, outside the scope of ANNs — and therefore for all other techniques used in this course — it is assumed that the samples are arranged in **rows**, meaning each **column** in the input matrix corresponds to a feature. This format is generally more intuitive and will be used throughout the rest of the course.\n",
    "\n",
    "\n",
    "### Question 6.2\n",
    "\n",
    "> ❓ As in the case of ANNs, a loop is necessary for training several models. Where in the code (inside or outside the loop) will you need to create the model? Which models will need to be trained several times and which ones only once? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7437a0",
   "metadata": {},
   "source": [
    "**Answer**  \n",
    "The model must be created inside the cross-validation loop, is necessary because each fold uses different training data, so we need untrained model for each fold to ensure fair evaluation.  \n",
    "SVM, DDecision Tree, \n",
    "k - once per fold, because they are deterministic, they produce the same result when trained on the same data with the same hyperparameters  \n",
    "ANNs - multiple times N times because training multiple times and averaging the results gives more reliable performance estimates and reduces the impact of unlucky initializationsNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dc832c",
   "metadata": {},
   "source": [
    "### Question 6.3\n",
    "\n",
    "> ❓ Which condition must the matrix of inputs and the vector of desired outputs passed as an argument to this function fulfil?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ed4cd",
   "metadata": {},
   "source": [
    "**Answer**  \n",
    "matrix of inputs: The features should be stored in a 2D array where each row is a sample and each column is a feature. Before training, this array must be converted to a table using MLJ.table(inputs) so that MLJ can read it correctly.  \n",
    "vector of desired outputs: The labels should be a 1D vector with the class names (not one-hot encoded). Then, convert them to categorical values using categorical(targets)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a267fcc1-45d6-4b06-ae4c-dedea71c57b6",
   "metadata": {},
   "source": [
    "Finally, once the model has been trained, it can be used to make predictions. This is done using the `predict` function. The following is an example of how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886ce4d7-835e-45bf-adca-163802d7690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "testOutputs = MLJ.predict(model, MLJ.table(testInputs));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b6a68-0c3e-4ad0-8eeb-d583e691b5eb",
   "metadata": {},
   "source": [
    "As shown, the `predict` function requires two arguments: the `machine` object and the input matrix, which must be converted to a table format.\n",
    "\n",
    "In classification problems, the type of the prediction result depends on the model and the underlying library:\n",
    "\n",
    "- **For SVMs**, `predict` returns a `CategoricalArray`, which can be directly compared with the ground truth labels. No post-processing is needed.\n",
    "- **For Decision Trees and kNN**, `predict` returns a `UnivariateFiniteArray`, which represents a probability distribution over the possible classes.  \n",
    "  To convert this into a single predicted label (so it can be compared with the true values), you can use the `mode` function to extract the most likely class.\n",
    "\n",
    "The model being used is stored in memory as a structured object with several fields, and it can be very useful to inspect its contents.  \n",
    "The `machine` object holds the model, the data, and the results of training. Therefore, you can access the trained model through the `machine`, or more directly through the variable `model`.\n",
    "\n",
    "For example, when training an SVM, you can access one of its hyperparameters in either of the following ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3696ef1c-191f-4966-9bc7-e2150cb393c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gamma\n",
    "mach.model.gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0490a57e-8944-4f04-9740-b9fc248b270a",
   "metadata": {},
   "source": [
    "To inspect the learned parameters after training, MLJ provides several options.\n",
    "\n",
    "One particularly interesting case is with SVMs, where it is useful to check which instances were selected as support vectors.  \n",
    "This can be done in two ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12558b7d-ace7-49fa-895a-193e52f5df14",
   "metadata": {},
   "outputs": [],
   "source": [
    "mach.fitresult[1].SVs.indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef68586-6d89-4d24-baa4-81f3bb8a647d",
   "metadata": {},
   "source": [
    "or using the higher-level MLJ interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6475af-37d1-430f-8b54-0956955a378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_params(mach)[:libsvm_model].SVs.indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f10de28-9d84-4634-b0a1-7eb309d92900",
   "metadata": {},
   "source": [
    "These commands return the indices of the support vectors in the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef0987a-95ed-45fe-b197-c56a137b4694",
   "metadata": {},
   "source": [
    "In this notebook, the task will be to develop a single function that allows training the three different models using the MLJ library, and, in addition, artificial neural networks (ANNs) using the functions developed in previous exercises.\n",
    "\n",
    "The training will be performed using cross-validation. For each fold, the specified model will be trained, and metrics will be computed on the test set.\n",
    "\n",
    "As in the previous exercise, it is useful to generate a confusion matrix that reflects the distribution of instances across the test sets. In this case, it is simpler than before because the methods used are deterministic, so only one confusion matrix will be created per fold, and the final confusion matrix will be the sum of all of them.\n",
    "\n",
    "Nevertheless, the considerations from the previous exercise still apply — in particular, that the metrics derived from this global confusion matrix may not match the metrics obtained through cross-validation.\n",
    "\n",
    "In this exercise, you will develop a single function called `modelCrossValidation` that, in addition to training artificial neural networks (ANNs), performs cross-validation for SVMs, decision trees, and kNN.\n",
    "\n",
    "The function should receive the following arguments:\n",
    "\n",
    "- **`modelType::Symbol`**: This parameter indicates the type of model to train. It should take one of the following values:\n",
    "  - `:ANN` — Artificial Neural Network\n",
    "  - `:SVC` — Support Vector Machine\n",
    "  - `:DecisionTreeClassifier` — Decision Trees\n",
    "  - `:KNeighborsClassifier` — k-Nearest Neighbors\n",
    "\n",
    "- **`modelHyperparameters::Dict`**: A dictionary containing the model's hyperparameters. Keys may be of type `String` or `Symbol`.\n",
    "  \n",
    "  To check whether a hyperparameter is defined, you can use `haskey`.  \n",
    "  To retrieve a value that may or may not exist in the dictionary, the `get` function is also useful.\n",
    "\n",
    "  - **ANN (`:ANN`)**:\n",
    "        The expected Hyperparameters are\n",
    "        - Topology (number of hidden layers and number of neurons in each hidden layer, required) and transfer funtion in each layer. In \"shallow\" networks such as those used in this course, the transfer function has less impact, so a standard one, shuch as `tansig` or `logsig`, can be used.\n",
    "        - Learning rate\n",
    "        - Ratio of patterns used for validation\n",
    "        - Number of consecutive iterations without improving the validation loss to stop the process\n",
    "        - Number of times each ANN is trained.\n",
    "        \n",
    "### Question 6.4    \n",
    "> ❓ Why should a linear transfer function not be used for neurons in the hidden layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606c790d-d2ef-4044-87cf-1e999c5e57b0",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "As we now from a teory class, multiple layers using linear activation functions can always be reduced to a single layer, and this is mathematically proven. Using linear transfer functions in hidden layers would make the neural network equivalent to a simple linear model, eliminating all the advantages of deep learning. Non-linear activation functions are what let neural networks learn complex patterns in data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecaf97a-afd8-4a4d-8027-5730da4efc7b",
   "metadata": {},
   "source": [
    "  For the other models, the expected hyperparameters are:\n",
    "\n",
    "  - **SVM (`:SVC`)**:  \n",
    "    The expected hyperparameters are:\n",
    "    - `C`\n",
    "    - `kernel`\n",
    "    - `degree`\n",
    "    - `gamma`\n",
    "    - `coef0`\n",
    "\n",
    "    The `kernel` parameter should be provided as a `String` with one of the following values:\n",
    "  `\"linear\"`, `\"rbf\"`, `\"sigmoid\"`, or `\"poly\"`.\n",
    "\n",
    "    Depending on the selected kernel, some of the hyperparameters may be ignored. For example:\n",
    "    - The `\"poly\"` kernel uses `degree`, `gamma`, and `coef0`.\n",
    "    - The `\"sigmoid\"` kernel uses `gamma` and `coef0`.\n",
    "    - The `\"linear\"` kernel only uses `C`.\n",
    "\n",
    "    The `C` hyperparameter must be passed using the keyword `cost`, and the kernel must be translated to one of the predefined constants in the `LIBSVM` library:\n",
    "\n",
    "    - `LIBSVM.Kernel.Linear`\n",
    "    - `LIBSVM.Kernel.RadialBasis`\n",
    "    - `LIBSVM.Kernel.Sigmoid`\n",
    "    - `LIBSVM.Kernel.Polynomial`\n",
    "\n",
    "    To avoid type errors, it is recommended to cast each value explicitly.  \n",
    "    For example, to create a polynomial SVM:\n",
    "\n",
    "  ```julia\n",
    "    model = SVMClassifier(\n",
    "        kernel = LIBSVM.Kernel.Polynomial,\n",
    "        cost = Float64(C),\n",
    "        gamma = Float64(gamma),\n",
    "        degree = Int32(degree),\n",
    "        coef0 = Float64(coef0)\n",
    "    )\n",
    "  ```\n",
    "\n",
    "  - **Decision Tree (`:DecisionTreeClassifier`)**:\n",
    "\n",
    "    - `max_depth`: defines the maximum depth of the tree.\n",
    "    - `rng`: the random seed generator. It should be set to `Random.MersenneTwister(1)` to ensure reproducibility.\n",
    "\n",
    "  - **k-Nearest Neighbors (`:KNeighborsClassifier`)**:\n",
    "    - `n_neighbors`: the value of k, which determines the number of neighbors to consider.\n",
    "\n",
    "- **`dataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{<:Any,1}}`**:  \n",
    "  A tuple containing two elements:\n",
    "  - The first is the input matrix (`X`). Unlike neural network training, there is no need to convert the data to `Float32`, since both `Float32` and `Float64` are commonly used in this library depending on the desired precision.\n",
    "  - The second is the target vector (`y`), which contains the labels.\n",
    "\n",
    "- **`crossValidationIndices::Array{Int64,1}`**:  \n",
    "  This vector contains the indices used to assign each sample to a fold in the cross-validation process.\n",
    "\n",
    "  As in the previous exercise, the fold assignment must be done **outside** the `modelCrossValidation` function.  \n",
    "  This ensures that the exact same data partitioning is used when training different models, allowing fair comparisons.\n",
    "\n",
    "\n",
    "The function will begin by checking whether the model to be trained is a neural network, by examining the `modelType` parameter.  \n",
    "If this is the case, it will call the `ANNCrossValidation` function, passing the hyperparameters provided in `modelHyperparameters`.\n",
    "\n",
    "Keep in mind that many of the hyperparameters for neural networks may not be defined in the dictionary.  \n",
    "As mentioned earlier, the function `haskey` can be used to check whether a key is present in a `Dict`.  \n",
    "Alternatively, the `get` function can be used to safely retrieve a value with a default if the key is missing.\n",
    "\n",
    "Once the call to `ANNCrossValidation` is made, the function returns its result and exits — meaning that no further processing will occur in this case.\n",
    "\n",
    "If a different type of model is to be trained, the logic continues similarly to the previous exercise:\n",
    "\n",
    "- Create seven vectors to store the results of the metrics for each fold.\n",
    "- Create a 2D array to accumulate the confusion matrix, initialized with zeros.\n",
    "\n",
    "A key modification when using models from the MLJ library is to **convert the target labels to strings** before training any model.  \n",
    "This helps prevent errors caused by internal type mismatches in some model implementations.\n",
    "\n",
    "This can be done with the following simple line:\n",
    "\n",
    "```julia\n",
    "targets = string.(targets);\n",
    "```\n",
    "\n",
    "Additionally, it will be necessary to compute the vector of unique classes, just like in the previous exercise.  \n",
    "This can be done with:\n",
    "\n",
    "```julia\n",
    "classes = unique(targets);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab6990e-eed7-4e30-9f27-756abc4e1b67",
   "metadata": {},
   "source": [
    "Once these initial steps are completed, the cross-validation loop can begin.\n",
    "\n",
    "In each iteration, the following steps are performed:\n",
    "\n",
    "1. Extract the training and test input matrices and the corresponding target vectors.  \n",
    "   These should be of type `AbstractArray{<:Any,1}` for the targets.\n",
    "\n",
    "2. Create the model with the specified hyperparameters.\n",
    "\n",
    "3. For MLJ models (SVM, Decision Tree, kNN):\n",
    "   - Instantiate the model using the appropriate constructor: `SVMClassifier`, `DTClassifier`, or `kNNClassifier`, depending on `modelType`.\n",
    "   - Wrap the model in a `machine` with the training data.\n",
    "   - Train the model using `fit!`.\n",
    "\n",
    "4. Perform predictions on the test data using `predict`.\n",
    "\n",
    "   - For Decision Trees and kNN, use `mode` to convert the probabilistic predictions into categorical labels:\n",
    "     ```julia\n",
    "     ŷ = mode.(predict(mach, MLJ.table(Xtest)))\n",
    "     ```\n",
    "\n",
    "   - For SVMs, the output of `predict` can be compared directly with the ground truth, since it returns a `CategoricalArray`.\n",
    "\n",
    "Although the general structure of the code will be the same for the three model types, each model requires a different constructor and may require post-processing (e.g., `mode`) depending on the prediction format.\n",
    "\n",
    "Once the predicted labels for the test set are available, the evaluation metrics and the confusion matrix should be computed using the `confusionMatrix` function.\n",
    "\n",
    "- The metrics returned should be stored in their respective positions within the metric vectors.\n",
    "- The confusion matrix obtained for each fold should be **added** to a global confusion matrix for the test set.\n",
    "\n",
    "A key difference compared to the ANN training in the previous exercise is that these models (SVM, Decision Tree, kNN) are **deterministic**.  \n",
    "Therefore, each model only needs to be trained **once per fold**, without requiring multiple executions or averaging across runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0f9e3d-c590-49c8-968a-eb2d8e27f0b7",
   "metadata": {},
   "source": [
    "   ### Question 6.5\n",
    "   > ❓ The other models do not have the number of times to train them as a parameter. Why? If you train several times, Which statistical properties will the results of these trainings have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229dc093-08a0-41e5-9f35-47ef962694e9",
   "metadata": {},
   "source": [
    "**Answer**  \n",
    "Because other models are deterministic and they give the very same result having same given the same input, so retraining is pointless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedae31a-0e8c-4b41-a10b-934e4b6af791",
   "metadata": {},
   "source": [
    "As previously described, when using techniques such as SVM, decision trees, kNN, **one-hot encoding is not used**.  \n",
    "Instead, metrics are computed using the `confusionMatrix` function developed in a previous exercise, which takes three arguments:\n",
    "- The predicted labels\n",
    "- The true labels\n",
    "- The list of class labels\n",
    "\n",
    "All of these must be of type `AbstractArray{<:Any,1}`.\n",
    "\n",
    "It is important to use the version of the `confusionMatrix` function that receives the vector of classes.\n",
    "\n",
    "### Question 6.6\n",
    "\n",
    "> ❓ What could happen if the version that does not receive the class vector is used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1434e675-c83c-48d1-9939-8bd784178170",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "When performing cross-validation we divide the data is divided into N folds. In a multi-class problem there is a chance that one of the test folds will miss one or more classes. Then when the code tries to accumulate the global confusion matrix it will be dimension mismatch.  \n",
    "Using the two-argument version (without the list of class labels) is dangerous because it determines the size and order of classes only based on the current fold, leading to dimensionality mismatch errors if a class is missing. Explicitly passing the full list of classes ensures that all confusion matrices have the same structure and allows them to be correctly summed into a global cross-validation matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a078f16-01c0-4719-9aef-96f19a751533",
   "metadata": {},
   "source": [
    "The `modelCrossValidation` function must return the same structure as in the previous exercise: a tuple with 8 elements.\n",
    "\n",
    "- The first **7 elements** correspond to metrics: **accuracy**, **error rate**, **recall**, **specificity**, **PPV**, **NPV**, and **F1-score**.  \n",
    "  Each of these is itself a tuple with the **mean** and **standard deviation** across the folds.\n",
    "\n",
    "- The **8th element** is the **global confusion matrix** computed on the test sets.\n",
    "\n",
    "Once the function has been developed, it can be used to evaluate different model configurations by comparing test results across the selected metrics.  \n",
    "This process does **not return a final model ready for production**, but rather identifies the best-performing model type and hyperparameter configuration.\n",
    "\n",
    "After selecting the best configuration, the final model should be trained **from scratch** using **all available data**, without performing cross-validation.  \n",
    "This training is done just once, without setting aside a test set.  \n",
    "As a result, the final model is expected to perform slightly better than during cross-validation, since it benefits from more training data.\n",
    "\n",
    "This final model is the one intended for production use, and a confusion matrix can be computed for it as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15eecf3-286c-40ee-9ed7-2920c62e4f3a",
   "metadata": {},
   "source": [
    "### Question 6.7\n",
    "\n",
    "> ❓ In the case of using decision trees or kNN, a corresponding function is not necessary to perform the \"one-against-all\" strategy, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e501314",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "Because decision trees and kNN can directly handle multiple classes without converting the problem into binary form.  \n",
    "- In decision trees, each split is chosen to maximize class separation across all existing classes, not just two. The tree grows branches that directly lead to leaves labeled with any of the possible classes. Thus, it inherently learns to distinguish among all L classes simultaneously.\n",
    "- In kNN, classification is based on a majority vote among the k nearest samples, which can belong to any class. The algorithm simply counts the class frequencies and assigns the label of the most frequent one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17e6c492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modelCrossValidation (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function modelCrossValidation(\n",
    "        modelType::Symbol, modelHyperparameters::Dict,\n",
    "        dataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{<:Any,1}},\n",
    "        crossValidationIndices::Array{Int64,1})\n",
    "    \n",
    "    # Helper function to get parameter value with flexible key types (String or Symbol)\n",
    "    function getParam(dict, key_str, key_sym, default)\n",
    "        if haskey(dict, key_str)\n",
    "            return dict[key_str]\n",
    "        elseif haskey(dict, key_sym)\n",
    "            return dict[key_sym]\n",
    "        else\n",
    "            return default\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Normalize model type symbols - accept both variants\n",
    "    normalizedModelType = modelType\n",
    "    if modelType == :SVC\n",
    "        normalizedModelType = :SVMClassifier\n",
    "    elseif modelType == :kNN\n",
    "        normalizedModelType = :KNeighborsClassifier\n",
    "    end\n",
    "    \n",
    "    # Handle ANN case - delegate to ANNCrossValidation\n",
    "    if normalizedModelType == :ANN\n",
    "        # Extract ANN hyperparameters\n",
    "        topology = getParam(modelHyperparameters, \"topology\", :topology, nothing)\n",
    "        @assert topology !== nothing \"Topology is required for ANN\"\n",
    "        \n",
    "        learningRate = getParam(modelHyperparameters, \"learningRate\", :learningRate, 0.01)\n",
    "        validationRatio = getParam(modelHyperparameters, \"validationRatio\", :validationRatio, 0.0)\n",
    "        numExecutions = getParam(modelHyperparameters, \"numExecutions\", :numExecutions, 50)\n",
    "        maxEpochs = getParam(modelHyperparameters, \"maxEpochs\", :maxEpochs, 1000)\n",
    "        maxEpochsVal = getParam(modelHyperparameters, \"maxEpochsVal\", :maxEpochsVal, 20)\n",
    "        \n",
    "        # Get transfer functions if provided\n",
    "        transferFunctions = getParam(modelHyperparameters, \"transferFunctions\", :transferFunctions, fill(σ, length(topology)))\n",
    "        \n",
    "        # Call ANNCrossValidation\n",
    "        return ANNCrossValidation(\n",
    "            topology,\n",
    "            dataset,\n",
    "            crossValidationIndices;\n",
    "            numExecutions = numExecutions,\n",
    "            transferFunctions = transferFunctions,\n",
    "            maxEpochs = maxEpochs,\n",
    "            minLoss = 0.0,\n",
    "            learningRate = learningRate,\n",
    "            validationRatio = validationRatio,\n",
    "            maxEpochsVal = maxEpochsVal\n",
    "        )\n",
    "    end\n",
    "    \n",
    "    # Handle MLJ models (SVM, Decision Tree, kNN)\n",
    "    inputs, targets = dataset\n",
    "    \n",
    "    # Convert targets to strings to prevent type issues\n",
    "    targets = string.(targets)\n",
    "    \n",
    "    # Get unique classes\n",
    "    classes = unique(targets)\n",
    "    \n",
    "    # Initialize metric vectors for each fold\n",
    "    accuracy_folds = Float64[]\n",
    "    error_folds = Float64[]\n",
    "    sensitivity_folds = Float64[]\n",
    "    specificity_folds = Float64[]\n",
    "    ppv_folds = Float64[]\n",
    "    npv_folds = Float64[]\n",
    "    f1_folds = Float64[]\n",
    "    \n",
    "    # Initialize global confusion matrix\n",
    "    global_confusion = zeros(Int64, length(classes), length(classes))\n",
    "    \n",
    "    # Get number of folds\n",
    "    numFolds = maximum(crossValidationIndices)\n",
    "    \n",
    "    # Cross-validation loop\n",
    "    for fold in 1:numFolds\n",
    "        # Split data into train and test for this fold\n",
    "        test_mask = crossValidationIndices .== fold\n",
    "        train_mask = .!test_mask\n",
    "        \n",
    "        train_inputs = inputs[train_mask, :]\n",
    "        train_targets = targets[train_mask]\n",
    "        test_inputs = inputs[test_mask, :]\n",
    "        test_targets = targets[test_mask]\n",
    "        \n",
    "        # Create model based on model type\n",
    "        model = nothing\n",
    "        \n",
    "        if normalizedModelType == :SVMClassifier\n",
    "            # Extract SVM hyperparameters\n",
    "            C = getParam(modelHyperparameters, \"C\", :C, 1.0)\n",
    "            kernel_str = getParam(modelHyperparameters, \"kernel\", :kernel, \"rbf\")\n",
    "            gamma = getParam(modelHyperparameters, \"gamma\", :gamma, 0.1)\n",
    "            degree = getParam(modelHyperparameters, \"degree\", :degree, 3)\n",
    "            coef0 = getParam(modelHyperparameters, \"coef0\", :coef0, 0.0)\n",
    "            \n",
    "            # Map kernel string to LIBSVM kernel type\n",
    "            kernel = if kernel_str == \"linear\"\n",
    "                LIBSVM.Kernel.Linear\n",
    "            elseif kernel_str == \"rbf\"\n",
    "                LIBSVM.Kernel.RadialBasis\n",
    "            elseif kernel_str == \"sigmoid\"\n",
    "                LIBSVM.Kernel.Sigmoid\n",
    "            elseif kernel_str == \"poly\" || kernel_str == \"polynomial\"\n",
    "                LIBSVM.Kernel.Polynomial\n",
    "            else\n",
    "                LIBSVM.Kernel.RadialBasis  # default\n",
    "            end\n",
    "            \n",
    "            # Create SVM model with kernel-specific parameters\n",
    "        model = if kernel_str == \"linear\"\n",
    "            # Linear kernel: only C\n",
    "            SVMClassifier(\n",
    "                kernel = LIBSVM.Kernel.Linear,\n",
    "                cost = Float64(C)\n",
    "            )\n",
    "        elseif kernel_str == \"rbf\"\n",
    "            # RBF kernel: C and gamma\n",
    "            SVMClassifier(\n",
    "                kernel = LIBSVM.Kernel.RadialBasis,\n",
    "                cost = Float64(C),\n",
    "                gamma = Float64(gamma)\n",
    "            )\n",
    "        elseif kernel_str == \"sigmoid\"\n",
    "            # Sigmoid kernel: C, gamma, and coef0\n",
    "            SVMClassifier(\n",
    "                kernel = LIBSVM.Kernel.Sigmoid,\n",
    "                cost = Float64(C),\n",
    "                gamma = Float64(gamma),\n",
    "                coef0 = Float64(coef0)\n",
    "            )\n",
    "        elseif kernel_str == \"poly\" || kernel_str == \"polynomial\"\n",
    "            # Polynomial kernel: C, degree, gamma, and coef0\n",
    "            SVMClassifier(\n",
    "                kernel = LIBSVM.Kernel.Polynomial,\n",
    "                cost = Float64(C),\n",
    "                gamma = Float64(gamma),\n",
    "                degree = Int32(degree),\n",
    "                coef0 = Float64(coef0)\n",
    "            )\n",
    "        else\n",
    "            # Default to RBF\n",
    "            SVMClassifier(\n",
    "                kernel = LIBSVM.Kernel.RadialBasis,\n",
    "                cost = Float64(C),\n",
    "                gamma = Float64(gamma)\n",
    "            )\n",
    "        end\n",
    "            \n",
    "        elseif normalizedModelType == :DecisionTreeClassifier\n",
    "            # Extract Decision Tree hyperparameters\n",
    "            max_depth = getParam(modelHyperparameters, \"max_depth\", :max_depth, -1)\n",
    "            \n",
    "            # Create Decision Tree model with fixed random seed for reproducibility\n",
    "            model = DTClassifier(\n",
    "                max_depth = max_depth,\n",
    "                rng = Random.MersenneTwister(1)\n",
    "            )\n",
    "            \n",
    "        elseif normalizedModelType == :KNeighborsClassifier\n",
    "            # Extract kNN hyperparameters\n",
    "            K = getParam(modelHyperparameters, \"K\", :K, 3)\n",
    "            n_neighbors = getParam(modelHyperparameters, \"n_neighbors\", :n_neighbors, K)\n",
    "            \n",
    "            # Create kNN model\n",
    "            model = kNNClassifier(K = n_neighbors)\n",
    "            \n",
    "        else\n",
    "            error(\"Unknown model type: $normalizedModelType\")\n",
    "        end\n",
    "        \n",
    "        # Create machine object with training data\n",
    "        mach = machine(model, MLJ.table(train_inputs), categorical(train_targets))\n",
    "        \n",
    "        # Train the model\n",
    "        MLJ.fit!(mach, verbosity=0)\n",
    "        \n",
    "        # Make predictions on test set\n",
    "        if normalizedModelType == :SVMClassifier\n",
    "            # SVM returns CategoricalArray directly\n",
    "            predictions = MLJ.predict(mach, MLJ.table(test_inputs))\n",
    "            predictions = string.(predictions)  # Convert to strings for consistency\n",
    "        else\n",
    "            # Decision Tree and kNN return UnivariateFiniteArray\n",
    "            # Need to use mode to get the most likely class\n",
    "            predictions = mode.(MLJ.predict(mach, MLJ.table(test_inputs)))\n",
    "            predictions = string.(predictions)  # Convert to strings for consistency\n",
    "        end\n",
    "        \n",
    "        # Compute confusion matrix and metrics for this fold\n",
    "        metrics = confusionMatrix(predictions, test_targets, classes)\n",
    "        acc, err, sens, spec, ppv, npv, f1, confusion = metrics\n",
    "        \n",
    "        # Store metrics\n",
    "        push!(accuracy_folds, acc)\n",
    "        push!(error_folds, err)\n",
    "        push!(sensitivity_folds, sens)\n",
    "        push!(specificity_folds, spec)\n",
    "        push!(ppv_folds, ppv)\n",
    "        push!(npv_folds, npv)\n",
    "        push!(f1_folds, f1)\n",
    "        \n",
    "        # Accumulate confusion matrix\n",
    "        global_confusion .+= confusion\n",
    "    end\n",
    "    \n",
    "    # Compute mean and standard deviation for each metric\n",
    "    stats(vector) = (mean(vector), std(vector; corrected=false))\n",
    "    \n",
    "    # Return results in the same format as ANNCrossValidation\n",
    "    return (stats(accuracy_folds),\n",
    "            stats(error_folds),\n",
    "            stats(sensitivity_folds),\n",
    "            stats(specificity_folds),\n",
    "            stats(ppv_folds),\n",
    "            stats(npv_folds),\n",
    "            stats(f1_folds),\n",
    "            global_confusion)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a7ded7",
   "metadata": {},
   "source": [
    "### Learn Julia\n",
    "\n",
    "#### Symbols and Dictionaries in Julia\n",
    "One Julia type that is important for this exercise is the `Symbol` type. An object of this type can be any symbol you want, simply by typing its name after a colon (\":\"). In this practice, you can use it to indicate which model you want to train, for example, in the `modelCrossValidation` function, symbols will be used to indicate which model to train:\n",
    "\n",
    "```julia\n",
    ":KNeighborsClassifier, :SVC, :DecisionTreeClassifier, :ANN\n",
    "```\n",
    "\n",
    "#### Passing Model-Specific Parameters\n",
    "This function will also require model-specific parameters to be passed.\n",
    "The recommended way to do this is to define a variable of type Dict, which works similarly to Python dictionaries.\n",
    "\n",
    "For instance, to define the hyperparameters for an artificial neural network (ANN):\n",
    "\n",
    "```julia\n",
    "  modelHyperparameters = Dict(\n",
    "      \"topology\" => [5, 3],\n",
    "      \"learningRate\" => 0.01,\n",
    "      \"validationRatio\" => 0.2,\n",
    "      \"numExecutions\" => 50,\n",
    "      \"maxEpochs\" => 1000,\n",
    "      \"maxEpochsVal\" => 6\n",
    "  )\n",
    "```\n",
    "\n",
    "Another way to define the same dictionary:\n",
    "\n",
    "```julia\n",
    "  modelHyperparameters = Dict()\n",
    "  modelHyperparameters[\"topology\"] = topology\n",
    "  modelHyperparameters[\"learningRate\"] = learningRate\n",
    "  modelHyperparameters[\"validationRatio\"] = validationRatio\n",
    "  modelHyperparameters[\"numExecutions\"] = numRepetitionsANNTraining\n",
    "  modelHyperparameters[\"maxEpochs\"] = numMaxEpochs\n",
    "  modelHyperparameters[\"maxEpochsVal\"] = maxEpochsVal\n",
    "```\n",
    "To access a value, simply use:\n",
    "```julia\n",
    "  modelHyperparameters[\"topology\"]\n",
    "```\n",
    "#### Example for SVM Parameters\n",
    "You can also define hyperparameters for other models similarly.\n",
    "For example, for an SVM:\n",
    "```julia\n",
    "  modelHyperparameters = Dict(\"C\" => 1, \"kernel\" => \"rbf\", \"gamma\" => 2)\n",
    "```\n",
    "Or using the alternative form:\n",
    "\n",
    "```julia\n",
    "  modelHyperparameters = Dict()\n",
    "  modelHyperparameters[\"C\"] = 1\n",
    "  modelHyperparameters[\"kernel\"] = \"rbf\"\n",
    "  modelHyperparameters[\"gamma\"] = 2\n",
    "```\n",
    "Other kernels may require different parameters, such as `degree` and `coef0`.\n",
    "\n",
    "When building the SVM model inside the function, you might write:\n",
    "```julia\n",
    "  if modelHyperparameters[\"kernel\"] == \"rbf\"\n",
    "    model = SVMClassifier(\n",
    "        kernel = LIBSVM.Kernel.RadialBasis,\n",
    "        cost = Float64(modelHyperparameters[\"C\"]),\n",
    "        gamma = Float64(modelHyperparameters[\"gamma\"])\n",
    "    )\n",
    "```\n",
    "\n",
    "You can apply a similar strategy for decision trees, kNN, and DoME models.\n",
    "\n",
    "In the examples above, the dictionary keys are `String`, but you may also use `Symbol` keys interchangeably.\n",
    "For example:\n",
    "```julia\n",
    "  modelHyperparameters = Dict(:C => 1, :kernel => \"rbf\", :gamma => 2)\n",
    "```\n",
    "\n",
    "Another type of Julia that may be interesting for this assignment is the `Symbol` type. An object of this type can be any symbol you want, simply by typing its name after a colon (\":\"). In this practice, you can use it to indicate which model you want to train, for example `:ANN`, `:SVM`, `:DecisionTree` or `:kNN`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f98668-1639-4bf8-8e34-852f1b53467c",
   "metadata": {},
   "source": [
    "**Test of modelCrossValidation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdc01a9-82ae-4c6c-af81-55eda562c1c3",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 150 samples, 4 features\n",
      "Classes: Any[\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"]\n",
      "Cross-validation folds: 5\n",
      "\n",
      "============================================================\n",
      "Test 1: SVM with Linear kernel\n",
      "============================================================\n",
      "Accuracy: 0.96 ± 0.0389\n",
      "Error Rate: 0.04 ± 0.0389\n",
      "Sensitivity: 0.96 ± 0.0389\n",
      "Specificity: 0.9811 ± 0.0202\n",
      "PPV: 0.9634 ± 0.0372\n",
      "NPV: 0.9824 ± 0.0167\n",
      "F1-Score: 0.9596 ± 0.0393\n",
      "Confusion Matrix:\n",
      "[50 0 0; 0 48 2; 0 4 46]\n",
      "\n",
      "============================================================\n",
      "Test 2: SVM with RBF kernel\n",
      "============================================================\n",
      "Accuracy: 0.9133 ± 0.0581\n",
      "Error Rate: 0.0867 ± 0.0581\n",
      "Sensitivity: 0.9133 ± 0.0581\n",
      "Specificity: 0.9664 ± 0.0225\n",
      "PPV: 0.9326 ± 0.0452\n",
      "NPV: 0.9502 ± 0.0428\n",
      "F1-Score: 0.9143 ± 0.0567\n",
      "Confusion Matrix:\n",
      "[50 0 0; 0 46 4; 0 9 41]\n",
      "\n",
      "============================================================\n",
      "Test 3: SVM with Sigmoid kernel\n",
      "============================================================\n",
      "Accuracy: 0.7333 ± 0.1366\n",
      "Error Rate: 0.2667 ± 0.1366\n",
      "Sensitivity: 0.7333 ± 0.1366\n",
      "Specificity: 0.9085 ± 0.0362\n",
      "PPV: 0.7656 ± 0.2167\n",
      "NPV: 0.8734 ± 0.069\n",
      "F1-Score: 0.6999 ± 0.1787\n",
      "Confusion Matrix:\n",
      "[50 0 0; 0 31 19; 0 21 29]\n",
      "\n",
      "============================================================\n",
      "Test 4: SVM with Polynomial kernel\n",
      "============================================================\n",
      "Accuracy: 0.9267 ± 0.0533\n",
      "Error Rate: 0.0733 ± 0.0533\n",
      "Sensitivity: 0.9267 ± 0.0533\n",
      "Specificity: 0.9696 ± 0.0234\n",
      "PPV: 0.9408 ± 0.0438\n",
      "NPV: 0.9588 ± 0.037\n",
      "F1-Score: 0.9268 ± 0.0529\n",
      "Confusion Matrix:\n",
      "[50 0 0; 0 48 2; 0 9 41]\n",
      "\n",
      "============================================================\n",
      "Test 5: Decision Tree (max_depth=4)\n",
      "============================================================\n",
      "Accuracy: 0.9267 ± 0.0533\n",
      "Error Rate: 0.0733 ± 0.0533\n",
      "Sensitivity: 0.9267 ± 0.0533\n",
      "Specificity: 0.9676 ± 0.0267\n",
      "PPV: 0.9297 ± 0.0542\n",
      "NPV: 0.9611 ± 0.0211\n",
      "F1-Score: 0.9267 ± 0.0536\n",
      "Confusion Matrix:\n",
      "[50 0 0; 0 45 5; 0 6 44]\n",
      "\n",
      "============================================================\n",
      "Test 6: k-Nearest Neighbors (K=3)\n",
      "============================================================\n",
      "Accuracy: 0.96 ± 0.0389\n",
      "Error Rate: 0.04 ± 0.0389\n",
      "Sensitivity: 0.96 ± 0.0389\n",
      "Specificity: 0.9825 ± 0.019\n",
      "PPV: 0.9613 ± 0.0385\n",
      "NPV: 0.9805 ± 0.0176\n",
      "F1-Score: 0.96 ± 0.0391\n",
      "Confusion Matrix:\n",
      "[50 0 0; 0 47 3; 0 3 47]\n",
      "\n",
      "============================================================\n",
      "Test 7: ANN with topology [4, 3]\n",
      "============================================================\n",
      "Accuracy: 0.468 ± 0.0408\n",
      "Error Rate: 0.532 ± 0.0408\n",
      "Sensitivity: 0.468 ± 0.0408\n",
      "Specificity: 0.7602 ± 0.0225\n",
      "PPV: 0.3177 ± 0.0441\n",
      "NPV: 0.5839 ± 0.026\n",
      "F1-Score: 0.3532 ± 0.0444\n",
      "Confusion Matrix:\n",
      "[27.9 6.500000000000001 15.600000000000001; 15.5 16.3 18.2; 16.6 7.4 26.0]\n",
      "\n",
      "===========================================================================\n",
      "Model Comparison Summary\n",
      "===========================================================================\n",
      "Model                   Accuracy (Mean ± Std)         F1-Score (Mean ± Std)\n",
      "---------------------------------------------------------------------------\n",
      "SVM (Linear)           0.96 ± 0.0389   0.9596 ± 0.0393\n",
      "SVM (RBF)              0.9133 ± 0.0581   0.9143 ± 0.0567\n",
      "SVM (Sigmoid)          0.7333 ± 0.1366   0.6999 ± 0.1787\n",
      "SVM (Polynomial)       0.9267 ± 0.0533   0.9268 ± 0.0529\n",
      "Decision Tree          0.9267 ± 0.0533   0.9267 ± 0.0536\n",
      "k-NN (K=3)             0.96 ± 0.0389   0.96 ± 0.0391\n",
      "ANN [4,3]              0.468 ± 0.0408   0.3532 ± 0.0444\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "### Test the modelCrossValidation function with the iris dataset\n",
    "\n",
    "# Load the iris dataset\n",
    "using DelimitedFiles\n",
    "\n",
    "# Read the iris data\n",
    "data = readdlm(\"iris.data\", ',')\n",
    "\n",
    "# Extract inputs (first 4 columns) and targets (last column)\n",
    "inputs = Float64.(data[:, 1:4])\n",
    "targets = data[:, 5]\n",
    "\n",
    "# Normalize the inputs\n",
    "inputs = normalizeMinMax(inputs)\n",
    "\n",
    "# Create cross-validation indices (5-fold cross-validation)\n",
    "using Random\n",
    "Random.seed!(42)  # For reproducibility\n",
    "numPatterns = size(inputs, 1)\n",
    "numFolds = 5\n",
    "crossValidationIndices = repeat(1:numFolds, outer=ceil(Int, numPatterns/numFolds))[1:numPatterns]\n",
    "crossValidationIndices = crossValidationIndices[randperm(numPatterns)]\n",
    "\n",
    "println(\"Dataset loaded: $(size(inputs, 1)) samples, $(size(inputs, 2)) features\")\n",
    "println(\"Classes: $(unique(targets))\")\n",
    "println(\"Cross-validation folds: $numFolds\")\n",
    "println()\n",
    "\n",
    "\n",
    "# Test 1: SVM with Linear kernel\n",
    "println(\"=\" ^ 60)\n",
    "println(\"Test 1: SVM with Linear kernel\")\n",
    "println(\"=\" ^ 60)\n",
    "svm_linear_params = Dict(\n",
    "    \"C\" => 1.0,\n",
    "    \"kernel\" => \"linear\"\n",
    ")\n",
    "\n",
    "results_svm_linear = modelCrossValidation(\n",
    "    :SVC,\n",
    "    svm_linear_params,\n",
    "    (inputs, targets),\n",
    "    crossValidationIndices\n",
    ")\n",
    "\n",
    "acc, err, sens, spec, ppv, npv, f1, conf = results_svm_linear\n",
    "println(\"Accuracy: $(round(acc[1], digits=4)) ± $(round(acc[2], digits=4))\")\n",
    "println(\"Error Rate: $(round(err[1], digits=4)) ± $(round(err[2], digits=4))\")\n",
    "println(\"Sensitivity: $(round(sens[1], digits=4)) ± $(round(sens[2], digits=4))\")\n",
    "println(\"Specificity: $(round(spec[1], digits=4)) ± $(round(spec[2], digits=4))\")\n",
    "println(\"PPV: $(round(ppv[1], digits=4)) ± $(round(ppv[2], digits=4))\")\n",
    "println(\"NPV: $(round(npv[1], digits=4)) ± $(round(npv[2], digits=4))\")\n",
    "println(\"F1-Score: $(round(f1[1], digits=4)) ± $(round(f1[2], digits=4))\")\n",
    "println(\"Confusion Matrix:\")\n",
    "println(conf)\n",
    "println()\n",
    "\n",
    "# Test 2: SVM with RBF kernel\n",
    "println(\"=\" ^ 60)\n",
    "println(\"Test 2: SVM with RBF kernel\")\n",
    "println(\"=\" ^ 60)\n",
    "svm_rbf_params = Dict(\n",
    "    \"C\" => 1.0,\n",
    "    \"kernel\" => \"rbf\",\n",
    "    \"gamma\" => 0.1\n",
    ")\n",
    "\n",
    "results_svm_rbf = modelCrossValidation(\n",
    "    :SVC,\n",
    "    svm_rbf_params,\n",
    "    (inputs, targets),\n",
    "    crossValidationIndices\n",
    ")\n",
    "\n",
    "acc, err, sens, spec, ppv, npv, f1, conf = results_svm_rbf\n",
    "println(\"Accuracy: $(round(acc[1], digits=4)) ± $(round(acc[2], digits=4))\")\n",
    "println(\"Error Rate: $(round(err[1], digits=4)) ± $(round(err[2], digits=4))\")\n",
    "println(\"Sensitivity: $(round(sens[1], digits=4)) ± $(round(sens[2], digits=4))\")\n",
    "println(\"Specificity: $(round(spec[1], digits=4)) ± $(round(spec[2], digits=4))\")\n",
    "println(\"PPV: $(round(ppv[1], digits=4)) ± $(round(ppv[2], digits=4))\")\n",
    "println(\"NPV: $(round(npv[1], digits=4)) ± $(round(npv[2], digits=4))\")\n",
    "println(\"F1-Score: $(round(f1[1], digits=4)) ± $(round(f1[2], digits=4))\")\n",
    "println(\"Confusion Matrix:\")\n",
    "println(conf)\n",
    "println()\n",
    "\n",
    "# Test 3: SVM with Sigmoid kernel\n",
    "println(\"=\" ^ 60)\n",
    "println(\"Test 3: SVM with Sigmoid kernel\")\n",
    "println(\"=\" ^ 60)\n",
    "svm_sigmoid_params = Dict(\n",
    "    \"C\" => 1.0,\n",
    "    \"kernel\" => \"sigmoid\",\n",
    "    \"gamma\" => 0.1,\n",
    "    \"coef0\" => 0.0\n",
    ")\n",
    "\n",
    "results_svm_sigmoid = modelCrossValidation(\n",
    "    :SVC,\n",
    "    svm_sigmoid_params,\n",
    "    (inputs, targets),\n",
    "    crossValidationIndices\n",
    ")\n",
    "\n",
    "acc, err, sens, spec, ppv, npv, f1, conf = results_svm_sigmoid\n",
    "println(\"Accuracy: $(round(acc[1], digits=4)) ± $(round(acc[2], digits=4))\")\n",
    "println(\"Error Rate: $(round(err[1], digits=4)) ± $(round(err[2], digits=4))\")\n",
    "println(\"Sensitivity: $(round(sens[1], digits=4)) ± $(round(sens[2], digits=4))\")\n",
    "println(\"Specificity: $(round(spec[1], digits=4)) ± $(round(spec[2], digits=4))\")\n",
    "println(\"PPV: $(round(ppv[1], digits=4)) ± $(round(ppv[2], digits=4))\")\n",
    "println(\"NPV: $(round(npv[1], digits=4)) ± $(round(npv[2], digits=4))\")\n",
    "println(\"F1-Score: $(round(f1[1], digits=4)) ± $(round(f1[2], digits=4))\")\n",
    "println(\"Confusion Matrix:\")\n",
    "println(conf)\n",
    "println()\n",
    "\n",
    "# Test 4: SVM with Polynomial kernel\n",
    "println(\"=\" ^ 60)\n",
    "println(\"Test 4: SVM with Polynomial kernel\")\n",
    "println(\"=\" ^ 60)\n",
    "svm_poly_params = Dict(\n",
    "    \"C\" => 1.0,\n",
    "    \"kernel\" => \"poly\",\n",
    "    \"degree\" => 3,\n",
    "    \"gamma\" => 0.1,\n",
    "    \"coef0\" => 1.0\n",
    ")\n",
    "\n",
    "results_svm_poly = modelCrossValidation(\n",
    "    :SVC,\n",
    "    svm_poly_params,\n",
    "    (inputs, targets),\n",
    "    crossValidationIndices\n",
    ")\n",
    "\n",
    "acc, err, sens, spec, ppv, npv, f1, conf = results_svm_poly\n",
    "println(\"Accuracy: $(round(acc[1], digits=4)) ± $(round(acc[2], digits=4))\")\n",
    "println(\"Error Rate: $(round(err[1], digits=4)) ± $(round(err[2], digits=4))\")\n",
    "println(\"Sensitivity: $(round(sens[1], digits=4)) ± $(round(sens[2], digits=4))\")\n",
    "println(\"Specificity: $(round(spec[1], digits=4)) ± $(round(spec[2], digits=4))\")\n",
    "println(\"PPV: $(round(ppv[1], digits=4)) ± $(round(ppv[2], digits=4))\")\n",
    "println(\"NPV: $(round(npv[1], digits=4)) ± $(round(npv[2], digits=4))\")\n",
    "println(\"F1-Score: $(round(f1[1], digits=4)) ± $(round(f1[2], digits=4))\")\n",
    "println(\"Confusion Matrix:\")\n",
    "println(conf)\n",
    "println()\n",
    "\n",
    "# Test 5: Decision Tree\n",
    "println(\"=\" ^ 60)\n",
    "println(\"Test 5: Decision Tree (max_depth=4)\")\n",
    "println(\"=\" ^ 60)\n",
    "dt_hyperparameters = Dict(\n",
    "    \"max_depth\" => 4\n",
    ")\n",
    "\n",
    "results_dt = modelCrossValidation(\n",
    "    :DecisionTreeClassifier,\n",
    "    dt_hyperparameters,\n",
    "    (inputs, targets),\n",
    "    crossValidationIndices\n",
    ")\n",
    "\n",
    "acc, err, sens, spec, ppv, npv, f1, conf = results_dt\n",
    "println(\"Accuracy: $(round(acc[1], digits=4)) ± $(round(acc[2], digits=4))\")\n",
    "println(\"Error Rate: $(round(err[1], digits=4)) ± $(round(err[2], digits=4))\")\n",
    "println(\"Sensitivity: $(round(sens[1], digits=4)) ± $(round(sens[2], digits=4))\")\n",
    "println(\"Specificity: $(round(spec[1], digits=4)) ± $(round(spec[2], digits=4))\")\n",
    "println(\"PPV: $(round(ppv[1], digits=4)) ± $(round(ppv[2], digits=4))\")\n",
    "println(\"NPV: $(round(npv[1], digits=4)) ± $(round(npv[2], digits=4))\")\n",
    "println(\"F1-Score: $(round(f1[1], digits=4)) ± $(round(f1[2], digits=4))\")\n",
    "println(\"Confusion Matrix:\")\n",
    "println(conf)\n",
    "println()\n",
    "\n",
    "# Test 6: k-Nearest Neighbors\n",
    "println(\"=\" ^ 60)\n",
    "println(\"Test 6: k-Nearest Neighbors (K=3)\")\n",
    "println(\"=\" ^ 60)\n",
    "knn_hyperparameters = Dict(\n",
    "    \"K\" => 3\n",
    ")\n",
    "\n",
    "results_knn = modelCrossValidation(\n",
    "    :KNeighborsClassifier,\n",
    "    knn_hyperparameters,\n",
    "    (inputs, targets),\n",
    "    crossValidationIndices\n",
    ")\n",
    "\n",
    "acc, err, sens, spec, ppv, npv, f1, conf = results_knn\n",
    "println(\"Accuracy: $(round(acc[1], digits=4)) ± $(round(acc[2], digits=4))\")\n",
    "println(\"Error Rate: $(round(err[1], digits=4)) ± $(round(err[2], digits=4))\")\n",
    "println(\"Sensitivity: $(round(sens[1], digits=4)) ± $(round(sens[2], digits=4))\")\n",
    "println(\"Specificity: $(round(spec[1], digits=4)) ± $(round(spec[2], digits=4))\")\n",
    "println(\"PPV: $(round(ppv[1], digits=4)) ± $(round(ppv[2], digits=4))\")\n",
    "println(\"NPV: $(round(npv[1], digits=4)) ± $(round(npv[2], digits=4))\")\n",
    "println(\"F1-Score: $(round(f1[1], digits=4)) ± $(round(f1[2], digits=4))\")\n",
    "println(\"Confusion Matrix:\")\n",
    "println(conf)\n",
    "println()\n",
    "\n",
    "# Test 7: Artificial Neural Network\n",
    "println(\"=\" ^ 60)\n",
    "println(\"Test 7: ANN with topology [4, 3]\")\n",
    "println(\"=\" ^ 60)\n",
    "ann_hyperparameters = Dict(\n",
    "    \"topology\" => [4, 3],\n",
    "    \"learningRate\" => 0.01,\n",
    "    \"validationRatio\" => 0.2,\n",
    "    \"numExecutions\" => 10,  # Reduced for faster testing\n",
    "    \"maxEpochs\" => 500,\n",
    "    \"maxEpochsVal\" => 10\n",
    ")\n",
    "\n",
    "results_ann = modelCrossValidation(\n",
    "    :ANN,\n",
    "    ann_hyperparameters,\n",
    "    (inputs, targets),\n",
    "    crossValidationIndices\n",
    ")\n",
    "\n",
    "acc, err, sens, spec, ppv, npv, f1, conf = results_ann\n",
    "println(\"Accuracy: $(round(acc[1], digits=4)) ± $(round(acc[2], digits=4))\")\n",
    "println(\"Error Rate: $(round(err[1], digits=4)) ± $(round(err[2], digits=4))\")\n",
    "println(\"Sensitivity: $(round(sens[1], digits=4)) ± $(round(sens[2], digits=4))\")\n",
    "println(\"Specificity: $(round(spec[1], digits=4)) ± $(round(spec[2], digits=4))\")\n",
    "println(\"PPV: $(round(ppv[1], digits=4)) ± $(round(ppv[2], digits=4))\")\n",
    "println(\"NPV: $(round(npv[1], digits=4)) ± $(round(npv[2], digits=4))\")\n",
    "println(\"F1-Score: $(round(f1[1], digits=4)) ± $(round(f1[2], digits=4))\")\n",
    "println(\"Confusion Matrix:\")\n",
    "println(conf)\n",
    "println()\n",
    "\n",
    "println(\"=\" ^ 75)\n",
    "println(\"Model Comparison Summary\")\n",
    "println(\"=\" ^ 75)\n",
    "println(\"Model   Accuracy (Mean ± Std)  F1-Score (Mean ± Std)\")\n",
    "println(\"-\" ^ 75)\n",
    "println(\"SVM (Linear) $(round(results_svm_linear[1][1], digits=4)) ± $(round(results_svm_linear[1][2], digits=4))   $(round(results_svm_linear[7][1], digits=4)) ± $(round(results_svm_linear[7][2], digits=4))\")\n",
    "println(\"SVM (RBF)   $(round(results_svm_rbf[1][1], digits=4)) ± $(round(results_svm_rbf[1][2], digits=4))   $(round(results_svm_rbf[7][1], digits=4)) ± $(round(results_svm_rbf[7][2], digits=4))\")\n",
    "println(\"SVM (Sigmoid)  $(round(results_svm_sigmoid[1][1], digits=4)) ± $(round(results_svm_sigmoid[1][2], digits=4))   $(round(results_svm_sigmoid[7][1], digits=4)) ± $(round(results_svm_sigmoid[7][2], digits=4))\")\n",
    "println(\"SVM (Polynomial)  $(round(results_svm_poly[1][1], digits=4)) ± $(round(results_svm_poly[1][2], digits=4))   $(round(results_svm_poly[7][1], digits=4)) ± $(round(results_svm_poly[7][2], digits=4))\")\n",
    "println(\"Decision Tree  $(round(results_dt[1][1], digits=4)) ± $(round(results_dt[1][2], digits=4))   $(round(results_dt[7][1], digits=4)) ± $(round(results_dt[7][2], digits=4))\")\n",
    "println(\"k-NN (K=3)   $(round(results_knn[1][1], digits=4)) ± $(round(results_knn[1][2], digits=4))   $(round(results_knn[7][1], digits=4)) ± $(round(results_knn[7][2], digits=4))\")\n",
    "println(\"ANN [4,3] $(round(results_ann[1][1], digits=4)) ± $(round(results_ann[1][2], digits=4))   $(round(results_ann[7][1], digits=4)) ± $(round(results_ann[7][2], digits=4))\")\n",
    "println(\"=\" ^ 75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c116879-c8a3-4427-829c-d52af9a436b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
