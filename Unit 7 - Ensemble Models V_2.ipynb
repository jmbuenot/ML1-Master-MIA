{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5dd4668",
   "metadata": {},
   "source": [
    "# Ensemble Models\n",
    "\n",
    "One of the latest trends in artificial intelligence modelling can be summarised as \"knowledge of the whole or the crowd\". What this somewhat familiar phrase defines is the use of a multitude of so-called \"weak\" models in a meta-classifier. The aim is to generate a \"strong\" model based on the knowledge extracted by the \"weak\" models. For example, although it will be detailed later, multiple, much simpler Decision Trees are developed in a Random Forest. The combination of these ones in the Random Forest exceeds the performance of any of the individual models. The models that emerge in this way, as meta-classifiers or meta-regressors, are generically called **Ensemble models**.\n",
    "\n",
    "Is is worth mentioned that these models may not be limited only to decision trees, but may instead be composed of any type of machine learning model that has been seen previously. They can even be mixed models where not all models have been obtained in the same way, but can be created through the combined use of several techniques such as K-NN, SVM, etc. Thus, the first criteria to classifify the ensemble models would be if they are homogeneous or heterogeneous models. However this is not the only criteria to classifity the ensemble models, in this unit, we will explore various ways of generating the models and how to combine them later on. We will also take a closer look at two of the most common techniques within ensemble models such as Random Forest and _XGBoost_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10e861d",
   "metadata": {},
   "source": [
    "First of all, ensure that the required packages are installed. Therefore, you should only execute the folowing cell the fist time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bbbb278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/projects/ML1/All_units`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `~/projects/ML1/All_units/Project.toml`\n",
      "  \u001b[90m[336ed68f] \u001b[39mCSV v0.10.15\n",
      "\u001b[33m⌅\u001b[39m \u001b[90m[324d7699] \u001b[39mCategoricalArrays v0.10.8\n",
      "  \u001b[90m[a93c6f00] \u001b[39mDataFrames v1.8.1\n",
      "  \u001b[90m[7806a523] \u001b[39mDecisionTree v0.12.4\n",
      "  \u001b[90m[f6006082] \u001b[39mEvoTrees v0.18.0\n",
      "  \u001b[90m[587475ba] \u001b[39mFlux v0.16.5\n",
      "  \u001b[90m[7073ff75] \u001b[39mIJulia v1.32.1\n",
      "  \u001b[90m[b1bec4e5] \u001b[39mLIBSVM v0.8.1\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[add582a8] \u001b[39mMLJ v0.20.0\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[a7f614a8] \u001b[39mMLJBase v1.7.0\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[c6f25543] \u001b[39mMLJDecisionTreeInterface v0.4.2\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[50ed68f4] \u001b[39mMLJEnsembles v0.4.3\n",
      "\u001b[32m⌃\u001b[39m \u001b[90m[61c7150f] \u001b[39mMLJLIBSVMInterface v0.2.1\n",
      "  \u001b[90m[6ee0df7b] \u001b[39mMLJLinearModels v0.10.1\n",
      "  \u001b[90m[e80e1ace] \u001b[39mMLJModelInterface v1.12.0\n",
      "\u001b[33m⌅\u001b[39m \u001b[90m[d491faf4] \u001b[39mMLJModels v0.16.17\n",
      "  \u001b[90m[33e4bacb] \u001b[39mMLJNaiveBayesInterface v0.1.6\n",
      "  \u001b[90m[5ae90465] \u001b[39mMLJScikitLearnInterface v0.7.0\n",
      "  \u001b[90m[9bbee03b] \u001b[39mNaiveBayes v0.5.6\n",
      "  \u001b[90m[636a865e] \u001b[39mNearestNeighborModels v0.2.3\n",
      "  \u001b[90m[91a5bcdd] \u001b[39mPlots v1.41.1\n",
      "  \u001b[90m[3646fa90] \u001b[39mScikitLearn v0.7.0\n",
      "  \u001b[90m[856ac37a] \u001b[39mUrlDownload v1.0.1\n",
      "  \u001b[90m[009559a3] \u001b[39mXGBoost v2.5.2\n",
      "  \u001b[90m[9a3f8284] \u001b[39mRandom v1.11.0\n",
      "\u001b[36m\u001b[1mInfo\u001b[22m\u001b[39m Packages marked with \u001b[32m⌃\u001b[39m and \u001b[33m⌅\u001b[39m have new versions available. Those with \u001b[32m⌃\u001b[39m may be upgradable, but those with \u001b[33m⌅\u001b[39m are restricted by compatibility constraints from upgrading. To see why use `status --outdated`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "Pkg.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e80662de-d6be-4989-95dc-a3d434f98292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pkg.add([\n",
    "#     \"MLJ\", \n",
    "#     \"MLJBase\", \n",
    "#     \"MLJModels\", \n",
    "#     \"MLJEnsembles\", \n",
    "#     \"MLJLinearModels\", \n",
    "#     \"DecisionTree\", \n",
    "#     \"MLJDecisionTreeInterface\",\n",
    "#     \"NaiveBayes\",\n",
    "#     \"MLJNaiveBayesInterface\", \n",
    "#     \"EvoTrees\", \n",
    "#     \"CategoricalArrays\", \n",
    "#     \"Random\",\n",
    "#     \"LIBSVM\",           \n",
    "#     \"MLJLIBSVMInterface\", \n",
    "#     \"Plots\",            \n",
    "#     \"MLJModelInterface\", \n",
    "#     \"CSV\",              \n",
    "#     \"DataFrames\",       \n",
    "#     \"UrlDownload\",      \n",
    "#     \"XGBoost\", \n",
    "#     \"Flux\",\n",
    "#     \"NearestNeighborModels\"\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8f73db",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Unlike the first tutorials, where the iris flower problem has been used as a benchmark, in this tutorial we will use a different one. The problem is also included in the UCI repository, although it is also small, the number of variables increases significantly and therefore it will give us some more room to explore. Specifically, it is a classic machine learning problem, which is informally called Rock or Mine? It is a small database consisting of 111 patterns corresponding to rocks and 97 to water mines (simulated as metal cylinders). Each of the patterns consists of 60 numerical measurements corresponding to a section of the sonar sequences. These values are already between 0.0 and 1.0, although it is worth normalising them to be on the safe side. These measurements represent the energy value of different wavelength ranges for a certain period of time.\n",
    "\n",
    "We are going to use a couple of new packages in the process, more specificly, [DataFrames.jl](https://juliaai.github.io/DataScienceTutorials.jl/data/dataframe/) and [UrlDownload.jl](https://github.com/Arkoniak/UrlDownload.jl). Therefore, first thing first, ensure that the packages are correcly installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0eb88ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Pkg;\n",
    "# Pkg.add(\"CSV\")\n",
    "# Pkg.add(\"DataFrames\")\n",
    "# Pkg.add(\"UrlDownload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad05475",
   "metadata": {},
   "source": [
    "After that, the data will be downloaded if they are not already available, for which the following code can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007c0232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>61×7 DataFrame</span></div><div style = \"float: right;\"><span style = \"font-style: italic;\">36 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">variable</th><th style = \"text-align: left;\">mean</th><th style = \"text-align: left;\">min</th><th style = \"text-align: left;\">median</th><th style = \"text-align: left;\">max</th><th style = \"text-align: left;\">nmissing</th><th style = \"text-align: left;\">eltype</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Symbol\" style = \"text-align: left;\">Symbol</th><th title = \"Union{Nothing, Float64}\" style = \"text-align: left;\">Union…</th><th title = \"Any\" style = \"text-align: left;\">Any</th><th title = \"Union{Nothing, Float64}\" style = \"text-align: left;\">Union…</th><th title = \"Any\" style = \"text-align: left;\">Any</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"DataType\" style = \"text-align: left;\">DataType</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">Column1</td><td style = \"text-align: left;\">0.0291639</td><td style = \"text-align: left;\">0.0015</td><td style = \"text-align: left;\">0.0228</td><td style = \"text-align: left;\">0.1371</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">Column2</td><td style = \"text-align: left;\">0.0384365</td><td style = \"text-align: left;\">0.0006</td><td style = \"text-align: left;\">0.0308</td><td style = \"text-align: left;\">0.2339</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">Column3</td><td style = \"text-align: left;\">0.0438322</td><td style = \"text-align: left;\">0.0015</td><td style = \"text-align: left;\">0.0343</td><td style = \"text-align: left;\">0.3059</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">Column4</td><td style = \"text-align: left;\">0.0538923</td><td style = \"text-align: left;\">0.0058</td><td style = \"text-align: left;\">0.04405</td><td style = \"text-align: left;\">0.4264</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: left;\">Column5</td><td style = \"text-align: left;\">0.0752024</td><td style = \"text-align: left;\">0.0067</td><td style = \"text-align: left;\">0.0625</td><td style = \"text-align: left;\">0.401</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: left;\">Column6</td><td style = \"text-align: left;\">0.10457</td><td style = \"text-align: left;\">0.0102</td><td style = \"text-align: left;\">0.09215</td><td style = \"text-align: left;\">0.3823</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: left;\">Column7</td><td style = \"text-align: left;\">0.121747</td><td style = \"text-align: left;\">0.0033</td><td style = \"text-align: left;\">0.10695</td><td style = \"text-align: left;\">0.3729</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: left;\">Column8</td><td style = \"text-align: left;\">0.134799</td><td style = \"text-align: left;\">0.0055</td><td style = \"text-align: left;\">0.1121</td><td style = \"text-align: left;\">0.459</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: left;\">Column9</td><td style = \"text-align: left;\">0.178003</td><td style = \"text-align: left;\">0.0075</td><td style = \"text-align: left;\">0.15225</td><td style = \"text-align: left;\">0.6828</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: left;\">Column10</td><td style = \"text-align: left;\">0.208259</td><td style = \"text-align: left;\">0.0113</td><td style = \"text-align: left;\">0.1824</td><td style = \"text-align: left;\">0.7106</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: left;\">Column11</td><td style = \"text-align: left;\">0.236013</td><td style = \"text-align: left;\">0.0289</td><td style = \"text-align: left;\">0.2248</td><td style = \"text-align: left;\">0.7342</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: left;\">Column12</td><td style = \"text-align: left;\">0.250221</td><td style = \"text-align: left;\">0.0236</td><td style = \"text-align: left;\">0.24905</td><td style = \"text-align: left;\">0.706</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: left;\">Column13</td><td style = \"text-align: left;\">0.273305</td><td style = \"text-align: left;\">0.0184</td><td style = \"text-align: left;\">0.26395</td><td style = \"text-align: left;\">0.7131</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">50</td><td style = \"text-align: left;\">Column50</td><td style = \"text-align: left;\">0.020424</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">0.0179</td><td style = \"text-align: left;\">0.0825</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">51</td><td style = \"text-align: left;\">Column51</td><td style = \"text-align: left;\">0.0160687</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">0.0139</td><td style = \"text-align: left;\">0.1004</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">52</td><td style = \"text-align: left;\">Column52</td><td style = \"text-align: left;\">0.0134202</td><td style = \"text-align: left;\">0.0008</td><td style = \"text-align: left;\">0.0114</td><td style = \"text-align: left;\">0.0709</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">53</td><td style = \"text-align: left;\">Column53</td><td style = \"text-align: left;\">0.0107091</td><td style = \"text-align: left;\">0.0005</td><td style = \"text-align: left;\">0.00955</td><td style = \"text-align: left;\">0.039</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">54</td><td style = \"text-align: left;\">Column54</td><td style = \"text-align: left;\">0.0109409</td><td style = \"text-align: left;\">0.001</td><td style = \"text-align: left;\">0.0093</td><td style = \"text-align: left;\">0.0352</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">55</td><td style = \"text-align: left;\">Column55</td><td style = \"text-align: left;\">0.00929038</td><td style = \"text-align: left;\">0.0006</td><td style = \"text-align: left;\">0.0075</td><td style = \"text-align: left;\">0.0447</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">56</td><td style = \"text-align: left;\">Column56</td><td style = \"text-align: left;\">0.00822163</td><td style = \"text-align: left;\">0.0004</td><td style = \"text-align: left;\">0.00685</td><td style = \"text-align: left;\">0.0394</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">57</td><td style = \"text-align: left;\">Column57</td><td style = \"text-align: left;\">0.00782019</td><td style = \"text-align: left;\">0.0003</td><td style = \"text-align: left;\">0.00595</td><td style = \"text-align: left;\">0.0355</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">58</td><td style = \"text-align: left;\">Column58</td><td style = \"text-align: left;\">0.00794904</td><td style = \"text-align: left;\">0.0003</td><td style = \"text-align: left;\">0.0058</td><td style = \"text-align: left;\">0.044</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">59</td><td style = \"text-align: left;\">Column59</td><td style = \"text-align: left;\">0.00794135</td><td style = \"text-align: left;\">0.0001</td><td style = \"text-align: left;\">0.0064</td><td style = \"text-align: left;\">0.0364</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">60</td><td style = \"text-align: left;\">Column60</td><td style = \"text-align: left;\">0.00650721</td><td style = \"text-align: left;\">0.0006</td><td style = \"text-align: left;\">0.0053</td><td style = \"text-align: left;\">0.0439</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">Float64</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">61</td><td style = \"text-align: left;\">Column61</td><td style = \"font-style: italic; text-align: left;\"></td><td style = \"text-align: left;\">M</td><td style = \"font-style: italic; text-align: left;\"></td><td style = \"text-align: left;\">R</td><td style = \"text-align: right;\">0</td><td style = \"text-align: left;\">String1</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& variable & mean & min & median & max & nmissing & eltype\\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Union… & Any & Union… & Any & Int64 & DataType\\\\\n",
       "\t\\hline\n",
       "\t1 & Column1 & 0.0291639 & 0.0015 & 0.0228 & 0.1371 & 0 & Float64 \\\\\n",
       "\t2 & Column2 & 0.0384365 & 0.0006 & 0.0308 & 0.2339 & 0 & Float64 \\\\\n",
       "\t3 & Column3 & 0.0438322 & 0.0015 & 0.0343 & 0.3059 & 0 & Float64 \\\\\n",
       "\t4 & Column4 & 0.0538923 & 0.0058 & 0.04405 & 0.4264 & 0 & Float64 \\\\\n",
       "\t5 & Column5 & 0.0752024 & 0.0067 & 0.0625 & 0.401 & 0 & Float64 \\\\\n",
       "\t6 & Column6 & 0.10457 & 0.0102 & 0.09215 & 0.3823 & 0 & Float64 \\\\\n",
       "\t7 & Column7 & 0.121747 & 0.0033 & 0.10695 & 0.3729 & 0 & Float64 \\\\\n",
       "\t8 & Column8 & 0.134799 & 0.0055 & 0.1121 & 0.459 & 0 & Float64 \\\\\n",
       "\t9 & Column9 & 0.178003 & 0.0075 & 0.15225 & 0.6828 & 0 & Float64 \\\\\n",
       "\t10 & Column10 & 0.208259 & 0.0113 & 0.1824 & 0.7106 & 0 & Float64 \\\\\n",
       "\t11 & Column11 & 0.236013 & 0.0289 & 0.2248 & 0.7342 & 0 & Float64 \\\\\n",
       "\t12 & Column12 & 0.250221 & 0.0236 & 0.24905 & 0.706 & 0 & Float64 \\\\\n",
       "\t13 & Column13 & 0.273305 & 0.0184 & 0.26395 & 0.7131 & 0 & Float64 \\\\\n",
       "\t14 & Column14 & 0.296568 & 0.0273 & 0.2811 & 0.997 & 0 & Float64 \\\\\n",
       "\t15 & Column15 & 0.320201 & 0.0031 & 0.2817 & 1.0 & 0 & Float64 \\\\\n",
       "\t16 & Column16 & 0.378487 & 0.0162 & 0.3047 & 0.9988 & 0 & Float64 \\\\\n",
       "\t17 & Column17 & 0.415983 & 0.0349 & 0.3084 & 1.0 & 0 & Float64 \\\\\n",
       "\t18 & Column18 & 0.452318 & 0.0375 & 0.3683 & 1.0 & 0 & Float64 \\\\\n",
       "\t19 & Column19 & 0.504812 & 0.0494 & 0.43495 & 1.0 & 0 & Float64 \\\\\n",
       "\t20 & Column20 & 0.563047 & 0.0656 & 0.5425 & 1.0 & 0 & Float64 \\\\\n",
       "\t21 & Column21 & 0.60906 & 0.0512 & 0.6177 & 1.0 & 0 & Float64 \\\\\n",
       "\t22 & Column22 & 0.624275 & 0.0219 & 0.6649 & 1.0 & 0 & Float64 \\\\\n",
       "\t23 & Column23 & 0.646975 & 0.0563 & 0.6997 & 1.0 & 0 & Float64 \\\\\n",
       "\t24 & Column24 & 0.672654 & 0.0239 & 0.6985 & 1.0 & 0 & Float64 \\\\\n",
       "\t25 & Column25 & 0.675424 & 0.024 & 0.7211 & 1.0 & 0 & Float64 \\\\\n",
       "\t26 & Column26 & 0.699866 & 0.0921 & 0.7545 & 1.0 & 0 & Float64 \\\\\n",
       "\t27 & Column27 & 0.702155 & 0.0481 & 0.7456 & 1.0 & 0 & Float64 \\\\\n",
       "\t28 & Column28 & 0.694024 & 0.0284 & 0.7319 & 1.0 & 0 & Float64 \\\\\n",
       "\t29 & Column29 & 0.642074 & 0.0144 & 0.6808 & 1.0 & 0 & Float64 \\\\\n",
       "\t30 & Column30 & 0.580928 & 0.0613 & 0.60715 & 1.0 & 0 & Float64 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m61×7 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m variable \u001b[0m\u001b[1m mean       \u001b[0m\u001b[1m min    \u001b[0m\u001b[1m median  \u001b[0m\u001b[1m max    \u001b[0m\u001b[1m nmissing \u001b[0m\u001b[1m eltype   \u001b[0m\n",
       "     │\u001b[90m Symbol   \u001b[0m\u001b[90m Union…     \u001b[0m\u001b[90m Any    \u001b[0m\u001b[90m Union…  \u001b[0m\u001b[90m Any    \u001b[0m\u001b[90m Int64    \u001b[0m\u001b[90m DataType \u001b[0m\n",
       "─────┼───────────────────────────────────────────────────────────────────\n",
       "   1 │ Column1   0.0291639   0.0015  0.0228   0.1371         0  Float64\n",
       "   2 │ Column2   0.0384365   0.0006  0.0308   0.2339         0  Float64\n",
       "   3 │ Column3   0.0438322   0.0015  0.0343   0.3059         0  Float64\n",
       "   4 │ Column4   0.0538923   0.0058  0.04405  0.4264         0  Float64\n",
       "   5 │ Column5   0.0752024   0.0067  0.0625   0.401          0  Float64\n",
       "   6 │ Column6   0.10457     0.0102  0.09215  0.3823         0  Float64\n",
       "   7 │ Column7   0.121747    0.0033  0.10695  0.3729         0  Float64\n",
       "   8 │ Column8   0.134799    0.0055  0.1121   0.459          0  Float64\n",
       "   9 │ Column9   0.178003    0.0075  0.15225  0.6828         0  Float64\n",
       "  10 │ Column10  0.208259    0.0113  0.1824   0.7106         0  Float64\n",
       "  11 │ Column11  0.236013    0.0289  0.2248   0.7342         0  Float64\n",
       "  ⋮  │    ⋮          ⋮         ⋮        ⋮       ⋮        ⋮         ⋮\n",
       "  52 │ Column52  0.0134202   0.0008  0.0114   0.0709         0  Float64\n",
       "  53 │ Column53  0.0107091   0.0005  0.00955  0.039          0  Float64\n",
       "  54 │ Column54  0.0109409   0.001   0.0093   0.0352         0  Float64\n",
       "  55 │ Column55  0.00929038  0.0006  0.0075   0.0447         0  Float64\n",
       "  56 │ Column56  0.00822163  0.0004  0.00685  0.0394         0  Float64\n",
       "  57 │ Column57  0.00782019  0.0003  0.00595  0.0355         0  Float64\n",
       "  58 │ Column58  0.00794904  0.0003  0.0058   0.044          0  Float64\n",
       "  59 │ Column59  0.00794135  0.0001  0.0064   0.0364         0  Float64\n",
       "  60 │ Column60  0.00650721  0.0006  0.0053   0.0439         0  Float64\n",
       "  61 │ Column61 \u001b[90m            \u001b[0m M      \u001b[90m         \u001b[0m R              0  String1\n",
       "\u001b[36m                                                          40 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using UrlDownload;\n",
    "using DataFrames;\n",
    "using CSV;\n",
    "using CategoricalArrays;\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\"\n",
    "data = urldownload(url, true, format=:CSV, header=false) |> DataFrame;\n",
    "describe(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ebf504",
   "metadata": {},
   "source": [
    "As it can be seen in the previos line, we have downloaded de data and pipe it, with the operator `|>`, into the function DataFrame. This is going to create an structure simular to a database table which is particular convinient to check for missing values or the ranges of the different variables. In fact, the library makes it particularly easy to deal with missing values with functions to fullfill or remove the samples with non-valid measures. However it is too long to see every single variable on the output report, if some queries are made we can identify  that here is no missing values. Additionally no variable is over 1.0 but some of them are not normalized. A similar structure can be found in other languages, like R or Python.\n",
    "\n",
    "As an example, of this process lets make the an additional column in order to convert to categorical the las column 60 which has a **M** for each Mine and an **R** for each sample of rock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a07ecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "insertcols!(data, :Mine => data[:, 61].==\"M\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efa393f",
   "metadata": {},
   "source": [
    "Once the data is loaded in the DataFrame for the checking proposes and that any posible process has been applied on the data. As in previous tutorials, the data has to be put on a Matrix form, such as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "590ea8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = Matrix{Float64}(data[!, 1:60]);\n",
    "output_data = categorical(data[!, :Mine]);\n",
    "\n",
    "@assert input_data isa Matrix\n",
    "@assert output_data isa CategoricalArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a148c7",
   "metadata": {},
   "source": [
    "It is worth to mention that in a DataFrame when a set of lines is queried such as in the case of the `X`, the results is also a DataFrame. Therefore, in order to applied the remaining operations it is needed to applied the `Matrix` function to retrive a matrix where the previous operations can be used as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d272d3b3",
   "metadata": {},
   "source": [
    "### Question 7.1\n",
    "\n",
    "> ❓ Now, the data is loaded and converted to the usual types. Now you should be able to apply in the next section and make asplit of the dataset in two subset, test and training, and apply the corresponding normalization. Put the code on the following section to perform both operations. *Tip: Due to the preparation for MLJ models, read the notes at the end of the document*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f87e4831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tables.MatrixTable{Matrix{Float64}} with 42 rows, 60 columns, and schema:\n",
       " :x1   Float64\n",
       " :x2   Float64\n",
       " :x3   Float64\n",
       " :x4   Float64\n",
       " :x5   Float64\n",
       " :x6   Float64\n",
       " :x7   Float64\n",
       " :x8   Float64\n",
       " :x9   Float64\n",
       " :x10  Float64\n",
       " :x11  Float64\n",
       " :x12  Float64\n",
       " :x13  Float64\n",
       " ⋮     \n",
       " :x49  Float64\n",
       " :x50  Float64\n",
       " :x51  Float64\n",
       " :x52  Float64\n",
       " :x53  Float64\n",
       " :x54  Float64\n",
       " :x55  Float64\n",
       " :x56  Float64\n",
       " :x57  Float64\n",
       " :x58  Float64\n",
       " :x59  Float64\n",
       " :x60  Float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_input, train_output, test_input, test_output = #TODO\n",
    "include(\"utils_ML1.jl\")  \n",
    "using .UtilsML1: calculateMinMaxNormalizationParameters, holdOut, normalizeMinMax\n",
    "using Flux\n",
    "using MLJ\n",
    "using Random  \n",
    "Random.seed!(123)  \n",
    "\n",
    "# 1. Split the dataset into training and testing sets (80/20 split)\n",
    "train_indices, test_indices = holdOut(size(input_data, 1), 0.2)\n",
    "\n",
    "# 2. Partition the input features and output labels using the generated indices\n",
    "train_input = input_data[train_indices, :];\n",
    "test_input  = input_data[test_indices, :];\n",
    "train_output = output_data[train_indices];   # remains a categorical vector\n",
    "test_output  = output_data[test_indices];\n",
    "\n",
    "# 3. Compute normalization parameters on the training features (min-max normalization)\n",
    "norm_params = calculateMinMaxNormalizationParameters(train_input)\n",
    "\n",
    "# 4. Apply min-max normalization to both training and test feature matrices using training params\n",
    "train_input_matrix = normalizeMinMax(train_input, norm_params);\n",
    "test_input_matrix  = normalizeMinMax(test_input, norm_params);\n",
    "\n",
    "# 5. Convert the normalized feature matrices to MLJ-compatible table format for modeling\n",
    "train_input = MLJ.table(train_input_matrix)\n",
    "test_input  = MLJ.table(test_input_matrix)\n",
    "\n",
    "# train_indices, test_indices = holdOut(size(input_data, 1), 0.2)\n",
    "\n",
    "# train_input_matrix = normalizeMinMax(input_data[train_indices, :])\n",
    "# test_input_matrix  = normalizeMinMax(input_data[test_indices, :]);\n",
    "# train_output = output_data[train_indices];   # remains a categorical vector\n",
    "# test_output  = output_data[test_indices];\n",
    "\n",
    "\n",
    "# norm_params = calculateMinMaxNormalizationParameters(input_data[train_indices, :])\n",
    "\n",
    "# train_input = MLJ.table(train_input_matrix)\n",
    "# test_input  = MLJ.table(test_input_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcbe5ef",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "As mentioned above, ensembles are a set of \"weaker\" classifiers that allow us to later overcome their limits by joining them together. That is why, before starting with ensembles, it will be necessary to have some reference models that will later be joined together in a meta-classifier. In the following example, some simple models, imlemented with `MLJ` library, are trained: an SVM with RBF kernel, a Linear Regression, a Naïve Bayes and a Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04b0a9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFor silent loading, specify `verbosity=0`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import MLJLIBSVMInterface ✔\n",
      "import MLJLinearModels ✔"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFor silent loading, specify `verbosity=0`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import MLJDecisionTreeInterface ✔\n",
      "import MLJNaiveBayesInterface ✔"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFor silent loading, specify `verbosity=0`. \n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFor silent loading, specify `verbosity=0`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{Any, Any}()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLJ\n",
    "\n",
    "# Load models (MLJ will prompt to add missing packages the first time you run these)\n",
    "SVC = @load ProbabilisticSVC pkg=LIBSVM\n",
    "LogisticClassifier = @load LogisticClassifier pkg=MLJLinearModels\n",
    "DecisionTreeClassifier = @load DecisionTreeClassifier pkg=DecisionTree\n",
    "GaussianNBClassifier = @load GaussianNBClassifier pkg=NaiveBayes\n",
    "\n",
    "#Define the models to train\n",
    "models = Dict(\n",
    "    \"SVM\" => SVC(),\n",
    "    \"LR\"  => LogisticClassifier(),\n",
    "    \"DT\"  => DecisionTreeClassifier(max_depth=4),\n",
    "    \"NB\"  => GaussianNBClassifier(),\n",
    ")\n",
    "\n",
    "base_models=  [ model for (name, model) in models]\n",
    "\n",
    "machines_dict = Dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3a595d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(GaussianNBClassifier(), …).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB: 78.57142857142857 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(ProbabilisticSVC(kernel = RadialBasis, …), …).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: 76.19047619047619 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(LogisticClassifier(lambda = 2.220446049250313e-16, …), …).\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSolver: MLJLinearModels.LBFGS{Optim.Options{Float64, Nothing}, @NamedTuple{}}\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m  optim_options: Optim.Options{Float64, Nothing}\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m  lbfgs_options: @NamedTuple{} NamedTuple()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 73.80952380952381 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(DecisionTreeClassifier(max_depth = 4, …), …).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT: 78.57142857142857 %\n"
     ]
    }
   ],
   "source": [
    "# Perform the training for each model and calculate the test values (accuracy)\n",
    "for (name, model) in models\n",
    "    machines_dict[name] = machine(model, train_input, train_output) |> fit!\n",
    "    acc = MLJ.accuracy(MLJ.mode.(predict(machines_dict[name], test_input)), test_output)\n",
    "    println(\"$name: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0ecb8c",
   "metadata": {},
   "source": [
    "## Combining weak models in an ensemble\n",
    "\n",
    "When it comes to combining the models, there are different strategies depending on the task of the model, i.e. whether we are classifying or regressing. In this particular case we are going to focus on classification, although for regression it would be similar, but the continuous nature of the values should be taken into account when combining the outputs.\n",
    "\n",
    "Regarding the combination of the classification, there are mainly two ways to combine the outputs of several classifiers. These combinations are called Majority voting and Weighted majority voting, also known as Soft Voting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3e9f1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLJModelInterface.metadata_model"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLJ\n",
    "using MLJBase\n",
    "using MLJModelInterface\n",
    "\n",
    "# ===================================================\n",
    "# DEFINITION OF VOTINGCLASSIFIER COMPATIBLE WITH MLJ\n",
    "# ===================================================\n",
    "\n",
    "\"\"\"\n",
    "    VotingClassifier <: Probabilistic\n",
    "\n",
    "An ensemble classifier that combines predictions from multiple base models using voting strategies.\n",
    "\n",
    "# Fields\n",
    "- `models::Vector{Probabilistic}`: Vector of base probabilistic models to be combined\n",
    "- `voting::Symbol`: Voting strategy, either `:hard` (majority vote) or `:soft` (averaged probabilities)\n",
    "- `weights::Union{Nothing, Vector{Float64}}`: Optional weights for each model. If `nothing`, all models have equal weight. Weights are automatically normalized to sum to 1.0.\n",
    "\n",
    "# Examples\n",
    "```julia\n",
    "# Equal weights (default)\n",
    "voting_clf = VotingClassifier(\n",
    "    models=[LogisticClassifier(), DecisionTreeClassifier()],\n",
    "    voting=:soft\n",
    ")\n",
    "\n",
    "# Custom weights (will be normalized automatically)\n",
    "voting_clf = VotingClassifier(\n",
    "    models=[LogisticClassifier(), DecisionTreeClassifier(), RandomForestClassifier()],\n",
    "    voting=:hard,\n",
    "    weights=[5, 3, 2]  # Will be normalized to [0.5, 0.3, 0.2]\n",
    ")\n",
    "```\n",
    "\"\"\"\n",
    "mutable struct VotingClassifier <: Probabilistic   # Models must be probabilistic, inherited from MLJBase\n",
    "    models::Vector{Probabilistic}\n",
    "    voting::Symbol  # :hard or :soft\n",
    "    weights::Union{Nothing, Vector{Float64}}\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    VotingClassifier(; models=Probabilistic[], voting=:hard, weights=nothing)\n",
    "\n",
    "Constructor for VotingClassifier.\n",
    "\n",
    "# Arguments\n",
    "- `models::Vector{Probabilistic}=Probabilistic[]`: Base models to combine\n",
    "- `voting::Symbol=:hard`: Voting strategy (`:hard` or `:soft`)\n",
    "- `weights::Union{Nothing, Vector{<:Real}}=nothing`: Weights for each model. Automatically normalized to sum to 1.0.\n",
    "\n",
    "# Throws\n",
    "- `AssertionError`: If voting is not `:hard` or `:soft`\n",
    "- `AssertionError`: If weights length doesn't match models length\n",
    "- `AssertionError`: If all weights are zero or negative\n",
    "\"\"\"\n",
    "function VotingClassifier(; models=Probabilistic[], voting=:hard, weights=nothing)\n",
    "    @assert voting in [:hard, :soft] \"The only possible labels are :hard or :soft\"\n",
    "    \n",
    "    normalized_weights = nothing\n",
    "    if weights !== nothing\n",
    "        @assert length(weights) == length(models) \"Number of weights must match number of models\"\n",
    "        @assert all(w >= 0 for w in weights) \"All weights must be non-negative\"\n",
    "        \n",
    "        # Normalize weights to sum to 1.0\n",
    "        normalized_weights = Float64.(weights) ./ sum(weights)\n",
    "    end\n",
    "    \n",
    "    return VotingClassifier(models, voting, normalized_weights)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    MLJModelInterface.fit(model::VotingClassifier, verbosity::Int, X, y)\n",
    "\n",
    "Fit the VotingClassifier by training each base model on the provided data.\n",
    "\n",
    "# Arguments\n",
    "- `model::VotingClassifier`: The voting classifier instance\n",
    "- `verbosity::Int`: Verbosity level for training output\n",
    "- `X`: Training features (table format)\n",
    "- `y`: Training target (categorical vector)\n",
    "\n",
    "# Returns\n",
    "- `fitresults`: Vector of trained machines (one per base model)\n",
    "- `cache`: Nothing (no caching implemented)\n",
    "- `report`: Named tuple with training information (number of models, voting strategy, and normalized weights)\n",
    "\"\"\"\n",
    "function MLJModelInterface.fit(model::VotingClassifier, verbosity::Int, X, y)\n",
    "    # Train each base model\n",
    "    machs = [begin\n",
    "        mm = machine(deepcopy(m), X, y)\n",
    "        MLJ.fit!(mm, verbosity=0)\n",
    "        mm\n",
    "    end for m in model.models]\n",
    "\n",
    "    fitresults = (\n",
    "        machines = machs,\n",
    "        class_levels = collect(levels(y)),   # Kept the levels to use them in the same order\n",
    "        class_pool = CategoricalArrays.pool(y)\n",
    "    )\n",
    "    \n",
    "    # Save necessary information\n",
    "    cache = nothing\n",
    "    report = (n_models=length(model.models), voting=model.voting, weights=model.weights)\n",
    "    \n",
    "    return fitresults, cache, report\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    MLJModelInterface.predict_mode(model::VotingClassifier, fitresult, Xnew)\n",
    "\n",
    "Predict class labels using hard voting (majority vote with optional weights).\n",
    "\n",
    "# Arguments\n",
    "- `model::VotingClassifier`: The voting classifier instance\n",
    "- `fitresult`: Vector of trained machines from fit\n",
    "- `Xnew`: New data to predict on\n",
    "\n",
    "# Returns\n",
    "- Categorical vector of predicted class labels based on (weighted) majority voting\n",
    "\n",
    "# Details\n",
    "Each base model votes for a class. If weights are provided, each vote is multiplied by its \n",
    "corresponding weight. The class with the highest (weighted) vote count is selected.\n",
    "\"\"\"\n",
    "function MLJModelInterface.predict_mode(model::VotingClassifier, fitresult, Xnew)\n",
    "    machines = fitresult.machines\n",
    "    class_levels = fitresult.class_levels\n",
    "    \n",
    "    # Get predictions from all models\n",
    "    predictions = [categorical(predict_mode(mach, Xnew), levels=class_levels) for mach in machines]\n",
    "    \n",
    "    # Get all unique classes\n",
    "    n_samples = length(predictions[1])\n",
    "    n_models = length(machines)\n",
    "    \n",
    "    # Determine weights (equal if not specified)\n",
    "    weights = model.weights === nothing ? fill(1.0/n_models, n_models) : model.weights\n",
    "    \n",
    "    # Output Vector with the same type as pthe predictions\n",
    "    ensemble_pred = similar(predictions[1])\n",
    "    \n",
    "    for i in 1:n_samples\n",
    "        # Count weighted votes for each class\n",
    "        vote_counts = Dict{eltype(predictions[1][1]), Float64}()\n",
    "        \n",
    "        for (j, prediction) in enumerate(predictions)\n",
    "            vote_counts[prediction[i]] = get(vote_counts, prediction[i], 0.0) + weights[j]\n",
    "        end\n",
    "        \n",
    "        # Have to change this for binary problems(sin usar argmax sobre Dict)\n",
    "        best_label = nothing\n",
    "        best_score = -Inf\n",
    "        for (lbl, sc) in vote_counts\n",
    "            if sc > best_score\n",
    "                best_score = sc\n",
    "                best_label = lbl\n",
    "            end\n",
    "        end\n",
    "\n",
    "        ensemble_pred[i] = best_label\n",
    "    end\n",
    "\n",
    "    return ensemble_pred\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    MLJModelInterface.predict(model::VotingClassifier, fitresult, Xnew)\n",
    "\n",
    "Predict class probabilities using the specified voting strategy.\n",
    "\n",
    "# Arguments\n",
    "- `model::VotingClassifier`: The voting classifier instance\n",
    "- `fitresult`: Vector of trained machines from fit\n",
    "- `Xnew`: New data to predict on\n",
    "\n",
    "# Returns\n",
    "- Vector of `UnivariateFinite` distributions representing class probabilities\n",
    "\n",
    "# Details\n",
    "- For `:hard` voting: Returns deterministic predictions wrapped in UnivariateFinite (with optional weights)\n",
    "- For `:soft` voting: Averages probability distributions from all base models using weights\n",
    "\"\"\"\n",
    "function MLJModelInterface.predict(model::VotingClassifier, fitresult, Xnew)\n",
    "    machines     = fitresult.machines\n",
    "    class_levels = fitresult.class_levels\n",
    "    class_pool   = fitresult.class_pool\n",
    "\n",
    "    result = if model.voting == :hard\n",
    "       # Hard voting, prediction based on majority class\n",
    "        yhat = MLJModelInterface.predict_mode(model, fitresult, Xnew)\n",
    "        yhat = categorical(yhat; levels=class_levels)  # asegura mismos niveles\n",
    "\n",
    "        # Return as one-hot encoded predictions\n",
    "        [MLJBase.UnivariateFinite(\n",
    "                    class_levels,\n",
    "                    [lvl == yhat[i] ? 1.0 : 0.0 for lvl in class_levels];\n",
    "                    pool=class_pool\n",
    "                ) for i in 1:length(yhat)]\n",
    "    else\n",
    "        # Soft voting averaging probabilities\n",
    "        all_predictions = [predict(mach, Xnew) for mach in machines]\n",
    "\n",
    "        n_samples = length(all_predictions[1])\n",
    "        n_models  = length(machines)\n",
    "        n_classes = length(class_levels)\n",
    "        weights   = model.weights === nothing ? fill(1.0/n_models, n_models) : model.weights\n",
    "\n",
    "        avg_probs = zeros(n_samples, n_classes)\n",
    "        for (w, prediction) in zip(weights, all_predictions)\n",
    "            for i in 1:n_samples\n",
    "                p_i = prediction[i]\n",
    "                if p_i isa MLJBase.UnivariateFinite\n",
    "                    for (j, level) in enumerate(class_levels)\n",
    "                        avg_probs[i, j] += w * pdf(p_i, level)\n",
    "                    end\n",
    "                else\n",
    "                    # determinista -> one-hot\n",
    "                    for (j, level) in enumerate(class_levels)\n",
    "                        avg_probs[i, j] += w * (p_i == level ? 1.0 : 0.0)\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # Normalize each probability to prevent numerical issues due to floating point arithmetic\n",
    "        for i in 1:n_samples\n",
    "            s = sum(@view avg_probs[i, :])\n",
    "            if s > 0\n",
    "                @. avg_probs[i, :] = avg_probs[i, :] / s\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # Uses the same pool for consistency than in training\n",
    "        [MLJBase.UnivariateFinite(class_levels, @view avg_probs[i, :]; pool=class_pool)\n",
    "         for i in 1:n_samples]\n",
    "    end\n",
    "\n",
    "    return result\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Model metadata registration for VotingClassifier.\n",
    "\n",
    "Specifies input/output types and capabilities for MLJ integration.\n",
    "\"\"\"\n",
    "MLJModelInterface.metadata_model(VotingClassifier,\n",
    "    input_scitype=Table(Continuous),\n",
    "    target_scitype=AbstractVector{<:Finite},\n",
    "    supports_weights=false,\n",
    "    load_path=\"VotingClassifier\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced2b2cf",
   "metadata": {},
   "source": [
    "## Majority Voting\n",
    "Although also known as Hard Voting, as the name suggests, they are based on selecting the most voted option among the predicted ones among the different models. Each model casts a deterministic vote or prediction. The final class or prediction is the one that receives the most votes or the average among the results. It’s equivalent to a “democratic election” where each model/expert has one vote, and the most-voted option wins. In this way, the problem could be solved taking into account different results or points of view on the problem. \n",
    "\n",
    "### Example\n",
    "\n",
    "With 3 classifiers predicting a pattern:\n",
    "\n",
    "* SVM predicts: **Mine**\n",
    "\n",
    "* Logistic Regression predicts: **Rock**\n",
    "\n",
    "* Naive Bayes predicts: **Rock**\n",
    "\n",
    "**Result:** Rock (2 votes vs 1 vote) ✓\n",
    "\n",
    "See an example in the code below of constructing such a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72a2da84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(VotingClassifier(models = Probabilistic[GaussianNBClassifier(), ProbabilisticSVC(kernel = RadialBasis, …), LogisticClassifier(lambda = 2.220446049250313e-16, …), DecisionTreeClassifier(max_depth = 4, …)], …), …).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB: 78.57142857142857 %\n",
      "SVM: 76.19047619047619 %\n",
      "LR: 73.80952380952381 %\n",
      "DT: 78.57142857142857 %\n",
      "Ensemble (Hard Voting): 76.19047619047619 %\n"
     ]
    }
   ],
   "source": [
    "#Define the metaclassifier based on the base_models\n",
    "models[\"Ensemble (Hard Voting)\"] = VotingClassifier(models = base_models, voting=:hard)\n",
    "machines_dict[\"Ensemble (Hard Voting)\"] = machine(models[\"Ensemble (Hard Voting)\"], train_input, train_output) |> fit!\n",
    "\n",
    "for (name, machine) in machines_dict\n",
    "    acc = MLJ.accuracy(MLJ.mode.(predict(machine, test_input)), test_output)\n",
    "    println(\"$name: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c3971c",
   "metadata": {},
   "source": [
    "The main problem is that we rely equally on all models when deciding on the response class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d50e61",
   "metadata": {},
   "source": [
    "## Soft Voting (Weighted Probability Voting)\n",
    "\n",
    "As mentioned in the previous section, one of the problems of the classical *emsemble* model is that all outcomes are weighted equally and in each of the \"weak\" models only the most voted option is taken into account. To solve this, **Soft Voting** propsose the use of the **probabilities** that each classifier assigns to each class, instead of just the predicted class. The final result is obtained by averaging these probabilities and selecting the class with the highest average probability.\n",
    "\n",
    "### Example without weights (all models equally important)\n",
    "\n",
    "| Classifier    | P(Mine) | P(Rock) |\n",
    "|--------------|---------|---------|\n",
    "| SVM          | 0.9     | 0.1     |\n",
    "| LR           | 0.3     | 0.7     |\n",
    "| NB           | 0.2     | 0.8     |\n",
    "| **Average ** | **0.47**| **0.53**|\n",
    "\n",
    "**Calculation:**\n",
    "- P(Mine) = (0.9 + 0.3 + 0.2) / 3 = 0.47\n",
    "- P(Rock) = (0.1 + 0.7 + 0.8) / 3 = 0.53\n",
    "\n",
    "**Result** Rock (highest average probability) ✓\n",
    "\n",
    "**Advantage over Hard Voting:** Even though SVM is very confident about Mine (0.9), the other two models are quite confident about Rock (0.7 and 0.8). Soft Voting captures this confidence information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7b01ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB: 78.57142857142857 %\n",
      "SVM: 76.19047619047619 %\n",
      "LR: 73.80952380952381 %\n",
      "DT: 78.57142857142857 %\n",
      "Ensemble (Hard Voting): 76.19047619047619 %\n",
      "Ensemble (Soft Voting - Equal): 80.95238095238095 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(VotingClassifier(models = Probabilistic[GaussianNBClassifier(), ProbabilisticSVC(kernel = RadialBasis, …), LogisticClassifier(lambda = 2.220446049250313e-16, …), DecisionTreeClassifier(max_depth = 4, …)], …), …).\n"
     ]
    }
   ],
   "source": [
    "#Define the metaclassifier based on the base_models\n",
    "models[\"Ensemble (Soft Voting - Equal)\"] = VotingClassifier( models = base_models, voting = :soft, weights = nothing) # All models equally weighted\n",
    "machines_dict[\"Ensemble (Soft Voting - Equal)\"] = machine(models[\"Ensemble (Soft Voting - Equal)\"], train_input, train_output) |> fit!\n",
    "\n",
    "for (name, machine) in machines_dict\n",
    "    acc = MLJ.accuracy(MLJ.mode.(predict(machine, test_input)), test_output)\n",
    "    println(\"$name: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf78117f",
   "metadata": {},
   "source": [
    "#### Weigthed Soft Voting \n",
    "Although it could improve because the confidence is taken into account, the main issue is the equality among the models. To solve this, one of the proposals is the use of a weighting in the decision.In many cases, we know that some models perform better than others. For example, if the SVM has an accuracy of 85% and the others are around 70%, we should give more importance to the SVM. This is achieved through weights. In the soft voting, the weights multiply each model’s probabilities before averaging them. Mathematically: \n",
    "\n",
    "$$P(clase) = \\frac{\\sum_{i=1}^{n} w_i \\cdot P_i(clase)}{\\sum_{i=1}^{n} w_i}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $w_i$ = weight of model $i$\n",
    "\n",
    "* $P_i(class)$ = probability assigned by model $i$ to that class\n",
    "\n",
    "* $n$ = number of models\n",
    "\n",
    "Going along with the same example, imagine that we want to increase the importance of the SVM\n",
    "### Example with weights [2, 1, 1] (double weight for SVM)\n",
    "\n",
    "| Classifier   | Weight | P(Mine) | P(Rock)| Mine Contribution | Rock Contribution |\n",
    "|--------------|-------|---------|---------|-------------------|-------------------|\n",
    "| SVM          | 2     | 0.9     | 0.1     | 2 × 0.9 = 1.8     | 2 × 0.1 = 0.2     |\n",
    "| LR           | 1     | 0.3     | 0.7     | 1 × 0.3 = 0.3     | 1 × 0.7 = 0.7     |\n",
    "| NB           | 1     | 0.2     | 0.8     | 1 × 0.2 = 0.2     | 1 × 0.8 = 0.8     |\n",
    "| **Sum**      | 4     |         |         | **2.3**           | **1.7**           |\n",
    "| **Weighted Avg.** |  |         |         | **0.575**         | **0.425** |\n",
    "\n",
    "**Calculation:**\n",
    "- P(Mine) = (1.8 + 0.3 + 0.2) / 4 = 2.3 / 4 = 0.575\n",
    "- P(Rock) = (0.2 + 0.7 + 0.8) / 4 = 1.7 / 4 = 0.425\n",
    "\n",
    "**Result:** Mine (highest weighted probabilit) ✓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "260da3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(VotingClassifier(models = Probabilistic[GaussianNBClassifier(), ProbabilisticSVC(kernel = RadialBasis, …), LogisticClassifier(lambda = 2.220446049250313e-16, …), DecisionTreeClassifier(max_depth = 4, …)], …), …).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB: 78.57142857142857 %\n",
      "SVM: 76.19047619047619 %\n",
      "LR: 73.80952380952381 %\n",
      "Ensembles (Soft Voting - Weighted): 78.57142857142857 %\n",
      "DT: 78.57142857142857 %\n",
      "Ensemble (Hard Voting): 76.19047619047619 %\n",
      "Ensemble (Soft Voting - Equal): 80.95238095238095 %\n"
     ]
    }
   ],
   "source": [
    "models[\"Ensemble (Soft Voting - Weighted)\"] = VotingClassifier(models = base_models, voting=:soft,weights=[1,2,2,1])\n",
    "machines_dict[\"Ensembles (Soft Voting - Weighted)\"] = machine(models[\"Ensemble (Soft Voting - Weighted)\"],train_input, train_output) |> fit!\n",
    "\n",
    "for (name, machine) in machines_dict\n",
    "    acc = MLJ.accuracy(MLJ.mode.(predict(machine, test_input)), test_output)\n",
    "    println(\"$name: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa26ee5d",
   "metadata": {},
   "source": [
    "## When to Use Each Strategy\n",
    "### Hard Voting\n",
    "\n",
    "- Models that only output categorical predictions (no probabilities)\n",
    "\n",
    "- When all models are equally reliable\n",
    "\n",
    "- Simpler and faster\n",
    "\n",
    "### Soft Voting (no weights)\n",
    "\n",
    "- Models that output probabilities\n",
    "\n",
    "- When all models perform similarly\n",
    "\n",
    "- Captures confidence in each prediction\n",
    "\n",
    "### Weighted Soft Voting\n",
    "\n",
    "- When some models are clearly better\n",
    "\n",
    "- When you want to give more importance to specific models\n",
    "\n",
    "- Weights can be based on:\n",
    "\n",
    "    - Validation accuracy\n",
    "    - Known model expertise\n",
    "    - F1-score or another relevant metric\n",
    "\n",
    "In order to chose the weights there are several strategies being the most importan: \n",
    "\n",
    "    1. Manual (based on prior knowledge)\n",
    "    2. Based on validation accuracy\n",
    "    3. Optimization via grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12be268b",
   "metadata": {},
   "source": [
    "### Question 7.2\n",
    "> ❓ We have perform every single test with a hold-out strategy, however, as it was appointed in a previous session, the application of a cross-validation approach is prefered to cut the dependency on the selection of the samples. In this case you could think that there are two different approaches one is apply the cross-validation to each model, choose the better one and combine those in a single ensemble. The other way arround would be applying the cross-validation at ensemble level before training the models. Which one is correct and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2fcbb5",
   "metadata": {},
   "source": [
    "**ANSWER**  \n",
    "It's correct to apply cross-validation at the esnemble level before training models. Because when we combine base models that becomes a new model, and its performance should be evaluated as a hole. The point of cross-validation is to evaluate the final result and understand how the model works on the test data, how good its generalization ablity. Therefore cross-validation only the base models has no sence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f2b304",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "\n",
    "This last approach to combining the models can be considered as a variant of Soft Voting. As mentioned in that section, soft voting allows the weights of each of the models to be fixed and this can be adjusted with a decaying gradient technique. Stacking is usually identified as creating a classification technique superior to a linear regression (which is what Soft Voting does) such as an ANN to combine the models.\n",
    "\n",
    "Thus, as has been done previously, the outputs of the different techniques could be taken and used as inputs to another classification model, allowing for the adjustment of the weights and the non-linear combinations of the responses of each one.\n",
    "\n",
    "You can see an example or this in the following code, which uses the implementation on `MLJ` whcih uses an SVC as compbining model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "128ac43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base models for stacking:\n",
      "  SVM: MLJLIBSVMInterface.ProbabilisticSVC\n",
      "  LR: MLJLinearModels.LogisticClassifier\n",
      "  DT: MLJDecisionTreeInterface.DecisionTreeClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(ProbabilisticStack(metalearner = ProbabilisticSVC(kernel = RadialBasis, …), …), …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:SVM, …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:LR, …).\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSolver: MLJLinearModels.LBFGS{Optim.Options{Float64, Nothing}, @NamedTuple{}}\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m  optim_options: Optim.Options{Float64, Nothing}\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m  lbfgs_options: @NamedTuple{} NamedTuple()\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:DT, …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:SVM, …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:LR, …).\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSolver: MLJLinearModels.LBFGS{Optim.Options{Float64, Nothing}, @NamedTuple{}}\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m  optim_options: Optim.Options{Float64, Nothing}\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m  lbfgs_options: @NamedTuple{} NamedTuple()\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:DT, …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:SVM, …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:LR, …).\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSolver: MLJLinearModels.LBFGS{Optim.Options{Float64, Nothing}, @NamedTuple{}}\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m  optim_options: Optim.Options{Float64, Nothing}\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m  lbfgs_options: @NamedTuple{} NamedTuple()\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:DT, …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:SVM, …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:LR, …).\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSolver: MLJLinearModels.LBFGS{Optim.Options{Float64, Nothing}, @NamedTuple{}}\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m  optim_options: Optim.Options{Float64, Nothing}\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m  lbfgs_options: @NamedTuple{} NamedTuple()\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:DT, …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:SVM, …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:LR, …).\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSolver: MLJLinearModels.LBFGS{Optim.Options{Float64, Nothing}, @NamedTuple{}}\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m  optim_options: Optim.Options{Float64, Nothing}\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m  lbfgs_options: @NamedTuple{} NamedTuple()\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:DT, …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:SVM, …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:LR, …).\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSolver: MLJLinearModels.LBFGS{Optim.Options{Float64, Nothing}, @NamedTuple{}}\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m  optim_options: Optim.Options{Float64, Nothing}\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m  lbfgs_options: @NamedTuple{} NamedTuple()\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:DT, …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(:metalearner, …).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trained Machine; does not cache data\n",
       "  model: ProbabilisticStack(metalearner = ProbabilisticSVC(kernel = RadialBasis, …), …)\n",
       "  args: \n",
       "    1:\tSource @737 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @215 ⏎ AbstractVector{Multiclass{2}}\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a NamedTuple of base models\n",
    "# Note: NaiveBayes is excluded because it can fail with CV on small folds\n",
    "# due to singular covariance matrices when the data is split into multiple folds\n",
    "base_models_for_stack = Dict(\n",
    "    \"SVM\" => SVC(),\n",
    "    \"LR\" => LogisticClassifier(),\n",
    "    \"DT\" => DecisionTreeClassifier()\n",
    ")\n",
    "\n",
    "# Check the types of base models\n",
    "println(\"Base models for stacking:\")\n",
    "for (name, m) in base_models_for_stack\n",
    "    println(\"  $name: \", typeof(m))\n",
    "end\n",
    "\n",
    "base_models_NamedTuple = (; (Symbol(name) => model for (name, model) in base_models_for_stack)...)\n",
    "\n",
    "# Build the stacking model\n",
    "# - resampling=CV(...) define how the out-of-fold predictions are generated\n",
    "# - measures=... only for internal reporting; does not affect final stack training\n",
    "models[\"Ensemble (Stacking)\"] = Stack(; \n",
    "    metalearner = SVC(),\n",
    "    resampling = CV(nfolds=5, shuffle=true, rng=123),\n",
    "    measures = log_loss,\n",
    "    base_models_NamedTuple...  # expands the named tuple of base models\n",
    ")\n",
    "\n",
    "# Train the stacking model on your train dataset\n",
    "machines_dict[\"Ensemble (Stacking)\"] = machine(models[\"Ensemble (Stacking)\"], train_input, train_output) |> fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed598b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB: 78.57142857142857 %\n",
      "SVM: 76.19047619047619 %\n",
      "LR: 73.80952380952381 %\n",
      "Ensembles (Soft Voting - Weighted): 78.57142857142857 %\n",
      "DT: 78.57142857142857 %\n",
      "Ensemble (Hard Voting): 76.19047619047619 %\n",
      "Ensemble (Soft Voting - Equal): 78.57142857142857 %\n",
      "Ensemble (Stacking): 76.19047619047619 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the stacking ensemble\n",
    "for (name, machine) in machines_dict\n",
    "    acc = MLJ.accuracy(predict_mode(machine, test_input), test_output)\n",
    "    println(\"$name: $(acc*100) %\")\n",
    "end\n",
    "\n",
    "# Optional: If you want to include NaiveBayes in stacking, use Holdout instead of CV\n",
    "# Uncomment below to try:\n",
    "# base_models_with_nb = Dict(\n",
    "#     \"NB\" => GaussianNBClassifier(),\n",
    "#     \"SVM\" => SVC(),\n",
    "#     \"LR\" => LogisticClassifier(),\n",
    "#     \"DT\" => DecisionTreeClassifier()\n",
    "# )\n",
    "# nb_tuple = (; (Symbol(name) => model for (name, model) in base_models_with_nb)...)\n",
    "# models[\"Ensemble (Stacking-Holdout)\"] = Stack(; \n",
    "#     metalearner = SVC(),\n",
    "#     resampling = Holdout(fraction_train=0.8, rng=123),  # Use Holdout instead of CV\n",
    "#     measures = log_loss,\n",
    "#     nb_tuple...\n",
    "# )\n",
    "# machines_dict[\"Ensemble (Stacking-Holdout)\"] = machine(models[\"Ensemble (Stacking-Holdout)\"], train_input, train_output) |> fit!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9273829",
   "metadata": {},
   "source": [
    "#### Note on NaiveBayes and Stacking\n",
    "\n",
    "**Why was NaiveBayes excluded from stacking?**\n",
    "\n",
    "During stacking, MLJ uses internal cross-validation (CV) to generate out-of-fold predictions from base models. When using **GaussianNBClassifier** with CV:\n",
    "- Each fold splits the data into smaller training sets\n",
    "- If a fold has too few samples per class, the covariance matrix can become **singular** (not positive definite)\n",
    "- This causes a `PosDefException` error\n",
    "\n",
    "**Solutions:**\n",
    "1. **Exclude NaiveBayes** from stacking (implemented above) ✓\n",
    "2. **Use Holdout instead of CV**: `resampling = Holdout(fraction_train=0.8)` \n",
    "3. **Reduce CV folds**: `resampling = CV(nfolds=3)` instead of 5\n",
    "4. **Use a different NB variant** with regularization (if available)\n",
    "\n",
    "The code above shows an example of using Holdout resampling if you want to include NaiveBayes.\n",
    "\n",
    "## Model creation\n",
    "\n",
    "One of the key elements that has not yet been addressed is the creation of the models that will compose the meta-classifier. So far, the approach that has been followed is not very adequate as the input dataset for all models is the same. This has the effect of an obvious lack of diversity in the models since whichever model we create, it will have the same information or \"point of view\" as the others. However, this is not the usual practice. Instead, the set of input patterns is usually divided into smaller sets with which to train one or more techniques in order to reduce the computational cost on the one hand, and to increase the diversity of the models on the other. It is necessary to remember at this point that \"weak\" models do not have to be perfect in all classes and do not even have to cover all possibilities, only models that are quick to train and offer a more or less consistent output.\n",
    "\n",
    "As for the way in which to partition the data for the creation of the models, most approaches usually consider two main approaches known as *Bagging* and *Boosting*. In the following, these two approaches will be briefly described.\n",
    "\n",
    "### Bagging or boostrap aggregation\n",
    "The technique known as _Bagging_ or selection with replacement was proposed by Breitman in 1996. It is based on the development of multiple models which can be trained in parallel. The key element of these models is that each model is trained on a subset of the training set. This subset of data is drawn randomly with replacement. This last point is particularly important because once an example has been selected from the possibilities, it is placed back among the possibilities so that it can be selected either in the subset being built, or in the subsets of the other models, i.e. non-disjoint sets of examples are created.\n",
    "\n",
    "![Bagging Example](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Ensemble_Bagging.svg/440px-Ensemble_Bagging.svg.png)\n",
    "\n",
    "The result is that \"experts\" are created on specialised data and depending on the partition. While common, or more frequent, data is correctly covered by all models, it is also true that less frequent data tends not to be in all partitions and may not be covered in all cases. Thus, you would get models that would be more specialised in certain data or have a different point of view, that would be experts in a particular region of the search space.\n",
    "\n",
    "Although it will be discussed in more detail later, a well-known technique that uses this approach for the construction of its \"weak\" models is RandomForest. It builds the decision trees that make up the metaclassifier in this way. Any classifier can be used as the basis of a *Bagging* with the class [EnsembleModel](https://juliaai.github.io/MLJ.jl/stable/models/EnsembleModel_MLJEnsembles/#EnsembleModel_MLJEnsembles). \n",
    "\n",
    "For example, in the following code, 10 SVM for classication has been chosen as weak models. Each of those models habe been trained on only 50% of the training patterns, and therefore the variance among them should be increased.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd7750d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(ProbabilisticEnsembleModel(model = ProbabilisticSVC(kernel = RadialBasis, …), …), …).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble-building in parallel on 1 threads.\n",
      "\n",
      "Bagging (SVC): 78.57142857142857 %\n"
     ]
    }
   ],
   "source": [
    "# Add a Bagging model using SVC as base model\n",
    "using MLJEnsembles: EnsembleModel, CPUThreads\n",
    "machines_dict = Dict{String, Any}()\n",
    "models[\"Bagging (SVC)\"] = EnsembleModel(\n",
    "    model = SVC(),              # or ProbabilisticSVC()\n",
    "    n = 10,                     # number of base models\n",
    "    bagging_fraction = 0.50,    # fraction of examples per base model\n",
    "    rng = 123,                  \n",
    "    acceleration = CPUThreads() # uses Threads to speed up the training due to the independence of base models\n",
    ")\n",
    "\n",
    "# machines[\"Bagging (SVC)\"] = machine(models[\"Bagging (SVC)\"], train_input, train_output) |> fit!\n",
    "machines_dict[\"Bagging (SVC)\"] = machine(models[\"Bagging (SVC)\"], train_input, train_output) |> fit!\n",
    "for (name, machine) in machines_dict\n",
    "    acc = MLJ.accuracy(MLJ.mode.(predict(machine, test_input)), test_output)\n",
    "    println(\"$name: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5787a56",
   "metadata": {},
   "source": [
    "As an alternative to extracting complete examples, a vertical partition of the training _dataset_ could be performed, thus extracting features. To implement this alternative, in the `EnsembleModel` function, the parameter *bagging_fraction* must be defined. This approach is used when the number of features is particularly high in order to create simpler models that do not use all the information that is often redundant. It should be noted that this feature extraction procedure for models is done without replacement, i.e. features extracted for one classifier are not re-entered into the list of possibilities until the set for the next classifier is created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0484f377",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "The other major family of techniques for ensemble metamodelling is what is known as *Boosting*. In this case, the approach is slightly different, since the aim is to create a chain of classifiers. The key idea is that every successive classifier becomes **more specialized in the patterns that previous models misclassified**. Therefore, as in the previous case, a subset of patterns is selected from the original set. However, this process is done sequentially and without replacement. This causes the new learners to focus increasingly on the difficult cases, gradually producing a stronger and more accurate composite model. Thus, as in *Bagging*, the underlying idea of this approach is that not all models have to have all patterns as a basis, but unlike _Bagging_, this process is linear because of the dependency in the construction of the models. In the end, the outputs of the individual models are combined through a **weighted majority vote**, where each classifier’s weight reflects its performance during training.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Ensemble_Boosting.svg/1920px-Ensemble_Boosting.svg.png\" alt=\"Boosting examples\" width=\"600\"/>\n",
    "\n",
    "#### AdaBoost\n",
    "\n",
    "The **AdaBoost** algorithm starts by assigning equal weights to all instances in the training set. A simple classifier (called a *stump*, which is a tree of only one level) is trained, and its performance is evaluated. Instances that are misclassified are given higher weights, so that the next classifier focuses more on those difficult cases.  This iterative process continues, each time updating the weights and creating a new weak learner that complements the previous ones. In AdaBoost, the weighting of both instances and classifiers is based on an **exponential loss function**, which penalizes misclassifications exponentially. The final ensemble prediction is obtained through a **weighted majority vote** among all weak classifiers. In MLJ, this behaviour is implemented by the `AdaBoostStumpClassifier`, provided by the `DecisionTree.jl` package.\n",
    "\n",
    "\n",
    "#### Gradient Boosting\n",
    "\n",
    "**Gradient Boosting** follows a different principle: instead of reweighting instances explicitly, it uses a **gradient descent approach** to minimize a loss function. Each new tree in the sequence is trained to predict the **residual errors** (or gradients) of the previous ensemble, gradually refining the model. In the case of classification, each decision tree models the **logistic likelihood** of the data, and its predictions are combined to estimate the class probabilities. The final decision is based on the sum of these probabilities across all trees. In MLJ, this can be implemented using the `EvoTreeClassifier` (from `EvoTrees.jl`), which is conceptually similar to scikit-learn’s `GradientBoostingClassifier`, but is written entirely in Julia and supports both CPU and GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "830a90e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFor silent loading, specify `verbosity=0`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import MLJDecisionTreeInterface ✔\n",
      "import EvoTrees ✔\n",
      "EvoTrees: 76.19047619047619 %\n",
      "AdaBoost: 78.57142857142857 %\n",
      "Bagging (SVC): 78.57142857142857 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFor silent loading, specify `verbosity=0`. \n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(AdaBoostStumpClassifier(n_iter = 30, …), …).\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mThe following kwargs are not supported and will be ignored: [:loss].\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(EvoTrees.EvoTreeClassifier\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - loss: mlogloss\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - metric: mlogloss\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - nrounds: 30\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - bagging_size: 1\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - early_stopping_rounds: 9223372036854775807\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - L2: 1.0\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - lambda: 0.0\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - gamma: 0.0\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - eta: 1.0\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - max_depth: 2\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - min_weight: 1.0\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - rowsample: 1.0\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - colsample: 1.0\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - nbins: 64\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - tree_type: binary\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - seed: 123\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m - device: cpu\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m, …).\n"
     ]
    }
   ],
   "source": [
    "using MLJ\n",
    "\n",
    "# Load model (pure Julia implementations)\n",
    "AdaBoostStumpClassifier = @load AdaBoostStumpClassifier pkg=DecisionTree\n",
    "EvoTreeClassifier = @load EvoTreeClassifier pkg=EvoTrees\n",
    "\n",
    "# AdaBoost (similar to sklearn AdaBoostClassifier with stumps)\n",
    "models[\"AdaBoost\"] = AdaBoostStumpClassifier(n_iter = 30)\n",
    "machines_dict[\"AdaBoost\"] = machine(models[\"AdaBoost\"], train_input, train_output) |> fit!\n",
    "\n",
    "# Gradient Boosting (similar to sklearn GradientBoostingClassifier)\n",
    "models[\"EvoTrees\"] = EvoTreeClassifier(\n",
    "    nrounds=30,\n",
    "    eta=1.0,\n",
    "    max_depth=2,\n",
    "    loss=:logistic\n",
    ")\n",
    "\n",
    "machines_dict[\"EvoTrees\"] = machine(models[\"EvoTrees\"], train_input, train_output) |> fit!\n",
    "\n",
    "for (name, machine) in machines_dict\n",
    "    acc = MLJ.accuracy(MLJ.mode.(predict(machine, test_input)), test_output)\n",
    "    println(\"$name: $(acc*100) %\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43fc04b",
   "metadata": {},
   "source": [
    "### Question 7.3\n",
    "\n",
    "Develop a function to **train and evaluate a homogeneous ensemble** using **`EnsembleModel`** from **MLJ**, where all base estimators are of the **same type** (e.g., SVC, DecisionTree, etc.).  \n",
    "The function, named `trainClassEnsemble`, should follow a **stratified cross-validation** scheme and return at least the test metric(s) value.\n",
    "\n",
    "**Requirements and Steps (summary):**\n",
    "\n",
    "1. **Results vector:** Create a vector of length *k* to store, for each fold, the test metric(s) values.  \n",
    "2. **k-fold loop:** For each iteration, and using the provided stratified indices, build the four data partitions:  \n",
    "   - `X_train`, `y_train`, `X_test`, `y_test`.  \n",
    "3. **Base model generation:** Instantiate the **base model** (from Unit 6) with its **hyperparameters**.  \n",
    "4. **Build the homogeneous ensemble:** Wrap the base model inside an `EnsembleModel` (MLJ), configuring the number of models (`n`) and, if applicable, bagging parameters (`bagging_fraction`, `sampling_fraction`, `n_subfeatures`, etc.).  \n",
    "5. **Training:** Fit the ensemble on the training set (`machine`, `fit!`).  \n",
    "6. **(Optional) Internal validation:** If a validation set is required (e.g., for early stopping), apply a **hold-out** split on `X_train`, `y_train`.  \n",
    "7. **Evaluation:** Compute the chosen metric(s) on the test set for the current fold and store the results.  \n",
    "8. **Aggregation:** Finally, report the **average** and **standard deviation** of each metric across all folds.\n",
    "\n",
    "> **Important:** You must implement this as a **homogeneous ensemble** (all base learners of the same type) using **`EnsembleModel` from MLJ** (bagging approach). *Stacking* or *boosting* is not required here.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47108dcf-44e3-451f-867d-b964a9e3c219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using LIBSVM.fit! in module Main conflicts with an existing identifier.\n",
      "WARNING: using LIBSVM.predict in module Main conflicts with an existing identifier.\n",
      "WARNING: using LIBSVM.SVC in module Main conflicts with an existing identifier.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trainClassEnsemble (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#version3\n",
    "using MLJ\n",
    "using CategoricalArrays\n",
    "using Random\n",
    "using Statistics\n",
    "using LIBSVM\n",
    "\n",
    "SVMClassifier = @load SVC pkg=LIBSVM verbosity=0\n",
    "DTClassifier  = @load DecisionTreeClassifier pkg=DecisionTree verbosity=0\n",
    "KNNClassifier = @load KNNClassifier pkg=NearestNeighborModels verbosity=0\n",
    "\n",
    "function trainClassEnsemble(\n",
    "    estimator::Symbol,\n",
    "    modelsHyperParameters::Dict,\n",
    "    ensembleHyperParameters::Dict,\n",
    "    trainingDataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,2}},\n",
    "    kFoldIndices::Array{Int64,1}\n",
    ")\n",
    "    # 1. Unpack and Prepare Data\n",
    "    X_mat, Y_bool = trainingDataset\n",
    "    n_samples = size(X_mat, 1)\n",
    "\n",
    "    # Convert Boolean/One-Hot to a single vector of Strings\n",
    "    y_raw = Vector{String}(undef, n_samples)\n",
    "    if size(Y_bool, 2) == 1\n",
    "        y_raw .= string.(vec(Y_bool))\n",
    "    else\n",
    "        for i in 1:n_samples\n",
    "            idx = findfirst(identity, @view(Y_bool[i, :]))\n",
    "            if isnothing(idx)\n",
    "                error(\"Sample $i has no true label.\")\n",
    "            end\n",
    "            y_raw[i] = string(idx)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # MLJ: Coerce to Categorical BEFORE splitting\n",
    "    y_categorical = categorical(y_raw)\n",
    "    classes = levels(y_categorical)\n",
    "\n",
    "    # 2. Helper to extract parameters safely\n",
    "    function get_p(dict, key_str, key_sym, default)\n",
    "        val = get(dict, key_str, get(dict, key_sym, default))\n",
    "        return val\n",
    "    end\n",
    "\n",
    "    # 3. Initialize Metrics\n",
    "    metrics_store = Dict(\n",
    "        :acc  => Float64[], :err  => Float64[],\n",
    "        :sens => Float64[], :spec => Float64[],\n",
    "        :ppv  => Float64[], :npv  => Float64[],\n",
    "        :f1   => Float64[]\n",
    "    )\n",
    "\n",
    "    # Storage for internal validation\n",
    "    val_metrics_store = Dict(\n",
    "        :acc  => Float64[], :err  => Float64[],\n",
    "        :sens => Float64[], :spec => Float64[],\n",
    "        :ppv  => Float64[], :npv  => Float64[],\n",
    "        :f1   => Float64[]\n",
    "    )\n",
    "\n",
    "    global_confusion = zeros(Int, length(classes), length(classes))\n",
    "\n",
    "    numFolds = maximum(kFoldIndices)\n",
    "\n",
    "    # 4. K-Fold Loop\n",
    "    for fold in 1:numFolds\n",
    "        # Stratified Split по заранее заданным индексам\n",
    "        test_mask  = (kFoldIndices .== fold)\n",
    "        train_mask = .!test_mask\n",
    "\n",
    "        X_train = X_mat[train_mask, :]\n",
    "        X_test  = X_mat[test_mask, :]\n",
    "        y_train = y_categorical[train_mask]\n",
    "        y_test  = y_categorical[test_mask]\n",
    "\n",
    "        # 5. Base Model Generation\n",
    "        base_model = nothing\n",
    "\n",
    "        if estimator == :SVC || estimator == :SVMClassifier\n",
    "            C     = get_p(modelsHyperParameters, \"C\",        :C,        1.0)\n",
    "            kern  = get_p(modelsHyperParameters, \"kernel\",   :kernel,   \"rbf\")\n",
    "            gamma = get_p(modelsHyperParameters, \"gamma\",    :gamma,    0.1)\n",
    "            deg   = get_p(modelsHyperParameters, \"degree\",   :degree,   3)\n",
    "            coef0 = get_p(modelsHyperParameters, \"coef0\",    :coef0,    0.0)\n",
    "\n",
    "            k_sym = Symbol(lowercase(string(kern)))\n",
    "            kernel_val = if k_sym == :linear\n",
    "                LIBSVM.Kernel.Linear\n",
    "            elseif k_sym == :sigmoid\n",
    "                LIBSVM.Kernel.Sigmoid\n",
    "            elseif k_sym in [:poly, :polynomial]\n",
    "                LIBSVM.Kernel.Polynomial\n",
    "            else\n",
    "                LIBSVM.Kernel.RadialBasis\n",
    "            end\n",
    "\n",
    "            base_model = SVMClassifier(\n",
    "                kernel = kernel_val,\n",
    "                cost   = Float64(C),\n",
    "                gamma  = Float64(gamma),\n",
    "                degree = Int32(deg),\n",
    "                coef0  = Float64(coef0)\n",
    "            )\n",
    "\n",
    "        elseif estimator == :DecisionTreeClassifier\n",
    "            depth = get_p(modelsHyperParameters, \"max_depth\", :max_depth, -1)\n",
    "            base_model = DTClassifier(max_depth = depth,\n",
    "                                      rng       = Random.MersenneTwister(1))\n",
    "\n",
    "        elseif estimator == :kNN || estimator == :KNN || estimator == :KNeighborsClassifier\n",
    "            K_val = get_p(modelsHyperParameters, \"n_neighbors\", :n_neighbors,\n",
    "                          get_p(modelsHyperParameters, \"K\", :K, 3))\n",
    "            base_model = KNNClassifier(K = Int(K_val))\n",
    "        else\n",
    "            error(\"Unknown estimator: $estimator\")\n",
    "        end\n",
    "\n",
    "        # 6. Build Homogeneous Ensemble (EnsembleModel)\n",
    "        n_estimators = get_p(ensembleHyperParameters, \"n\", :n, 10)\n",
    "        bag_frac     = get_p(ensembleHyperParameters, \"bagging_fraction\", :bagging_fraction, 1.0)\n",
    "        rng_val      = get_p(ensembleHyperParameters, \"rng\", :rng, Random.MersenneTwister(1))\n",
    "\n",
    "        rng_obj = isa(rng_val, Integer) ? Random.MersenneTwister(rng_val) : rng_val\n",
    "\n",
    "        ensemble_model = EnsembleModel(\n",
    "            model           = base_model,\n",
    "            n               = n_estimators,\n",
    "            bagging_fraction = bag_frac,\n",
    "            rng             = rng_obj\n",
    "        )\n",
    "\n",
    "        # 7. Internal validation (hold-out на train)\n",
    "        val_fraction = get_p(ensembleHyperParameters, \"val_fraction\", :val_fraction, 0.2)\n",
    "        \n",
    "        # Share for INTERNAL training:\n",
    "        train_inner_fraction = 1.0 - val_fraction \n",
    "\n",
    "        train_inner_idx, val_inner_idx = MLJ.partition(\n",
    "            eachindex(y_train),\n",
    "            train_inner_fraction; \n",
    "            shuffle = true,\n",
    "            rng    = rng_obj\n",
    "        )\n",
    "        X_train_inner = X_train[train_inner_idx, :]\n",
    "        X_val         = X_train[val_inner_idx, :]\n",
    "        y_train_inner = y_train[train_inner_idx]\n",
    "        y_val         = y_train[val_inner_idx]\n",
    "\n",
    "        # Teach the model on (train_inner) and calculate metrics on validation\n",
    "        mach_val = machine(ensemble_model, MLJ.table(X_train_inner), y_train_inner)\n",
    "        fit!(mach_val, verbosity=0)\n",
    "\n",
    "        val_preds = predict(mach_val, MLJ.table(X_val))\n",
    "\n",
    "        val_pred_labels = if val_preds[1] isa MLJ.UnivariateFinite\n",
    "            string.(mode.(val_preds))\n",
    "        else\n",
    "            string.(val_preds)\n",
    "        end\n",
    "\n",
    "        y_val_str = string.(y_val)\n",
    "\n",
    "        (v_acc, v_err, v_sens, v_spec, v_ppv, v_npv, v_f1, _) =\n",
    "            UtilsML1.confusionMatrix(val_pred_labels, y_val_str, string.(classes))\n",
    "\n",
    "        push!(val_metrics_store[:acc],  v_acc)\n",
    "        push!(val_metrics_store[:err],  v_err)\n",
    "        push!(val_metrics_store[:sens], v_sens)\n",
    "        push!(val_metrics_store[:spec], v_spec)\n",
    "        push!(val_metrics_store[:ppv],  v_ppv)\n",
    "        push!(val_metrics_store[:npv],  v_npv)\n",
    "        push!(val_metrics_store[:f1],   v_f1)\n",
    "\n",
    "        # 8. Training на всём train для тестовой оценки\n",
    "        mach = machine(ensemble_model, MLJ.table(X_train), y_train)\n",
    "        fit!(mach, verbosity=0)\n",
    "\n",
    "        # 9. Evaluation на тесте\n",
    "        preds = predict(mach, MLJ.table(X_test))\n",
    "\n",
    "        y_pred_labels = if preds[1] isa MLJ.UnivariateFinite\n",
    "            string.(mode.(preds))\n",
    "        else\n",
    "            string.(preds)\n",
    "        end\n",
    "\n",
    "        y_test_str = string.(y_test)\n",
    "\n",
    "        (acc, err, sens, spec, ppv, npv, f1, conf) =\n",
    "            UtilsML1.confusionMatrix(y_pred_labels, y_test_str, string.(classes))\n",
    "\n",
    "        push!(metrics_store[:acc],  acc)\n",
    "        push!(metrics_store[:err],  err)\n",
    "        push!(metrics_store[:sens], sens)\n",
    "        push!(metrics_store[:spec], spec)\n",
    "        push!(metrics_store[:ppv],  ppv)\n",
    "        push!(metrics_store[:npv],  npv)\n",
    "        push!(metrics_store[:f1],   f1)\n",
    "\n",
    "        global_confusion .+= conf\n",
    "    end\n",
    "\n",
    "    # 10. Aggregation\n",
    "    agg(v) = (mean(v), std(v; corrected = false))\n",
    "\n",
    "    return (\n",
    "        agg(metrics_store[:acc]),\n",
    "        agg(metrics_store[:err]),\n",
    "        agg(metrics_store[:sens]),\n",
    "        agg(metrics_store[:spec]),\n",
    "        agg(metrics_store[:ppv]),\n",
    "        agg(metrics_store[:npv]),\n",
    "        agg(metrics_store[:f1]),\n",
    "        global_confusion\n",
    "    )\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63507595-9e68-413c-ad25-88e2fe11b899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULTS(Test Set)\n",
      "Accuracy          : 81.4% ± 4.86%\n",
      "Error             : 18.6% ± 4.86%\n",
      "Sensitivity       : 86.47% ± 7.63%\n",
      "Specificity       : 75.67% ± 14.87%\n",
      "Precision (PPV)   : 81.48% ± 8.79%\n",
      "NPV               : 84.6% ± 8.01%\n",
      "F1-score          : 83.25% ± 3.6%\n",
      "\n",
      "Global Confusion (Test Set):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Int64}:\n",
       " 59  19\n",
       " 12  76"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using .UtilsML1:oneHotEncoding\n",
    "try\n",
    "    global train_output_bool = UtilsML1.oneHotEncoding(train_output)\n",
    "catch e\n",
    "    println(\"\\n[ERROR] 'UtilsML1.oneHotEncoding' function not found.\")\n",
    "    println(\"Please make sure it is defined in 'utils_ML1.jl'.\")\n",
    "    rethrow(e)\n",
    "end\n",
    "\n",
    "\n",
    "#  Support function for K-Fold (Creates stratified K-fold indices from a categorical vector.)\n",
    "function stratified_kfold_indices(y::CategoricalVector, k::Int; rng = Random.MersenneTwister(123))\n",
    "    N = length(y)\n",
    "    folds = zeros(Int, N)\n",
    "    for lvl in levels(y)\n",
    "        # Find indices for each class\n",
    "        idxs = findall(==(lvl), y)\n",
    "        Random.shuffle!(rng, idxs)\n",
    "        # Distribute it by folds (1, 2, ..., k, 1, 2, ...)\n",
    "        for (i, idx) in enumerate(idxs)\n",
    "            folds[idx] = ((i - 1) % k) + 1\n",
    "        end\n",
    "    end\n",
    "    return folds\n",
    "end\n",
    "\n",
    "# Generate K-Fold indices\n",
    "k = 5 \n",
    "kFoldIndices = stratified_kfold_indices(train_output, k)\n",
    "\n",
    "\n",
    "# Definition of hyperparameters for the test\n",
    "\n",
    "#We will test the ensemble from Trees of Solutions\n",
    "test_estimator = :DecisionTreeClassifier\n",
    "\n",
    "test_model_params = Dict{Any,Any}(\n",
    "    \"max_depth\" => 4,\n",
    "    \"min_samples_leaf\" => 3\n",
    ")\n",
    "\n",
    "test_ensemble_params = Dict{Any,Any}(\n",
    "    \"n\" => 50,                # 50 trees\n",
    "    \"bagging_fraction\" => 0.8, # 80% data per tree\n",
    "    \"val_fraction\" => 0.25    # 25% of K-fold train on internal loading\n",
    ")\n",
    "\n",
    "\n",
    "# Function call\n",
    "\n",
    "\n",
    "results = trainClassEnsemble(\n",
    "    test_estimator,\n",
    "    test_model_params,\n",
    "    test_ensemble_params,\n",
    "    (train_input_matrix, train_output_bool),\n",
    "    kFoldIndices\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# --- Results ---\n",
    "\n",
    "println(\"\\nRESULTS(Test Set)\")\n",
    "metric_names = [\n",
    "    \"Accuracy\", \"Error\", \"Sensitivity\", \"Specificity\",\n",
    "    \"Precision (PPV)\", \"NPV\", \"F1-score\"\n",
    "]\n",
    "\n",
    "# results[1] - is (mean, std) for accuracy\n",
    "# results[2] - is (mean, std) for error и т.д.\n",
    "# results[end] - is confusion matrix\n",
    "\n",
    "for (name, (mean_val, std_val)) in zip(metric_names, results[1:end-1])\n",
    "    println(\"$(rpad(name, 18)): $(round(mean_val*100, digits=2))% ± $(round(std_val*100, digits=2))%\")\n",
    "end\n",
    "\n",
    "println(\"\\nGlobal Confusion (Test Set):\")\n",
    "display(results[end]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a265be0c",
   "metadata": {},
   "source": [
    "### Question 7.4\n",
    "> ❓ Repeated the previous function, but this time for a heterogeneous Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "128d7aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainClassEnsemble (generic function with 2 methods)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function trainClassEnsemble(estimators:AbstractArray{Symbol, 1}, \n",
    "#         modelsHyperParameters:: AbstractArray{Dict, 1},\n",
    "#         ensembleHyperParameters:: Dict,     \n",
    "#         trainingDataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,2}},    \n",
    "#         kFoldIndices::     Array{Int64,1})\n",
    "#     #TODO\n",
    "# end\n",
    "using .UtilsML1\n",
    "function trainClassEnsemble(\n",
    "    estimators::AbstractVector{Symbol}, \n",
    "    modelsHyperParameters, \n",
    "    ensembleHyperParameters::Dict,\n",
    "    trainingDataset::Tuple{AbstractMatrix{<:Real}, AbstractArray{Bool,2}},\n",
    "    kFoldIndices::Vector{Int}\n",
    ")\n",
    "    # 1. Initialization and data preprocessing\n",
    "    inputs, outputs_bool = trainingDataset\n",
    "    N = size(outputs_bool, 1)\n",
    "    num_classes = size(outputs_bool, 2)\n",
    "    # Convert boolean label matrix to 1D string labels\n",
    "    labels = Vector{String}(undef, N)\n",
    "    if num_classes == 1\n",
    "        # Binary case: use \"true\"/\"false\" as class labels\n",
    "        for i in 1:N\n",
    "            labels[i] = outputs_bool[i, 1] ? \"true\" : \"false\"\n",
    "        end\n",
    "    else\n",
    "        # Multi-class case: assign \"Class1\", \"Class2\", ... based on one-hot position\n",
    "        for i in 1:N\n",
    "            # find the index of the true label in one-hot row i\n",
    "            local idx = findfirst(outputs_bool[i, :] .== true)\n",
    "            @assert idx !== nothing \"No class label found for sample $i\"\n",
    "            labels[i] = \"Class$(idx)\"\n",
    "        end\n",
    "    end\n",
    "    # Unique class names for confusion matrix calculations\n",
    "    classes = unique(labels)\n",
    "    # Metric accumulators for each fold\n",
    "    accuracy_folds = Float64[]; error_folds = Float64[]\n",
    "    sensitivity_folds = Float64[]; specificity_folds = Float64[]\n",
    "    ppv_folds = Float64[]; npv_folds = Float64[]; f1_folds = Float64[]\n",
    "    # Global confusion matrix (initialized to all zeros)\n",
    "    global_confusion = zeros(Int, length(classes), length(classes))\n",
    "    # Determine number of folds from kFoldIndices\n",
    "    numFolds = maximum(kFoldIndices)\n",
    "    \n",
    "    # Helper to retrieve hyperparameter values (accepts string or symbol keys)\n",
    "    getParam = function(dict, key_str::AbstractString, key_sym::Symbol, default)\n",
    "        if haskey(dict, key_str)\n",
    "            return dict[key_str]\n",
    "        elseif haskey(dict, key_sym)\n",
    "            return dict[key_sym]\n",
    "        else\n",
    "            return default\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Load model types (if not already loaded in the environment)\n",
    "    ProbabilisticSVC = MLJ.@load ProbabilisticSVC pkg=LIBSVM verbosity=0\n",
    "    DecisionTreeClassifier = MLJ.@load DecisionTreeClassifier pkg=DecisionTree verbosity=0\n",
    "    KNNClassifier = MLJ.@load KNNClassifier pkg=NearestNeighborModels verbosity=0\n",
    "    \n",
    "    # 4. Cross-validation loop over each fold\n",
    "    for fold in 1:numFolds\n",
    "        # Split indices for this fold\n",
    "        local test_mask = (kFoldIndices .== fold)\n",
    "        local train_mask = (kFoldIndices .!= fold)\n",
    "        # Partition the data into training and testing sets\n",
    "        local train_inputs = inputs[train_mask, :]\n",
    "        local test_inputs  = inputs[test_mask, :]\n",
    "        local train_targets = labels[train_mask]   # string vector\n",
    "        local test_targets  = labels[test_mask]    # string vector\n",
    "        # 2. Instantiate each base model for this fold\n",
    "        local base_models = Vector{Any}()  # will hold model instances\n",
    "        for (est, hyp) in zip(estimators, modelsHyperParameters)\n",
    "            local model = nothing\n",
    "            # Determine which model to build based on `est` symbol\n",
    "            if est == :SVC || est == :SVMClassifier || est == :ProbabilisticSVC\n",
    "                # Support Vector Classifier (LIBSVM)\n",
    "                local C      = getParam(hyp, \"C\", :C, 1.0)\n",
    "                local kernel = getParam(hyp, \"kernel\", :kernel, \"rbf\")\n",
    "                local gamma  = getParam(hyp, \"gamma\", :gamma, 0.1)\n",
    "                local degree = getParam(hyp, \"degree\", :degree, 3)\n",
    "                local coef0  = getParam(hyp, \"coef0\", :coef0, 0.0)\n",
    "                # Choose kernel type\n",
    "                local kernel_type = kernel == \"linear\"       ? LIBSVM.Kernel.Linear :\n",
    "                                     kernel == \"rbf\"          ? LIBSVM.Kernel.RadialBasis :\n",
    "                                     kernel == \"sigmoid\"      ? LIBSVM.Kernel.Sigmoid :\n",
    "                                     (kernel == \"poly\" || kernel == \"polynomial\") ? LIBSVM.Kernel.Polynomial :\n",
    "                                     LIBSVM.Kernel.RadialBasis  # default to RBF\n",
    "                # Instantiate SVM model with appropriate parameters\n",
    "                if kernel_type == LIBSVM.Kernel.Linear\n",
    "                    model = ProbabilisticSVC(kernel=LIBSVM.Kernel.Linear, cost=Float64(C))\n",
    "                elseif kernel_type == LIBSVM.Kernel.RadialBasis && kernel == \"rbf\"\n",
    "                    model = ProbabilisticSVC(kernel=LIBSVM.Kernel.RadialBasis, cost=Float64(C), gamma=Float64(gamma))\n",
    "                elseif kernel_type == LIBSVM.Kernel.Sigmoid\n",
    "                    model = ProbabilisticSVC(kernel=LIBSVM.Kernel.Sigmoid, cost=Float64(C), gamma=Float64(gamma), coef0=Float64(coef0))\n",
    "                elseif kernel_type == LIBSVM.Kernel.Polynomial\n",
    "                    model = ProbabilisticSVC(kernel=LIBSVM.Kernel.Polynomial, cost=Float64(C), gamma=Float64(gamma),\n",
    "                                            degree=Int32(degree), coef0=Float64(coef0))\n",
    "                else\n",
    "                    model = ProbabilisticSVC(kernel=LIBSVM.Kernel.RadialBasis, cost=Float64(C), gamma=Float64(gamma))\n",
    "                end\n",
    "            elseif est == :DecisionTree || est == :DecisionTreeClassifier\n",
    "                # Decision Tree classifier\n",
    "                local max_depth = getParam(hyp, \"max_depth\", :max_depth, -1)\n",
    "                model = DecisionTreeClassifier(max_depth=max_depth, rng=Random.MersenneTwister(1))\n",
    "            elseif est == :kNN || est == :KNNClassifier || est == :KNeighborsClassifier\n",
    "                # k-Nearest Neighbors classifier\n",
    "                local K_val = getParam(hyp, \"K\", :K, 3)\n",
    "                local n_neighbors = getParam(hyp, \"n_neighbors\", :n_neighbors, K_val)\n",
    "                model = KNNClassifier(K = n_neighbors)\n",
    "            else\n",
    "                error(\"Unsupported model type: $est\")\n",
    "            end\n",
    "            push!(base_models, model)\n",
    "        end\n",
    "        \n",
    "        # 3. Create the VotingClassifier ensemble for this fold\n",
    "        local voting_strategy = getParam(ensembleHyperParameters, \"voting\", :voting, :hard)\n",
    "        local weights = getParam(ensembleHyperParameters, \"weights\", :weights, nothing)\n",
    "        # Initialize ensemble model with base models, voting type, and weights\n",
    "        local ensemble_model = VotingClassifier(models=base_models, voting=voting_strategy, weights=weights)\n",
    "        # Train the ensemble on this fold's training data\n",
    "        local ensemble_machine = MLJ.machine(ensemble_model, MLJ.table(train_inputs), categorical(train_targets))\n",
    "        MLJ.fit!(ensemble_machine, verbosity=0)\n",
    "        \n",
    "        # Evaluate on the test set\n",
    "        # Get predicted class labels (use predict_mode for final ensemble prediction)\n",
    "        local y_pred = MLJ.predict_mode(ensemble_machine, MLJ.table(test_inputs))\n",
    "        y_pred = string.(y_pred)  # convert predictions to String vector\n",
    "        # True labels for this test fold (already strings in test_targets)\n",
    "        local y_true = test_targets\n",
    "        # 5. Compute metrics using confusionMatrix from UtilsML1\n",
    "        local (acc, err, sens, spec, ppv, npv, f1, confusion) = confusionMatrix(y_pred, y_true, classes)\n",
    "        # Store metrics for this fold\n",
    "        push!(accuracy_folds, acc)\n",
    "        push!(error_folds, err)\n",
    "        push!(sensitivity_folds, sens)\n",
    "        push!(specificity_folds, spec)\n",
    "        push!(ppv_folds, ppv)\n",
    "        push!(npv_folds, npv)\n",
    "        push!(f1_folds, f1)\n",
    "        # Accumulate the confusion matrix\n",
    "        global_confusion .+= confusion\n",
    "    end\n",
    "    \n",
    "    # 6. Compute mean and standard deviation of metrics across folds\n",
    "    stats(v) = (mean(v), std(v; corrected=false))\n",
    "    return (\n",
    "        stats(accuracy_folds),\n",
    "        stats(error_folds),\n",
    "        stats(sensitivity_folds),\n",
    "        stats(specificity_folds),\n",
    "        stats(ppv_folds),\n",
    "        stats(npv_folds),\n",
    "        stats(f1_folds),\n",
    "        global_confusion\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9dff69a-cbc4-4bac-926a-0d1274720b10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.41% ± 3.13%\n",
      "Error: 15.59% ± 3.13%\n",
      "Sensitivity: 87.08% ± 4.36%\n",
      "Specificity: 82.03% ± 7.96%\n",
      "Precision (PPV): 81.88% ± 6.7%\n",
      "NPV: 87.99% ± 3.0%\n",
      "F1-score: 84.09% ± 2.52%\n",
      "Confusion matrix (global):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Int64}:\n",
       " 72  16\n",
       " 10  68"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using MLJ\n",
    "using LIBSVM\n",
    "using Random\n",
    "using CategoricalArrays\n",
    "# using .UtilsML1: oneHotEncoding, calculateMinMaxNormalizationParameters, holdOut, normalizeMinMax\n",
    "using .UtilsML1\n",
    "Random.seed!(123)\n",
    "\n",
    "# --- 1. Hold-out + нормализация (в виде матриц, НЕ MLJ.table) ---\n",
    "\n",
    "# train_indices, test_indices = UtilsML1.holdOut(size(input_data, 1), 0.2)\n",
    "\n",
    "# train_input_raw = input_data[train_indices, :]\n",
    "# test_input_raw  = input_data[test_indices, :]\n",
    "\n",
    "# train_output = output_data[train_indices]\n",
    "# test_output  = output_data[test_indices]\n",
    "\n",
    "# norm_params = UtilsML1.calculateMinMaxNormalizationParameters(train_input_raw)\n",
    "# train_input_matrix = UtilsML1.normalizeMinMax(train_input_raw, norm_params)\n",
    "# test_input_matrix  = UtilsML1.normalizeMinMax(test_input_raw, norm_params)\n",
    "\n",
    "# --- One-hot для выходов (Bool-матрица, как требует trainClassEnsemble)  ---\n",
    "\n",
    "train_output_bool = UtilsML1.oneHotEncoding(train_output)  # BitMatrix <: AbstractArray{Bool,2}\n",
    "\n",
    "# --- Stratified k-fold indices by original categorical train_output ----\n",
    "\n",
    "function stratified_kfold_indices(y::CategoricalVector, k::Int; rng = Random.MersenneTwister(123))\n",
    "    N = length(y)\n",
    "    folds = zeros(Int, N)\n",
    "    for lvl in levels(y)\n",
    "        idxs = findall(==(lvl), y)\n",
    "        Random.shuffle!(rng, idxs)\n",
    "        for (i, idx) in enumerate(idxs)\n",
    "            folds[idx] = ((i - 1) % k) + 1\n",
    "        end\n",
    "    end\n",
    "    return folds\n",
    "end\n",
    "\n",
    "k = 5\n",
    "kFoldIndices = stratified_kfold_indices(train_output, k)\n",
    "\n",
    "# --- Heterogeneous ensemble: models and hyperparameters ---\n",
    "\n",
    "estimators = Symbol[:ProbabilisticSVC, :DecisionTreeClassifier, :KNNClassifier]\n",
    "\n",
    "modelsHyperParameters = Dict{Any,Any}[\n",
    "    Dict{Any,Any}(\"C\" => 1.0, \"kernel\" => \"rbf\", \"gamma\" => 0.1),  # SVC\n",
    "    Dict{Any,Any}(\"max_depth\" => 4),                                # DT\n",
    "    Dict{Any,Any}(\"K\" => 3)                                         # kNN\n",
    "]\n",
    "\n",
    "ensembleHyperParameters = Dict{Any,Any}(\n",
    "    \"voting\"  => :soft,                  \n",
    "    \"weights\" => [2.0, 1.0, 1.0]\n",
    ")\n",
    "\n",
    "# --- 5. Calling trainClassEnsemble ---\n",
    "\n",
    "results = trainClassEnsemble(\n",
    "    estimators,\n",
    "    modelsHyperParameters,\n",
    "    ensembleHyperParameters,\n",
    "    (train_input_matrix, train_output_bool),\n",
    "    kFoldIndices\n",
    ")\n",
    "\n",
    "# --- Mertics ---\n",
    "\n",
    "metric_names = [\n",
    "    \"Accuracy\", \"Error\", \"Sensitivity\", \"Specificity\",\n",
    "    \"Precision (PPV)\", \"NPV\", \"F1-score\"\n",
    "]\n",
    "\n",
    "for (name, (mean_val, std_val)) in zip(metric_names, results[1:end-1])\n",
    "    println(\"$name: $(round(mean_val*100, digits=2))% ± $(round(std_val*100, digits=2))%\")\n",
    "end\n",
    "\n",
    "println(\"Confusion matrix (global):\")\n",
    "display(results[end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1154068c",
   "metadata": {},
   "source": [
    "## Techniques integrating the Ensemble approach\n",
    "\n",
    "Some of the best-known and currently used algorithms are based on this type of approach. Among these approaches, perhaps the most famous and widely used are those based on the generation of simple Decision Tress (DT). The reason for the use of the trees is their easy interpretation, as well as the speed of calculation and training. In the following we will see the two approaches known today in this sense, ***Random Forest*** and ***XGBoost***.\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "The **Random Forest** algorithm, proposed by Breiman and Cutler in 2006 (building upon an earlier idea by Ho in 1995, known as *Random Subspaces*), is one of the most representative examples of ensemble learning. It combines multiple simple classifiers — in this case, **Decision Trees (DTs)** — into a single, more robust model. Each tree in the forest is trained on a **bootstrap sample** (a random subset with replacement) of the original data, following a *bagging* approach. Because each tree is trained independently, the process can be fully **parallelized**. For classification problems, the final prediction is obtained by **majority vote** among all trees; for regression, by averaging their outputs.\n",
    "\n",
    "Random Forests are known for performing remarkably well with **minimal hyperparameter tuning**. Typically, the most important parameter is the number of trees (`n_trees` in MLJ or `n_estimators` in scikit-learn), which controls the size of the ensemble. A common heuristic suggests using:\n",
    "\n",
    "- *$\\sqrt{\\textrm{\\#feature}}$* for classification tasks  \n",
    "- *$\\frac{\\textrm{\\#feature}}{3}$* for regression tasks  \n",
    "\n",
    "Although increasing the number of trees usually improves performance, it tends to **saturate** beyond 500–1000 trees in most practical cases.\n",
    "\n",
    "In addition to the bootstrapping process, Random Forests introduce a **second level of randomness**: at each node split, only a random subset of features is considered as candidates for partitioning. This enhances the **diversity of the trees** and helps reduce the model’s variance while maintaining strong predictive power. An important byproduct of this mechanism is the ability to **quantify feature importance**. By analysing how much each variable contributes to the reduction of node impurity across all trees, Random Forests can estimate the relative importance of each feature.  \n",
    "This impurity-based importance is often used for **feature selection**. The most common impurity metric is the **Gini index**, defined as:\n",
    "\n",
    "$$G = \\sum_{i=1}^C p(i) * (1 - p(i))$$\n",
    "\n",
    "where $ C $ is the number of classes and $ p(i) $ is the probability of randomly selecting an instance of class $ i $.  Intuitively, it measures the probability of incorrectly classifying a randomly chosen instance if labels were assigned according to the class distribution. For an excellent visual explanation, see [this reference](https://victorzhou.com/blog/gini-impurity/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "668be4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mFor silent loading, specify `verbosity=0`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import MLJDecisionTreeInterface ✔\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(RandomForestClassifier(max_depth = -1, …), …).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvoTrees: 76.19047619047619 %\n",
      "AdaBoost: 78.57142857142857 %\n",
      "RF: 76.19047619047619 %\n",
      "Bagging (SVC): 78.57142857142857 %\n"
     ]
    }
   ],
   "source": [
    "using MLJ\n",
    "using Plots\n",
    "\n",
    "# Load the native Julia Random Forest model\n",
    "RandomForestClassifier = @load RandomForestClassifier pkg=DecisionTree\n",
    "\n",
    "# Define the model\n",
    "models[\"RF\"] = RandomForestClassifier(\n",
    "    n_trees=8,              \n",
    "    max_depth=-1,           \n",
    "    min_samples_split=2,\n",
    "    n_subfeatures=-1,       \n",
    "    sampling_fraction=1.0   \n",
    ")\n",
    "\n",
    "# Train The modeel\n",
    "machines_dict[\"RF\"] = machine(models[\"RF\"], train_input, train_output) |> fit!\n",
    "\n",
    "    \n",
    "# Evaluate accuracy\n",
    "for (name, machine) in machines_dict\n",
    "    acc = MLJ.accuracy(MLJ.mode.(predict(machine, test_input)), test_output)\n",
    "    println(\"$name: $(acc*100) %\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dbe372",
   "metadata": {},
   "source": [
    "### Key Hyperparameters\n",
    "\n",
    "| **Parameter**        | **Description** |\n",
    "|-----------------------|-----------------|\n",
    "| `n_trees`             | Number of trees in the forest (equivalent to `n_estimators` in scikit-learn). |\n",
    "| `max_depth`           | Maximum depth of each tree. Use `-1` for no limit. |\n",
    "| `min_samples_split`   | Minimum number of samples required to split a node. |\n",
    "| `n_subfeatures`       | Number of random features considered at each split (`-1` uses √(#features)). |\n",
    "| `sampling_fraction`   | Fraction of training samples used to build each tree (bootstrapping). |\n",
    "| `rng`                 | Random number generator for reproducibility. |\n",
    "\n",
    "In this example, the number of trees (`n_trees`) is defined following the heuristic of $\\sqrt{\\textrm{\\#features}}$. Because the dataset used in this example is relatively small, results may vary slightly between runs depending on the random partitions used for training.\n",
    "\n",
    "#### Feature Importance\n",
    "Once the model has been trained, we can compute the feature importance based on the average Gini impurity reduction across all trees in the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea85f5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract fitted model parameters\n",
    "fitted_model = fitted_params(machines_dict[\"RF\"])\n",
    "\n",
    "# Obtain feature importance values\n",
    "feature_importances = fitted_model.feature_importance\n",
    "\n",
    "# Plot feature importance\n",
    "p = bar(\n",
    "    1:length(feature_importances),\n",
    "    feature_importances,\n",
    "    orientation = :horizontal,\n",
    "    legend = false\n",
    ")\n",
    "xlabel!(p, \"Gini Gain\")\n",
    "ylabel!(p, \"Feature\")\n",
    "title!(p, \"Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4feb1ec",
   "metadata": {},
   "source": [
    "As shown in the plot, most of the predictive information may be concentrated in a small number of features.\n",
    "Therefore, this metric can also be used for feature filtering or selection, which will be discussed in subsequent sections.\n",
    "\n",
    "Random Forests represent one of the most robust and widely used ensemble methods.\n",
    "They leverage bagging and feature randomness to build diverse trees, reducing variance and improving generalisation.\n",
    "Moreover, they naturally provide interpretable measures such as feature importance, making them not only powerful predictors but also useful tools for exploratory data analysis.\n",
    "\n",
    "### XGBoost (eXtreme Gradient Boosting)\n",
    "\n",
    "Finally, in this last section, Gradient Boosting should be mentioned again, specifically an implementation that in recent years has become very famous for its versatility and speed. This implementation is known as ***XGBoost (eXtreme Gradient Boosting)*** , which has stood out especially in competitions such as the Kaggle platform for its speed in obtaining results and robustness. \n",
    "\n",
    "The ***XGBoost*** will be a similar ensemble to Random Forest but uses a different base classifier known as CART (classification and regression trees) instead of *Decision Trees*. This change comes from the need for the algorithm to obtain the probability of the decisions, as was the case with *Gradient Tree Boosting*. The other fundamental change in this algotimo, since it is based on *Gradient Tree Boosting*, is the change from *bagging* to *boosting* strategy for the creation of the classifier training sets.\n",
    "\n",
    "Subsequently, this technique performs an additive training approach whose weights are adjusted based on a **Declining Gradient** on a *loss* function to be defined. By adding the *loss* function with the regularisation term, the second derivative of the functions can be calculated in order to update the classification weights of the different trees. The calculation of this gradient thus allows the adjustment of the values of the classifiers that are generated following a given one in order to allow the weights to focus attention on the patterns that are incorrectly classified. The mathematical details of the implementation can be found in this [link](https://xgboost.readthedocs.io/en/stable/tutorials/model.html).\n",
    "\n",
    "Unlike the other approaches we have seen, the `xgboost` is not currently implemented in `scikit learn` but it is integrated in MLJ. However, the reference version must be installed if it is not already present on the machine which is the one used here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499e0c0f",
   "metadata": {},
   "source": [
    "After that installation, the library could be used as shown in the following example. Unlike other implementations, the Julia implementation supports Julia Array, SparseMatrixCSC, libSVM format text and XGBoost binary file as input.  Althouugh the vastly options given by Julia libraría in deep to change internaly to the format [LIBSVM](https://xgboost.readthedocs.io/en/stable/tutorials/input_format.html) as any other library. This library has not all the posibilities and, more especificl, the BitVector is not supported nowadays in their function `DMatrix`. So, an small change in the format is required in order to use the library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7fd5e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using XGBoost.predict in module Main conflicts with an existing identifier.\n"
     ]
    }
   ],
   "source": [
    "using XGBoost;\n",
    "\n",
    "train_input = input_data\n",
    "train_output = output_data\n",
    "\n",
    "test_input = input_data\n",
    "test_output = output_data\n",
    "\n",
    "train_output_asNumber= Vector{Number}(train_output);\n",
    "\n",
    "@assert train_output_asNumber isa Vector{Number}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad57fb4",
   "metadata": {},
   "source": [
    "Once this data adaptation is done, you can proceed with the training of a model from the `xgboost` library. To do so, it is only necessary to call the function train with the corresponding parameters. Among these parameters, the most important are:\n",
    "\n",
    "- **eta**, term that will determine the compression of the weights after each new stage of *boosting*. It takes values between 0 and 1.\n",
    "- **max_depth**, maximum depth of the trees has by default a value of 6, increasing it will allow more complex models.\n",
    "- **gamma**, parameter that controls the minimum loss reduction necessary to perform a new partition on a leaf node of the tree. The higher it is, the more conservative it will be\n",
    "- **alpha** and **lambda**, are the parameters that control the L1 and L2 regulation respectively.\n",
    "- **objective**, sets the loss function to be used which can be one of the predefined ones, which can be consulted in this [link](https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster)\n",
    "\n",
    "Further it is only necessary to set the maximum number of iterations of the boosting process as shown in the following example with 20 rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eec905d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mXGBoost: starting training.\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39m[20:58:00] WARNING: /workspace/srcdir/xgboost/src/learner.cc:740: \n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mParameters: { \"rounds\" } are not used.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ XGBoost ~/.julia/packages/XGBoost/5SES5/src/XGBoost.jl:34\u001b[39m\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[1]\ttrain-rmse:0.06685667952816027\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[2]\ttrain-rmse:0.03784067051373981\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[3]\ttrain-rmse:0.02057971291063566\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[4]\ttrain-rmse:0.01269548720252575\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[5]\ttrain-rmse:0.00709807877639992\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[6]\ttrain-rmse:0.00428344147805759\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[7]\ttrain-rmse:0.00305694459598080\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[8]\ttrain-rmse:0.00142192344718740\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[9]\ttrain-rmse:0.00098684141249462\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[10]\ttrain-rmse:0.00068744187963404\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining rounds complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Booster()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_data = DMatrix(train_input, label=train_output_asNumber)\n",
    "\n",
    "model = xgboost(svm_data, rounds=20, eta = 1, max_depth = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c275ed94",
   "metadata": {},
   "source": [
    "In the following piece of code, several parameters as passed as a dictionary and two different metrics are calculated. First, error refers to the incorrectly classified ones over the total amount, and the second one is the Area Under the Curve ROC (AUC).\n",
    "\n",
    "### Question 7.5\n",
    "> ❓ Which is the canonical name of the first measure that is been monitored?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04f5411",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "Learning Rate. It describes the rate at which the model adapts and learns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40ab769b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mXGBoost: starting training.\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39m[20:58:04] WARNING: /workspace/srcdir/xgboost/src/learner.cc:740: \n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mParameters: { \"metrics\", \"param\", \"rounds\" } are not used.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ XGBoost ~/.julia/packages/XGBoost/5SES5/src/XGBoost.jl:34\u001b[39m\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[1]\ttrain-rmse:0.36104272805218679\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[2]\ttrain-rmse:0.26349314167054072\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[3]\ttrain-rmse:0.19403141904608778\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[4]\ttrain-rmse:0.14254849879038894\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[5]\ttrain-rmse:0.10912439741860382\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[6]\ttrain-rmse:0.08405248067916692\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[7]\ttrain-rmse:0.06422803732657284\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[8]\ttrain-rmse:0.05041922616002128\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[9]\ttrain-rmse:0.04010784619540812\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m[10]\ttrain-rmse:0.03205229098624832\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining rounds complete.\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching predict(::Booster, ::Matrix{Float64})\nThe function `predict` exists, but no method is defined for this combination of argument types.\n\n\u001b[0mClosest candidates are:\n\u001b[0m  predict(\u001b[91m::MLJModels.DeterministicConstantClassifier\u001b[39m, ::Any, \u001b[91m::Any\u001b[39m)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[35mMLJModels\u001b[39m \u001b[90m~/.julia/packages/MLJModels/ziReN/src/builtins/\u001b[39m\u001b[90m\u001b[4mConstant.jl:90\u001b[24m\u001b[39m\n\u001b[0m  predict(\u001b[91m::VotingClassifier\u001b[39m, ::Any, \u001b[91m::Any\u001b[39m)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[32mMain\u001b[39m \u001b[90m\u001b[4mIn[10]:183\u001b[24m\u001b[39m\n\u001b[0m  predict(\u001b[91m::MLJNaiveBayesInterface.MultinomialNBClassifier\u001b[39m, ::Any, \u001b[91m::Any\u001b[39m)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[33mMLJNaiveBayesInterface\u001b[39m \u001b[90m~/.julia/packages/MLJNaiveBayesInterface/a9Udq/src/\u001b[39m\u001b[90m\u001b[4mMLJNaiveBayesInterface.jl:107\u001b[24m\u001b[39m\n\u001b[0m  ...\n",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching predict(::Booster, ::Matrix{Float64})\nThe function `predict` exists, but no method is defined for this combination of argument types.\n\n\u001b[0mClosest candidates are:\n\u001b[0m  predict(\u001b[91m::MLJModels.DeterministicConstantClassifier\u001b[39m, ::Any, \u001b[91m::Any\u001b[39m)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[35mMLJModels\u001b[39m \u001b[90m~/.julia/packages/MLJModels/ziReN/src/builtins/\u001b[39m\u001b[90m\u001b[4mConstant.jl:90\u001b[24m\u001b[39m\n\u001b[0m  predict(\u001b[91m::VotingClassifier\u001b[39m, ::Any, \u001b[91m::Any\u001b[39m)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[32mMain\u001b[39m \u001b[90m\u001b[4mIn[10]:183\u001b[24m\u001b[39m\n\u001b[0m  predict(\u001b[91m::MLJNaiveBayesInterface.MultinomialNBClassifier\u001b[39m, ::Any, \u001b[91m::Any\u001b[39m)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[33mMLJNaiveBayesInterface\u001b[39m \u001b[90m~/.julia/packages/MLJNaiveBayesInterface/a9Udq/src/\u001b[39m\u001b[90m\u001b[4mMLJNaiveBayesInterface.jl:107\u001b[24m\u001b[39m\n\u001b[0m  ...\n",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "\u001b[90m   @\u001b[39m \u001b[90m\u001b[4mIn[32]:7\u001b[24m\u001b[39m"
     ]
    }
   ],
   "source": [
    "param = [\"max_depth\" => 2,\n",
    "         \"eta\" => 1,\n",
    "         \"objective\" => \"binary:logistic\"]\n",
    "metrics = metrics = [\"error\", \"auc\"]\n",
    "model = xgboost(DMatrix(train_input, label=train_output_asNumber), rounds=20, param=param, metrics=metrics)\n",
    "\n",
    "pred = predict(model, train_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6120deba",
   "metadata": {},
   "source": [
    "***Important***.\n",
    "\n",
    "In case a validation set is used, this must be passed in the *evals* parameter of the training function. In addition, and only when the mentioned *evals* parameter is defined, you can set the rounds for the pre-stop with the *early_stopping_rounds* parameter of the training function. The code would be similar to:\n",
    "``` julia\n",
    "    evals = DMatrix(val_input, label=val_output)\n",
    "    xgb_model = xgb.train(param, train_input, num_round,label = train_output_asNumber, evals=evals,\n",
    "                    early_stopping_rounds=10)\n",
    "```\n",
    "\n",
    "The value provided in the output corresponds to the sum of the outputs of the trees, being between 0 and 1 for membership of a given class. Since this is a binary class, simply set a limit of 0.5 to the output to determine what the answer is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a468354c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error of XGboost= 0.0\n"
     ]
    }
   ],
   "source": [
    "using XGBoost: predict as predict_xgb\n",
    "\n",
    "pred = predict_xgb(model, test_input)\n",
    "print(\"Error of XGboost= \", sum((pred .> 0.5) .!= test_output) / float(size(pred)[1]), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569d26f2",
   "metadata": {},
   "source": [
    "Finally, as in the case of the Random Forest it is possible to identify the importance and paint it for each of the variables in the ranking. With the following code it is possible to see such a marker ordered in a ascendent way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74e86f1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip290\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip290)\" d=\"M0 1600 L2400 1600 L2400 0 L0 0  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip291\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip290)\" d=\"M205.121 1423.18 L2352.76 1423.18 L2352.76 123.472 L205.121 123.472  Z\" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip292\">\n",
       "    <rect x=\"205\" y=\"123\" width=\"2149\" height=\"1301\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"265.903,1423.18 265.903,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"603.582,1423.18 603.582,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"941.26,1423.18 941.26,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1278.94,1423.18 1278.94,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1616.62,1423.18 1616.62,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"1954.3,1423.18 1954.3,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"2291.97,1423.18 2291.97,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,1403.47 2352.76,1403.47 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,1190.14 2352.76,1190.14 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,976.803 2352.76,976.803 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,763.47 2352.76,763.47 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,550.137 2352.76,550.137 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,336.805 2352.76,336.805 \"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"205.121,123.472 2352.76,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1423.18 2352.76,1423.18 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"265.903,1423.18 265.903,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"603.582,1423.18 603.582,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"941.26,1423.18 941.26,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1278.94,1423.18 1278.94,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1616.62,1423.18 1616.62,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1954.3,1423.18 1954.3,1404.28 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2291.97,1423.18 2291.97,1404.28 \"/>\n",
       "<path clip-path=\"url(#clip290)\" d=\"M265.903 1454.1 Q262.292 1454.1 260.463 1457.66 Q258.658 1461.2 258.658 1468.33 Q258.658 1475.44 260.463 1479.01 Q262.292 1482.55 265.903 1482.55 Q269.537 1482.55 271.343 1479.01 Q273.172 1475.44 273.172 1468.33 Q273.172 1461.2 271.343 1457.66 Q269.537 1454.1 265.903 1454.1 M265.903 1450.39 Q271.713 1450.39 274.769 1455 Q277.847 1459.58 277.847 1468.33 Q277.847 1477.06 274.769 1481.67 Q271.713 1486.25 265.903 1486.25 Q260.093 1486.25 257.014 1481.67 Q253.959 1477.06 253.959 1468.33 Q253.959 1459.58 257.014 1455 Q260.093 1450.39 265.903 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M578.269 1481.64 L585.908 1481.64 L585.908 1455.28 L577.598 1456.95 L577.598 1452.69 L585.862 1451.02 L590.538 1451.02 L590.538 1481.64 L598.176 1481.64 L598.176 1485.58 L578.269 1485.58 L578.269 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M617.621 1454.1 Q614.01 1454.1 612.181 1457.66 Q610.375 1461.2 610.375 1468.33 Q610.375 1475.44 612.181 1479.01 Q614.01 1482.55 617.621 1482.55 Q621.255 1482.55 623.061 1479.01 Q624.889 1475.44 624.889 1468.33 Q624.889 1461.2 623.061 1457.66 Q621.255 1454.1 617.621 1454.1 M617.621 1450.39 Q623.431 1450.39 626.487 1455 Q629.565 1459.58 629.565 1468.33 Q629.565 1477.06 626.487 1481.67 Q623.431 1486.25 617.621 1486.25 Q611.811 1486.25 608.732 1481.67 Q605.676 1477.06 605.676 1468.33 Q605.676 1459.58 608.732 1455 Q611.811 1450.39 617.621 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M920.033 1481.64 L936.353 1481.64 L936.353 1485.58 L914.408 1485.58 L914.408 1481.64 Q917.07 1478.89 921.654 1474.26 Q926.26 1469.61 927.441 1468.27 Q929.686 1465.74 930.566 1464.01 Q931.468 1462.25 931.468 1460.56 Q931.468 1457.8 929.524 1456.07 Q927.603 1454.33 924.501 1454.33 Q922.302 1454.33 919.848 1455.09 Q917.418 1455.86 914.64 1457.41 L914.64 1452.69 Q917.464 1451.55 919.918 1450.97 Q922.371 1450.39 924.408 1450.39 Q929.779 1450.39 932.973 1453.08 Q936.167 1455.77 936.167 1460.26 Q936.167 1462.39 935.357 1464.31 Q934.57 1466.2 932.464 1468.8 Q931.885 1469.47 928.783 1472.69 Q925.681 1475.88 920.033 1481.64 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M956.167 1454.1 Q952.556 1454.1 950.728 1457.66 Q948.922 1461.2 948.922 1468.33 Q948.922 1475.44 950.728 1479.01 Q952.556 1482.55 956.167 1482.55 Q959.802 1482.55 961.607 1479.01 Q963.436 1475.44 963.436 1468.33 Q963.436 1461.2 961.607 1457.66 Q959.802 1454.1 956.167 1454.1 M956.167 1450.39 Q961.977 1450.39 965.033 1455 Q968.112 1459.58 968.112 1468.33 Q968.112 1477.06 965.033 1481.67 Q961.977 1486.25 956.167 1486.25 Q950.357 1486.25 947.278 1481.67 Q944.223 1477.06 944.223 1468.33 Q944.223 1459.58 947.278 1455 Q950.357 1450.39 956.167 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1267.78 1466.95 Q1271.14 1467.66 1273.01 1469.93 Q1274.91 1472.2 1274.91 1475.53 Q1274.91 1480.65 1271.39 1483.45 Q1267.87 1486.25 1261.39 1486.25 Q1259.22 1486.25 1256.9 1485.81 Q1254.61 1485.39 1252.16 1484.54 L1252.16 1480.02 Q1254.1 1481.16 1256.42 1481.74 Q1258.73 1482.32 1261.25 1482.32 Q1265.65 1482.32 1267.94 1480.58 Q1270.26 1478.84 1270.26 1475.53 Q1270.26 1472.48 1268.11 1470.77 Q1265.98 1469.03 1262.16 1469.03 L1258.13 1469.03 L1258.13 1465.19 L1262.34 1465.19 Q1265.79 1465.19 1267.62 1463.82 Q1269.45 1462.43 1269.45 1459.84 Q1269.45 1457.18 1267.55 1455.77 Q1265.67 1454.33 1262.16 1454.33 Q1260.23 1454.33 1258.04 1454.75 Q1255.84 1455.16 1253.2 1456.04 L1253.2 1451.88 Q1255.86 1451.14 1258.17 1450.77 Q1260.51 1450.39 1262.57 1450.39 Q1267.9 1450.39 1271 1452.83 Q1274.1 1455.23 1274.1 1459.35 Q1274.1 1462.22 1272.46 1464.21 Q1270.81 1466.18 1267.78 1466.95 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1293.78 1454.1 Q1290.17 1454.1 1288.34 1457.66 Q1286.53 1461.2 1286.53 1468.33 Q1286.53 1475.44 1288.34 1479.01 Q1290.17 1482.55 1293.78 1482.55 Q1297.41 1482.55 1299.22 1479.01 Q1301.04 1475.44 1301.04 1468.33 Q1301.04 1461.2 1299.22 1457.66 Q1297.41 1454.1 1293.78 1454.1 M1293.78 1450.39 Q1299.59 1450.39 1302.64 1455 Q1305.72 1459.58 1305.72 1468.33 Q1305.72 1477.06 1302.64 1481.67 Q1299.59 1486.25 1293.78 1486.25 Q1287.97 1486.25 1284.89 1481.67 Q1281.83 1477.06 1281.83 1468.33 Q1281.83 1459.58 1284.89 1455 Q1287.97 1450.39 1293.78 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1604.79 1455.09 L1592.98 1473.54 L1604.79 1473.54 L1604.79 1455.09 M1603.56 1451.02 L1609.44 1451.02 L1609.44 1473.54 L1614.37 1473.54 L1614.37 1477.43 L1609.44 1477.43 L1609.44 1485.58 L1604.79 1485.58 L1604.79 1477.43 L1589.19 1477.43 L1589.19 1472.92 L1603.56 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1632.1 1454.1 Q1628.49 1454.1 1626.66 1457.66 Q1624.86 1461.2 1624.86 1468.33 Q1624.86 1475.44 1626.66 1479.01 Q1628.49 1482.55 1632.1 1482.55 Q1635.74 1482.55 1637.54 1479.01 Q1639.37 1475.44 1639.37 1468.33 Q1639.37 1461.2 1637.54 1457.66 Q1635.74 1454.1 1632.1 1454.1 M1632.1 1450.39 Q1637.91 1450.39 1640.97 1455 Q1644.05 1459.58 1644.05 1468.33 Q1644.05 1477.06 1640.97 1481.67 Q1637.91 1486.25 1632.1 1486.25 Q1626.29 1486.25 1623.21 1481.67 Q1620.16 1477.06 1620.16 1468.33 Q1620.16 1459.58 1623.21 1455 Q1626.29 1450.39 1632.1 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1928.99 1451.02 L1947.35 1451.02 L1947.35 1454.96 L1933.28 1454.96 L1933.28 1463.43 Q1934.3 1463.08 1935.31 1462.92 Q1936.33 1462.73 1937.35 1462.73 Q1943.14 1462.73 1946.52 1465.9 Q1949.9 1469.08 1949.9 1474.49 Q1949.9 1480.07 1946.43 1483.17 Q1942.95 1486.25 1936.63 1486.25 Q1934.46 1486.25 1932.19 1485.88 Q1929.94 1485.51 1927.54 1484.77 L1927.54 1480.07 Q1929.62 1481.2 1931.84 1481.76 Q1934.06 1482.32 1936.54 1482.32 Q1940.55 1482.32 1942.88 1480.21 Q1945.22 1478.1 1945.22 1474.49 Q1945.22 1470.88 1942.88 1468.77 Q1940.55 1466.67 1936.54 1466.67 Q1934.67 1466.67 1932.79 1467.08 Q1930.94 1467.5 1928.99 1468.38 L1928.99 1451.02 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1969.11 1454.1 Q1965.5 1454.1 1963.67 1457.66 Q1961.86 1461.2 1961.86 1468.33 Q1961.86 1475.44 1963.67 1479.01 Q1965.5 1482.55 1969.11 1482.55 Q1972.74 1482.55 1974.55 1479.01 Q1976.38 1475.44 1976.38 1468.33 Q1976.38 1461.2 1974.55 1457.66 Q1972.74 1454.1 1969.11 1454.1 M1969.11 1450.39 Q1974.92 1450.39 1977.98 1455 Q1981.05 1459.58 1981.05 1468.33 Q1981.05 1477.06 1977.98 1481.67 Q1974.92 1486.25 1969.11 1486.25 Q1963.3 1486.25 1960.22 1481.67 Q1957.17 1477.06 1957.17 1468.33 Q1957.17 1459.58 1960.22 1455 Q1963.3 1450.39 1969.11 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M2277.38 1466.44 Q2274.23 1466.44 2272.38 1468.59 Q2270.55 1470.74 2270.55 1474.49 Q2270.55 1478.22 2272.38 1480.39 Q2274.23 1482.55 2277.38 1482.55 Q2280.53 1482.55 2282.36 1480.39 Q2284.21 1478.22 2284.21 1474.49 Q2284.21 1470.74 2282.36 1468.59 Q2280.53 1466.44 2277.38 1466.44 M2286.66 1451.78 L2286.66 1456.04 Q2284.9 1455.21 2283.1 1454.77 Q2281.31 1454.33 2279.55 1454.33 Q2274.93 1454.33 2272.47 1457.45 Q2270.04 1460.58 2269.69 1466.9 Q2271.06 1464.89 2273.12 1463.82 Q2275.18 1462.73 2277.66 1462.73 Q2282.87 1462.73 2285.87 1465.9 Q2288.91 1469.05 2288.91 1474.49 Q2288.91 1479.82 2285.76 1483.03 Q2282.61 1486.25 2277.38 1486.25 Q2271.38 1486.25 2268.21 1481.67 Q2265.04 1477.06 2265.04 1468.33 Q2265.04 1460.14 2268.93 1455.28 Q2272.82 1450.39 2279.37 1450.39 Q2281.13 1450.39 2282.91 1450.74 Q2284.72 1451.09 2286.66 1451.78 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M2306.96 1454.1 Q2303.35 1454.1 2301.52 1457.66 Q2299.72 1461.2 2299.72 1468.33 Q2299.72 1475.44 2301.52 1479.01 Q2303.35 1482.55 2306.96 1482.55 Q2310.6 1482.55 2312.4 1479.01 Q2314.23 1475.44 2314.23 1468.33 Q2314.23 1461.2 2312.4 1457.66 Q2310.6 1454.1 2306.96 1454.1 M2306.96 1450.39 Q2312.77 1450.39 2315.83 1455 Q2318.91 1459.58 2318.91 1468.33 Q2318.91 1477.06 2315.83 1481.67 Q2312.77 1486.25 2306.96 1486.25 Q2301.15 1486.25 2298.07 1481.67 Q2295.02 1477.06 2295.02 1468.33 Q2295.02 1459.58 2298.07 1455 Q2301.15 1450.39 2306.96 1450.39 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1243.74 1561.26 L1243.74 1548.5 L1233.23 1548.5 L1233.23 1543.22 L1250.1 1543.22 L1250.1 1563.62 Q1246.38 1566.26 1241.89 1567.63 Q1237.4 1568.97 1232.31 1568.97 Q1221.17 1568.97 1214.87 1562.47 Q1208.6 1555.95 1208.6 1544.33 Q1208.6 1532.68 1214.87 1526.19 Q1221.17 1519.66 1232.31 1519.66 Q1236.96 1519.66 1241.13 1520.81 Q1245.33 1521.96 1248.86 1524.18 L1248.86 1531.03 Q1245.3 1528 1241.29 1526.48 Q1237.27 1524.95 1232.85 1524.95 Q1224.13 1524.95 1219.74 1529.82 Q1215.38 1534.69 1215.38 1544.33 Q1215.38 1553.94 1219.74 1558.81 Q1224.13 1563.68 1232.85 1563.68 Q1236.26 1563.68 1238.93 1563.11 Q1241.6 1562.51 1243.74 1561.26 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1277.79 1550.12 Q1270.69 1550.12 1267.96 1551.75 Q1265.22 1553.37 1265.22 1557.29 Q1265.22 1560.4 1267.26 1562.25 Q1269.33 1564.07 1272.86 1564.07 Q1277.73 1564.07 1280.66 1560.63 Q1283.62 1557.16 1283.62 1551.43 L1283.62 1550.12 L1277.79 1550.12 M1289.47 1547.71 L1289.47 1568.04 L1283.62 1568.04 L1283.62 1562.63 Q1281.61 1565.88 1278.62 1567.44 Q1275.63 1568.97 1271.3 1568.97 Q1265.83 1568.97 1262.58 1565.91 Q1259.36 1562.82 1259.36 1557.67 Q1259.36 1551.65 1263.37 1548.6 Q1267.42 1545.54 1275.41 1545.54 L1283.62 1545.54 L1283.62 1544.97 Q1283.62 1540.93 1280.94 1538.73 Q1278.3 1536.5 1273.5 1536.5 Q1270.44 1536.5 1267.54 1537.23 Q1264.65 1537.97 1261.97 1539.43 L1261.97 1534.02 Q1265.19 1532.78 1268.21 1532.17 Q1271.24 1531.54 1274.1 1531.54 Q1281.83 1531.54 1285.65 1535.55 Q1289.47 1539.56 1289.47 1547.71 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1301.54 1532.4 L1307.39 1532.4 L1307.39 1568.04 L1301.54 1568.04 L1301.54 1532.4 M1301.54 1518.52 L1307.39 1518.52 L1307.39 1525.93 L1301.54 1525.93 L1301.54 1518.52 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1349.28 1546.53 L1349.28 1568.04 L1343.42 1568.04 L1343.42 1546.72 Q1343.42 1541.66 1341.45 1539.14 Q1339.48 1536.63 1335.53 1536.63 Q1330.79 1536.63 1328.05 1539.65 Q1325.31 1542.68 1325.31 1547.9 L1325.31 1568.04 L1319.42 1568.04 L1319.42 1532.4 L1325.31 1532.4 L1325.31 1537.93 Q1327.41 1534.72 1330.25 1533.13 Q1333.11 1531.54 1336.83 1531.54 Q1342.98 1531.54 1346.13 1535.36 Q1349.28 1539.14 1349.28 1546.53 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1423.18 205.121,123.472 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1403.47 224.019,1403.47 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,1190.14 224.019,1190.14 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,976.803 224.019,976.803 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,763.47 224.019,763.47 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,550.137 224.019,550.137 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,336.805 224.019,336.805 \"/>\n",
       "<polyline clip-path=\"url(#clip290)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"205.121,123.472 224.019,123.472 \"/>\n",
       "<path clip-path=\"url(#clip290)\" d=\"M157.177 1389.27 Q153.566 1389.27 151.737 1392.83 Q149.931 1396.37 149.931 1403.5 Q149.931 1410.61 151.737 1414.17 Q153.566 1417.72 157.177 1417.72 Q160.811 1417.72 162.616 1414.17 Q164.445 1410.61 164.445 1403.5 Q164.445 1396.37 162.616 1392.83 Q160.811 1389.27 157.177 1389.27 M157.177 1385.56 Q162.987 1385.56 166.042 1390.17 Q169.121 1394.75 169.121 1403.5 Q169.121 1412.23 166.042 1416.84 Q162.987 1421.42 157.177 1421.42 Q151.366 1421.42 148.288 1416.84 Q145.232 1412.23 145.232 1403.5 Q145.232 1394.75 148.288 1390.17 Q151.366 1385.56 157.177 1385.56 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M117.825 1203.48 L125.464 1203.48 L125.464 1177.11 L117.154 1178.78 L117.154 1174.52 L125.418 1172.86 L130.093 1172.86 L130.093 1203.48 L137.732 1203.48 L137.732 1207.42 L117.825 1207.42 L117.825 1203.48 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M157.177 1175.93 Q153.566 1175.93 151.737 1179.5 Q149.931 1183.04 149.931 1190.17 Q149.931 1197.28 151.737 1200.84 Q153.566 1204.38 157.177 1204.38 Q160.811 1204.38 162.616 1200.84 Q164.445 1197.28 164.445 1190.17 Q164.445 1183.04 162.616 1179.5 Q160.811 1175.93 157.177 1175.93 M157.177 1172.23 Q162.987 1172.23 166.042 1176.84 Q169.121 1181.42 169.121 1190.17 Q169.121 1198.9 166.042 1203.5 Q162.987 1208.09 157.177 1208.09 Q151.366 1208.09 148.288 1203.5 Q145.232 1198.9 145.232 1190.17 Q145.232 1181.42 148.288 1176.84 Q151.366 1172.23 157.177 1172.23 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M121.043 990.147 L137.362 990.147 L137.362 994.083 L115.418 994.083 L115.418 990.147 Q118.08 987.393 122.663 982.763 Q127.269 978.111 128.45 976.768 Q130.695 974.245 131.575 972.509 Q132.478 970.749 132.478 969.06 Q132.478 966.305 130.533 964.569 Q128.612 962.833 125.51 962.833 Q123.311 962.833 120.857 963.597 Q118.427 964.361 115.649 965.912 L115.649 961.189 Q118.473 960.055 120.927 959.476 Q123.38 958.898 125.418 958.898 Q130.788 958.898 133.982 961.583 Q137.177 964.268 137.177 968.759 Q137.177 970.888 136.367 972.81 Q135.579 974.708 133.473 977.3 Q132.894 977.972 129.792 981.189 Q126.691 984.384 121.043 990.147 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M157.177 962.601 Q153.566 962.601 151.737 966.166 Q149.931 969.708 149.931 976.837 Q149.931 983.944 151.737 987.509 Q153.566 991.05 157.177 991.05 Q160.811 991.05 162.616 987.509 Q164.445 983.944 164.445 976.837 Q164.445 969.708 162.616 966.166 Q160.811 962.601 157.177 962.601 M157.177 958.898 Q162.987 958.898 166.042 963.504 Q169.121 968.087 169.121 976.837 Q169.121 985.564 166.042 990.171 Q162.987 994.754 157.177 994.754 Q151.366 994.754 148.288 990.171 Q145.232 985.564 145.232 976.837 Q145.232 968.087 148.288 963.504 Q151.366 958.898 157.177 958.898 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M131.181 762.116 Q134.538 762.833 136.413 765.102 Q138.311 767.37 138.311 770.704 Q138.311 775.819 134.792 778.62 Q131.274 781.421 124.793 781.421 Q122.617 781.421 120.302 780.981 Q118.01 780.565 115.556 779.708 L115.556 775.194 Q117.501 776.329 119.816 776.907 Q122.13 777.486 124.654 777.486 Q129.052 777.486 131.343 775.75 Q133.658 774.014 133.658 770.704 Q133.658 767.648 131.505 765.935 Q129.376 764.199 125.556 764.199 L121.529 764.199 L121.529 760.357 L125.742 760.357 Q129.191 760.357 131.019 758.991 Q132.848 757.602 132.848 755.009 Q132.848 752.347 130.95 750.935 Q129.075 749.5 125.556 749.5 Q123.635 749.5 121.436 749.917 Q119.237 750.334 116.598 751.213 L116.598 747.046 Q119.26 746.306 121.575 745.935 Q123.913 745.565 125.973 745.565 Q131.297 745.565 134.399 747.996 Q137.501 750.403 137.501 754.523 Q137.501 757.394 135.857 759.384 Q134.214 761.352 131.181 762.116 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M157.177 749.269 Q153.566 749.269 151.737 752.833 Q149.931 756.375 149.931 763.505 Q149.931 770.611 151.737 774.176 Q153.566 777.718 157.177 777.718 Q160.811 777.718 162.616 774.176 Q164.445 770.611 164.445 763.505 Q164.445 756.375 162.616 752.833 Q160.811 749.269 157.177 749.269 M157.177 745.565 Q162.987 745.565 166.042 750.171 Q169.121 754.755 169.121 763.505 Q169.121 772.232 166.042 776.838 Q162.987 781.421 157.177 781.421 Q151.366 781.421 148.288 776.838 Q145.232 772.232 145.232 763.505 Q145.232 754.755 148.288 750.171 Q151.366 745.565 157.177 745.565 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M129.862 536.931 L118.056 555.38 L129.862 555.38 L129.862 536.931 M128.635 532.857 L134.515 532.857 L134.515 555.38 L139.445 555.38 L139.445 559.269 L134.515 559.269 L134.515 567.417 L129.862 567.417 L129.862 559.269 L114.26 559.269 L114.26 554.755 L128.635 532.857 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M157.177 535.936 Q153.566 535.936 151.737 539.501 Q149.931 543.043 149.931 550.172 Q149.931 557.279 151.737 560.843 Q153.566 564.385 157.177 564.385 Q160.811 564.385 162.616 560.843 Q164.445 557.279 164.445 550.172 Q164.445 543.043 162.616 539.501 Q160.811 535.936 157.177 535.936 M157.177 532.232 Q162.987 532.232 166.042 536.839 Q169.121 541.422 169.121 550.172 Q169.121 558.899 166.042 563.505 Q162.987 568.089 157.177 568.089 Q151.366 568.089 148.288 563.505 Q145.232 558.899 145.232 550.172 Q145.232 541.422 148.288 536.839 Q151.366 532.232 157.177 532.232 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M117.061 319.525 L135.417 319.525 L135.417 323.46 L121.343 323.46 L121.343 331.932 Q122.362 331.585 123.38 331.423 Q124.399 331.238 125.418 331.238 Q131.205 331.238 134.584 334.409 Q137.964 337.58 137.964 342.997 Q137.964 348.576 134.492 351.677 Q131.019 354.756 124.7 354.756 Q122.524 354.756 120.255 354.386 Q118.01 354.015 115.603 353.275 L115.603 348.576 Q117.686 349.71 119.908 350.265 Q122.13 350.821 124.607 350.821 Q128.612 350.821 130.95 348.714 Q133.288 346.608 133.288 342.997 Q133.288 339.386 130.95 337.279 Q128.612 335.173 124.607 335.173 Q122.732 335.173 120.857 335.589 Q119.006 336.006 117.061 336.886 L117.061 319.525 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M157.177 322.603 Q153.566 322.603 151.737 326.168 Q149.931 329.71 149.931 336.839 Q149.931 343.946 151.737 347.511 Q153.566 351.052 157.177 351.052 Q160.811 351.052 162.616 347.511 Q164.445 343.946 164.445 336.839 Q164.445 329.71 162.616 326.168 Q160.811 322.603 157.177 322.603 M157.177 318.9 Q162.987 318.9 166.042 323.506 Q169.121 328.09 169.121 336.839 Q169.121 345.566 166.042 350.173 Q162.987 354.756 157.177 354.756 Q151.366 354.756 148.288 350.173 Q145.232 345.566 145.232 336.839 Q145.232 328.09 148.288 323.506 Q151.366 318.9 157.177 318.9 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M127.593 121.609 Q124.445 121.609 122.593 123.761 Q120.765 125.914 120.765 129.664 Q120.765 133.391 122.593 135.567 Q124.445 137.72 127.593 137.72 Q130.742 137.72 132.57 135.567 Q134.422 133.391 134.422 129.664 Q134.422 125.914 132.57 123.761 Q130.742 121.609 127.593 121.609 M136.876 106.956 L136.876 111.215 Q135.117 110.382 133.311 109.942 Q131.529 109.502 129.769 109.502 Q125.14 109.502 122.686 112.627 Q120.255 115.752 119.908 122.072 Q121.274 120.058 123.334 118.993 Q125.394 117.905 127.871 117.905 Q133.08 117.905 136.089 121.076 Q139.121 124.224 139.121 129.664 Q139.121 134.988 135.973 138.206 Q132.825 141.423 127.593 141.423 Q121.598 141.423 118.427 136.84 Q115.256 132.234 115.256 123.507 Q115.256 115.312 119.144 110.451 Q123.033 105.567 129.584 105.567 Q131.343 105.567 133.126 105.914 Q134.931 106.262 136.876 106.956 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M157.177 109.271 Q153.566 109.271 151.737 112.836 Q149.931 116.377 149.931 123.507 Q149.931 130.613 151.737 134.178 Q153.566 137.72 157.177 137.72 Q160.811 137.72 162.616 134.178 Q164.445 130.613 164.445 123.507 Q164.445 116.377 162.616 112.836 Q160.811 109.271 157.177 109.271 M157.177 105.567 Q162.987 105.567 166.042 110.174 Q169.121 114.757 169.121 123.507 Q169.121 132.234 166.042 136.84 Q162.987 141.423 157.177 141.423 Q151.366 141.423 148.288 136.84 Q145.232 132.234 145.232 123.507 Q145.232 114.757 148.288 110.174 Q151.366 105.567 157.177 105.567 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M16.4842 891.553 L16.4842 864.244 L21.895 864.244 L21.895 885.124 L35.8996 885.124 L35.8996 866.281 L41.3104 866.281 L41.3104 885.124 L64.0042 885.124 L64.0042 891.553 L16.4842 891.553 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M44.7161 827.387 L47.5806 827.387 L47.5806 854.314 Q53.6281 853.932 56.8109 850.685 Q59.9619 847.407 59.9619 841.582 Q59.9619 838.208 59.1344 835.057 Q58.3069 831.875 56.6518 828.755 L62.1899 828.755 Q63.5267 831.906 64.227 835.217 Q64.9272 838.527 64.9272 841.932 Q64.9272 850.462 59.9619 855.46 Q54.9967 860.425 46.5303 860.425 Q37.7774 860.425 32.6531 855.714 Q27.4968 850.972 27.4968 842.951 Q27.4968 835.758 32.1438 831.588 Q36.7589 827.387 44.7161 827.387 M42.9973 833.243 Q38.1912 833.307 35.3266 835.949 Q32.4621 838.559 32.4621 842.887 Q32.4621 847.789 35.2312 850.749 Q38.0002 853.677 43.0292 854.123 L42.9973 833.243 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M46.0847 801.574 Q46.0847 808.672 47.7079 811.409 Q49.3312 814.146 53.2461 814.146 Q56.3653 814.146 58.2114 812.109 Q60.0256 810.04 60.0256 806.507 Q60.0256 801.637 56.5881 798.709 Q53.1188 795.749 47.3897 795.749 L46.0847 795.749 L46.0847 801.574 M43.6657 789.893 L64.0042 789.893 L64.0042 795.749 L58.5933 795.749 Q61.8398 797.754 63.3994 800.746 Q64.9272 803.738 64.9272 808.067 Q64.9272 813.541 61.8716 816.788 Q58.7843 820.003 53.6281 820.003 Q47.6125 820.003 44.5569 815.992 Q41.5014 811.95 41.5014 803.961 L41.5014 795.749 L40.9285 795.749 Q36.8862 795.749 34.6901 798.423 Q32.4621 801.065 32.4621 805.871 Q32.4621 808.926 33.1941 811.823 Q33.9262 814.719 35.3903 817.393 L29.9795 817.393 Q28.7381 814.178 28.1334 811.154 Q27.4968 808.13 27.4968 805.266 Q27.4968 797.532 31.5072 793.712 Q35.5176 789.893 43.6657 789.893 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M18.2347 772.037 L28.3562 772.037 L28.3562 759.974 L32.9077 759.974 L32.9077 772.037 L52.2594 772.037 Q56.6199 772.037 57.8613 770.859 Q59.1026 769.65 59.1026 765.99 L59.1026 759.974 L64.0042 759.974 L64.0042 765.99 Q64.0042 772.769 61.4897 775.347 Q58.9434 777.925 52.2594 777.925 L32.9077 777.925 L32.9077 782.222 L28.3562 782.222 L28.3562 777.925 L18.2347 777.925 L18.2347 772.037 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M49.9359 752.876 L28.3562 752.876 L28.3562 747.02 L49.7131 747.02 Q54.7739 747.02 57.3202 745.046 Q59.8346 743.073 59.8346 739.126 Q59.8346 734.384 56.8109 731.647 Q53.7872 728.877 48.5673 728.877 L28.3562 728.877 L28.3562 723.021 L64.0042 723.021 L64.0042 728.877 L58.5296 728.877 Q61.7762 731.01 63.3676 733.843 Q64.9272 736.644 64.9272 740.368 Q64.9272 746.51 61.1078 749.693 Q57.2883 752.876 49.9359 752.876 M27.4968 738.14 L27.4968 738.14 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M33.8307 690.301 Q33.2578 691.288 33.0032 692.466 Q32.7167 693.611 32.7167 695.012 Q32.7167 699.977 35.9632 702.651 Q39.1779 705.292 45.2253 705.292 L64.0042 705.292 L64.0042 711.181 L28.3562 711.181 L28.3562 705.292 L33.8944 705.292 Q30.6479 703.446 29.0883 700.486 Q27.4968 697.526 27.4968 693.293 Q27.4968 692.688 27.5923 691.956 Q27.656 691.224 27.8151 690.333 L33.8307 690.301 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M44.7161 655.099 L47.5806 655.099 L47.5806 682.026 Q53.6281 681.644 56.8109 678.397 Q59.9619 675.119 59.9619 669.294 Q59.9619 665.921 59.1344 662.77 Q58.3069 659.587 56.6518 656.468 L62.1899 656.468 Q63.5267 659.619 64.227 662.929 Q64.9272 666.239 64.9272 669.645 Q64.9272 678.175 59.9619 683.172 Q54.9967 688.137 46.5303 688.137 Q37.7774 688.137 32.6531 683.426 Q27.4968 678.684 27.4968 670.663 Q27.4968 663.47 32.1438 659.3 Q36.7589 655.099 44.7161 655.099 M42.9973 660.955 Q38.1912 661.019 35.3266 663.661 Q32.4621 666.271 32.4621 670.599 Q32.4621 675.501 35.2312 678.461 Q38.0002 681.389 43.0292 681.835 L42.9973 660.955 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M877.575 12.096 L912.332 12.096 L912.332 18.9825 L885.758 18.9825 L885.758 36.8065 L909.739 36.8065 L909.739 43.6931 L885.758 43.6931 L885.758 72.576 L877.575 72.576 L877.575 12.096 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M959.241 48.0275 L959.241 51.6733 L924.97 51.6733 Q925.457 59.3701 929.588 63.421 Q933.761 67.4314 941.174 67.4314 Q945.468 67.4314 949.478 66.3781 Q953.529 65.3249 957.499 63.2184 L957.499 70.267 Q953.489 71.9684 949.276 72.8596 Q945.063 73.7508 940.728 73.7508 Q929.872 73.7508 923.512 67.4314 Q917.193 61.1119 917.193 50.3365 Q917.193 39.1965 923.188 32.6746 Q929.224 26.1121 939.432 26.1121 Q948.587 26.1121 953.894 32.0264 Q959.241 37.9003 959.241 48.0275 M951.787 45.84 Q951.706 39.7232 948.344 36.0774 Q945.022 32.4315 939.513 32.4315 Q933.275 32.4315 929.507 35.9558 Q925.781 39.4801 925.213 45.8805 L951.787 45.84 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M992.094 49.7694 Q983.06 49.7694 979.577 51.8354 Q976.093 53.9013 976.093 58.8839 Q976.093 62.8538 978.685 65.2034 Q981.319 67.5124 985.815 67.5124 Q992.013 67.5124 995.74 63.1374 Q999.507 58.7219 999.507 51.4303 L999.507 49.7694 L992.094 49.7694 M1006.96 46.6907 L1006.96 72.576 L999.507 72.576 L999.507 65.6895 Q996.955 69.8214 993.147 71.8063 Q989.339 73.7508 983.83 73.7508 Q976.863 73.7508 972.731 69.8619 Q968.639 65.9325 968.639 59.3701 Q968.639 51.7138 973.743 47.825 Q978.888 43.9361 989.056 43.9361 L999.507 43.9361 L999.507 43.2069 Q999.507 38.0623 996.104 35.2672 Q992.742 32.4315 986.625 32.4315 Q982.736 32.4315 979.05 33.3632 Q975.364 34.295 971.961 36.1584 L971.961 29.2718 Q976.052 27.692 979.901 26.9223 Q983.749 26.1121 987.395 26.1121 Q997.239 26.1121 1002.1 31.2163 Q1006.96 36.3204 1006.96 46.6907 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1029.69 14.324 L1029.69 27.2059 L1045.04 27.2059 L1045.04 32.9987 L1029.69 32.9987 L1029.69 57.6282 Q1029.69 63.1779 1031.19 64.7578 Q1032.72 66.3376 1037.38 66.3376 L1045.04 66.3376 L1045.04 72.576 L1037.38 72.576 Q1028.75 72.576 1025.47 69.3758 Q1022.19 66.1351 1022.19 57.6282 L1022.19 32.9987 L1016.72 32.9987 L1016.72 27.2059 L1022.19 27.2059 L1022.19 14.324 L1029.69 14.324 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1054.07 54.671 L1054.07 27.2059 L1061.53 27.2059 L1061.53 54.3874 Q1061.53 60.8284 1064.04 64.0691 Q1066.55 67.2693 1071.57 67.2693 Q1077.61 67.2693 1081.09 63.421 Q1084.62 59.5726 1084.62 52.9291 L1084.62 27.2059 L1092.07 27.2059 L1092.07 72.576 L1084.62 72.576 L1084.62 65.6084 Q1081.9 69.7404 1078.3 71.7658 Q1074.73 73.7508 1069.99 73.7508 Q1062.17 73.7508 1058.12 68.8897 Q1054.07 64.0286 1054.07 54.671 M1072.83 26.1121 L1072.83 26.1121 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1133.71 34.1734 Q1132.46 33.4443 1130.96 33.1202 Q1129.5 32.7556 1127.72 32.7556 Q1121.4 32.7556 1118 36.8875 Q1114.63 40.9789 1114.63 48.6757 L1114.63 72.576 L1107.14 72.576 L1107.14 27.2059 L1114.63 27.2059 L1114.63 34.2544 Q1116.98 30.1225 1120.75 28.1376 Q1124.52 26.1121 1129.91 26.1121 Q1130.68 26.1121 1131.61 26.2337 Q1132.54 26.3147 1133.67 26.5172 L1133.71 34.1734 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1178.52 48.0275 L1178.52 51.6733 L1144.25 51.6733 Q1144.73 59.3701 1148.86 63.421 Q1153.04 67.4314 1160.45 67.4314 Q1164.74 67.4314 1168.75 66.3781 Q1172.8 65.3249 1176.77 63.2184 L1176.77 70.267 Q1172.76 71.9684 1168.55 72.8596 Q1164.34 73.7508 1160 73.7508 Q1149.15 73.7508 1142.79 67.4314 Q1136.47 61.1119 1136.47 50.3365 Q1136.47 39.1965 1142.46 32.6746 Q1148.5 26.1121 1158.71 26.1121 Q1167.86 26.1121 1173.17 32.0264 Q1178.52 37.9003 1178.52 48.0275 M1171.06 45.84 Q1170.98 39.7232 1167.62 36.0774 Q1164.3 32.4315 1158.79 32.4315 Q1152.55 32.4315 1148.78 35.9558 Q1145.06 39.4801 1144.49 45.8805 L1171.06 45.84 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1217.45 12.096 L1225.63 12.096 L1225.63 72.576 L1217.45 72.576 L1217.45 12.096 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1276.91 35.9153 Q1279.71 30.8922 1283.6 28.5022 Q1287.49 26.1121 1292.75 26.1121 Q1299.84 26.1121 1303.69 31.0947 Q1307.54 36.0368 1307.54 45.1919 L1307.54 72.576 L1300.04 72.576 L1300.04 45.4349 Q1300.04 38.913 1297.73 35.7533 Q1295.43 32.5936 1290.69 32.5936 Q1284.89 32.5936 1281.53 36.4419 Q1278.17 40.2903 1278.17 46.9338 L1278.17 72.576 L1270.67 72.576 L1270.67 45.4349 Q1270.67 38.8725 1268.37 35.7533 Q1266.06 32.5936 1261.24 32.5936 Q1255.52 32.5936 1252.16 36.4824 Q1248.8 40.3308 1248.8 46.9338 L1248.8 72.576 L1241.31 72.576 L1241.31 27.2059 L1248.8 27.2059 L1248.8 34.2544 Q1251.35 30.082 1254.92 28.0971 Q1258.48 26.1121 1263.38 26.1121 Q1268.33 26.1121 1271.77 28.6237 Q1275.25 31.1352 1276.91 35.9153 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1329.62 65.7705 L1329.62 89.8329 L1322.12 89.8329 L1322.12 27.2059 L1329.62 27.2059 L1329.62 34.0924 Q1331.96 30.0415 1335.53 28.0971 Q1339.13 26.1121 1344.12 26.1121 Q1352.38 26.1121 1357.53 32.6746 Q1362.71 39.2371 1362.71 49.9314 Q1362.71 60.6258 1357.53 67.1883 Q1352.38 73.7508 1344.12 73.7508 Q1339.13 73.7508 1335.53 71.8063 Q1331.96 69.8214 1329.62 65.7705 M1354.97 49.9314 Q1354.97 41.7081 1351.57 37.0496 Q1348.21 32.3505 1342.29 32.3505 Q1336.38 32.3505 1332.98 37.0496 Q1329.62 41.7081 1329.62 49.9314 Q1329.62 58.1548 1332.98 62.8538 Q1336.38 67.5124 1342.29 67.5124 Q1348.21 67.5124 1351.57 62.8538 Q1354.97 58.1548 1354.97 49.9314 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1392.65 32.4315 Q1386.65 32.4315 1383.17 37.1306 Q1379.68 41.7891 1379.68 49.9314 Q1379.68 58.0738 1383.13 62.7728 Q1386.61 67.4314 1392.65 67.4314 Q1398.6 67.4314 1402.09 62.7323 Q1405.57 58.0333 1405.57 49.9314 Q1405.57 41.8701 1402.09 37.1711 Q1398.6 32.4315 1392.65 32.4315 M1392.65 26.1121 Q1402.37 26.1121 1407.92 32.4315 Q1413.47 38.7509 1413.47 49.9314 Q1413.47 61.0714 1407.92 67.4314 Q1402.37 73.7508 1392.65 73.7508 Q1382.88 73.7508 1377.33 67.4314 Q1371.83 61.0714 1371.83 49.9314 Q1371.83 38.7509 1377.33 32.4315 Q1382.88 26.1121 1392.65 26.1121 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1452.11 34.1734 Q1450.86 33.4443 1449.36 33.1202 Q1447.9 32.7556 1446.12 32.7556 Q1439.8 32.7556 1436.4 36.8875 Q1433.03 40.9789 1433.03 48.6757 L1433.03 72.576 L1425.54 72.576 L1425.54 27.2059 L1433.03 27.2059 L1433.03 34.2544 Q1435.38 30.1225 1439.15 28.1376 Q1442.92 26.1121 1448.31 26.1121 Q1449.08 26.1121 1450.01 26.2337 Q1450.94 26.3147 1452.07 26.5172 L1452.11 34.1734 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1467.31 14.324 L1467.31 27.2059 L1482.66 27.2059 L1482.66 32.9987 L1467.31 32.9987 L1467.31 57.6282 Q1467.31 63.1779 1468.8 64.7578 Q1470.34 66.3376 1475 66.3376 L1482.66 66.3376 L1482.66 72.576 L1475 72.576 Q1466.37 72.576 1463.09 69.3758 Q1459.81 66.1351 1459.81 57.6282 L1459.81 32.9987 L1454.34 32.9987 L1454.34 27.2059 L1459.81 27.2059 L1459.81 14.324 L1467.31 14.324 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1513.08 49.7694 Q1504.05 49.7694 1500.56 51.8354 Q1497.08 53.9013 1497.08 58.8839 Q1497.08 62.8538 1499.67 65.2034 Q1502.31 67.5124 1506.8 67.5124 Q1513 67.5124 1516.73 63.1374 Q1520.49 58.7219 1520.49 51.4303 L1520.49 49.7694 L1513.08 49.7694 M1527.95 46.6907 L1527.95 72.576 L1520.49 72.576 L1520.49 65.6895 Q1517.94 69.8214 1514.13 71.8063 Q1510.33 73.7508 1504.82 73.7508 Q1497.85 73.7508 1493.72 69.8619 Q1489.63 65.9325 1489.63 59.3701 Q1489.63 51.7138 1494.73 47.825 Q1499.87 43.9361 1510.04 43.9361 L1520.49 43.9361 L1520.49 43.2069 Q1520.49 38.0623 1517.09 35.2672 Q1513.73 32.4315 1507.61 32.4315 Q1503.72 32.4315 1500.04 33.3632 Q1496.35 34.295 1492.95 36.1584 L1492.95 29.2718 Q1497.04 27.692 1500.89 26.9223 Q1504.74 26.1121 1508.38 26.1121 Q1518.23 26.1121 1523.09 31.2163 Q1527.95 36.3204 1527.95 46.6907 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1581.01 45.1919 L1581.01 72.576 L1573.56 72.576 L1573.56 45.4349 Q1573.56 38.994 1571.05 35.7938 Q1568.54 32.5936 1563.51 32.5936 Q1557.48 32.5936 1553.99 36.4419 Q1550.51 40.2903 1550.51 46.9338 L1550.51 72.576 L1543.02 72.576 L1543.02 27.2059 L1550.51 27.2059 L1550.51 34.2544 Q1553.18 30.163 1556.79 28.1376 Q1560.44 26.1121 1565.18 26.1121 Q1572.99 26.1121 1577 30.9732 Q1581.01 35.7938 1581.01 45.1919 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1628.53 28.9478 L1628.53 35.9153 Q1625.37 34.1734 1622.17 33.3227 Q1619.01 32.4315 1615.77 32.4315 Q1608.52 32.4315 1604.51 37.0496 Q1600.5 41.6271 1600.5 49.9314 Q1600.5 58.2358 1604.51 62.8538 Q1608.52 67.4314 1615.77 67.4314 Q1619.01 67.4314 1622.17 66.5807 Q1625.37 65.6895 1628.53 63.9476 L1628.53 70.8341 Q1625.41 72.2924 1622.05 73.0216 Q1618.73 73.7508 1614.96 73.7508 Q1604.71 73.7508 1598.68 67.3098 Q1592.64 60.8689 1592.64 49.9314 Q1592.64 38.832 1598.72 32.472 Q1604.83 26.1121 1615.45 26.1121 Q1618.89 26.1121 1622.17 26.8413 Q1625.45 27.5299 1628.53 28.9478 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip290)\" d=\"M1680.3 48.0275 L1680.3 51.6733 L1646.03 51.6733 Q1646.52 59.3701 1650.65 63.421 Q1654.82 67.4314 1662.23 67.4314 Q1666.53 67.4314 1670.54 66.3781 Q1674.59 65.3249 1678.56 63.2184 L1678.56 70.267 Q1674.55 71.9684 1670.34 72.8596 Q1666.12 73.7508 1661.79 73.7508 Q1650.93 73.7508 1644.57 67.4314 Q1638.25 61.1119 1638.25 50.3365 Q1638.25 39.1965 1644.25 32.6746 Q1650.28 26.1121 1660.49 26.1121 Q1669.65 26.1121 1674.95 32.0264 Q1680.3 37.9003 1680.3 48.0275 M1672.85 45.84 Q1672.77 39.7232 1669.41 36.0774 Q1666.08 32.4315 1660.57 32.4315 Q1654.34 32.4315 1650.57 35.9558 Q1646.84 39.4801 1646.27 45.8805 L1672.85 45.84 Z\" fill=\"#000000\" fill-rule=\"nonzero\" fill-opacity=\"1\" /><path clip-path=\"url(#clip292)\" d=\"M637.349 1390.67 L265.903 1390.67 L265.903 1373.6 L637.349 1373.6 L637.349 1390.67 L637.349 1390.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"637.349,1390.67 265.903,1390.67 265.903,1373.6 637.349,1373.6 637.349,1390.67 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M806.189 1369.33 L265.903 1369.33 L265.903 1352.27 L806.189 1352.27 L806.189 1369.33 L806.189 1369.33  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"806.189,1369.33 265.903,1369.33 265.903,1352.27 806.189,1352.27 806.189,1369.33 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M1414.01 1348 L265.903 1348 L265.903 1330.93 L1414.01 1330.93 L1414.01 1348 L1414.01 1348  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1414.01,1348 265.903,1348 265.903,1330.93 1414.01,1330.93 1414.01,1348 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M1312.71 1326.67 L265.903 1326.67 L265.903 1309.6 L1312.71 1309.6 L1312.71 1326.67 L1312.71 1326.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1312.71,1326.67 265.903,1326.67 265.903,1309.6 1312.71,1309.6 1312.71,1326.67 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M2021.83 1305.33 L265.903 1305.33 L265.903 1288.27 L2021.83 1288.27 L2021.83 1305.33 L2021.83 1305.33  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2021.83,1305.33 265.903,1305.33 265.903,1288.27 2021.83,1288.27 2021.83,1305.33 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M1481.55 1284 L265.903 1284 L265.903 1266.94 L1481.55 1266.94 L1481.55 1284 L1481.55 1284  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1481.55,1284 265.903,1284 265.903,1266.94 1481.55,1266.94 1481.55,1284 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M772.421 1262.67 L265.903 1262.67 L265.903 1245.6 L772.421 1245.6 L772.421 1262.67 L772.421 1262.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"772.421,1262.67 265.903,1262.67 265.903,1245.6 772.421,1245.6 772.421,1262.67 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M400.974 1241.34 L265.903 1241.34 L265.903 1224.27 L400.974 1224.27 L400.974 1241.34 L400.974 1241.34  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"400.974,1241.34 265.903,1241.34 265.903,1224.27 400.974,1224.27 400.974,1241.34 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M1785.46 1220 L265.903 1220 L265.903 1202.94 L1785.46 1202.94 L1785.46 1220 L1785.46 1220  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1785.46,1220 265.903,1220 265.903,1202.94 1785.46,1202.94 1785.46,1220 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M1177.63 1198.67 L265.903 1198.67 L265.903 1181.6 L1177.63 1181.6 L1177.63 1198.67 L1177.63 1198.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1177.63,1198.67 265.903,1198.67 265.903,1181.6 1177.63,1181.6 1177.63,1198.67 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M873.724 1177.34 L265.903 1177.34 L265.903 1160.27 L873.724 1160.27 L873.724 1177.34 L873.724 1177.34  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"873.724,1177.34 265.903,1177.34 265.903,1160.27 873.724,1160.27 873.724,1177.34 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M569.814 1156 L265.903 1156 L265.903 1138.94 L569.814 1138.94 L569.814 1156 L569.814 1156  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"569.814,1156 265.903,1156 265.903,1138.94 569.814,1138.94 569.814,1156 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M1042.56 1134.67 L265.903 1134.67 L265.903 1117.6 L1042.56 1117.6 L1042.56 1134.67 L1042.56 1134.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1042.56,1134.67 265.903,1134.67 265.903,1117.6 1042.56,1117.6 1042.56,1134.67 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M2123.13 1113.34 L265.903 1113.34 L265.903 1096.27 L2123.13 1096.27 L2123.13 1113.34 L2123.13 1113.34  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2123.13,1113.34 265.903,1113.34 265.903,1096.27 2123.13,1096.27 2123.13,1113.34 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M1211.4 1092 L265.903 1092 L265.903 1074.94 L1211.4 1074.94 L1211.4 1092 L1211.4 1092  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1211.4,1092 265.903,1092 265.903,1074.94 1211.4,1074.94 1211.4,1092 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M1616.62 1070.67 L265.903 1070.67 L265.903 1053.6 L1616.62 1053.6 L1616.62 1070.67 L1616.62 1070.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1616.62,1070.67 265.903,1070.67 265.903,1053.6 1616.62,1053.6 1616.62,1070.67 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M1717.92 1049.34 L265.903 1049.34 L265.903 1032.27 L1717.92 1032.27 L1717.92 1049.34 L1717.92 1049.34  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1717.92,1049.34 265.903,1049.34 265.903,1032.27 1717.92,1032.27 1717.92,1049.34 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M1886.76 1028 L265.903 1028 L265.903 1010.94 L1886.76 1010.94 L1886.76 1028 L1886.76 1028  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1886.76,1028 265.903,1028 265.903,1010.94 1886.76,1010.94 1886.76,1028 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M1852.99 1006.67 L265.903 1006.67 L265.903 989.603 L1852.99 989.603 L1852.99 1006.67 L1852.99 1006.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1852.99,1006.67 265.903,1006.67 265.903,989.603 1852.99,989.603 1852.99,1006.67 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M1751.69 985.336 L265.903 985.336 L265.903 968.269 L1751.69 968.269 L1751.69 985.336 L1751.69 985.336  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1751.69,985.336 265.903,985.336 265.903,968.269 1751.69,968.269 1751.69,985.336 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M1988.06 964.003 L265.903 964.003 L265.903 946.936 L1988.06 946.936 L1988.06 964.003 L1988.06 964.003  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1988.06,964.003 265.903,964.003 265.903,946.936 1988.06,946.936 1988.06,964.003 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M1515.31 942.669 L265.903 942.669 L265.903 925.603 L1515.31 925.603 L1515.31 942.669 L1515.31 942.669  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1515.31,942.669 265.903,942.669 265.903,925.603 1515.31,925.603 1515.31,942.669 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M299.671 921.336 L265.903 921.336 L265.903 904.27 L299.671 904.27 L299.671 921.336 L299.671 921.336  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"299.671,921.336 265.903,921.336 265.903,904.27 299.671,904.27 299.671,921.336 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M536.046 900.003 L265.903 900.003 L265.903 882.936 L536.046 882.936 L536.046 900.003 L536.046 900.003  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"536.046,900.003 265.903,900.003 265.903,882.936 536.046,882.936 536.046,900.003 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M367.207 878.67 L265.903 878.67 L265.903 861.603 L367.207 861.603 L367.207 878.67 L367.207 878.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"367.207,878.67 265.903,878.67 265.903,861.603 367.207,861.603 367.207,878.67 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M333.439 857.336 L265.903 857.336 L265.903 840.27 L333.439 840.27 L333.439 857.336 L333.439 857.336  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"333.439,857.336 265.903,857.336 265.903,840.27 333.439,840.27 333.439,857.336 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M975.028 836.003 L265.903 836.003 L265.903 818.937 L975.028 818.937 L975.028 836.003 L975.028 836.003  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"975.028,836.003 265.903,836.003 265.903,818.937 975.028,818.937 975.028,836.003 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M941.26 814.67 L265.903 814.67 L265.903 797.603 L941.26 797.603 L941.26 814.67 L941.26 814.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"941.26,814.67 265.903,814.67 265.903,797.603 941.26,797.603 941.26,814.67 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M1549.08 793.337 L265.903 793.337 L265.903 776.27 L1549.08 776.27 L1549.08 793.337 L1549.08 793.337  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1549.08,793.337 265.903,793.337 265.903,776.27 1549.08,776.27 1549.08,793.337 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M839.956 772.003 L265.903 772.003 L265.903 754.937 L839.956 754.937 L839.956 772.003 L839.956 772.003  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"839.956,772.003 265.903,772.003 265.903,754.937 839.956,754.937 839.956,772.003 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M434.742 750.67 L265.903 750.67 L265.903 733.603 L434.742 733.603 L434.742 750.67 L434.742 750.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"434.742,750.67 265.903,750.67 265.903,733.603 434.742,733.603 434.742,750.67 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M502.278 729.337 L265.903 729.337 L265.903 712.27 L502.278 712.27 L502.278 729.337 L502.278 729.337  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"502.278,729.337 265.903,729.337 265.903,712.27 502.278,712.27 502.278,729.337 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M468.51 708.004 L265.903 708.004 L265.903 690.937 L468.51 690.937 L468.51 708.004 L468.51 708.004  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"468.51,708.004 265.903,708.004 265.903,690.937 468.51,690.937 468.51,708.004 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M603.582 686.67 L265.903 686.67 L265.903 669.604 L603.582 669.604 L603.582 686.67 L603.582 686.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"603.582,686.67 265.903,686.67 265.903,669.604 603.582,669.604 603.582,686.67 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M1278.94 665.337 L265.903 665.337 L265.903 648.27 L1278.94 648.27 L1278.94 665.337 L1278.94 665.337  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1278.94,665.337 265.903,665.337 265.903,648.27 1278.94,648.27 1278.94,665.337 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M671.117 644.004 L265.903 644.004 L265.903 626.937 L671.117 626.937 L671.117 644.004 L671.117 644.004  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"671.117,644.004 265.903,644.004 265.903,626.937 671.117,626.937 671.117,644.004 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M1684.15 622.67 L265.903 622.67 L265.903 605.604 L1684.15 605.604 L1684.15 622.67 L1684.15 622.67  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1684.15,622.67 265.903,622.67 265.903,605.604 1684.15,605.604 1684.15,622.67 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M1143.87 601.337 L265.903 601.337 L265.903 584.271 L1143.87 584.271 L1143.87 601.337 L1143.87 601.337  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1143.87,601.337 265.903,601.337 265.903,584.271 1143.87,584.271 1143.87,601.337 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M1380.24 580.004 L265.903 580.004 L265.903 562.937 L1380.24 562.937 L1380.24 580.004 L1380.24 580.004  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1380.24,580.004 265.903,580.004 265.903,562.937 1380.24,562.937 1380.24,580.004 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M1819.22 558.671 L265.903 558.671 L265.903 541.604 L1819.22 541.604 L1819.22 558.671 L1819.22 558.671  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1819.22,558.671 265.903,558.671 265.903,541.604 1819.22,541.604 1819.22,558.671 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M1110.1 537.337 L265.903 537.337 L265.903 520.271 L1110.1 520.271 L1110.1 537.337 L1110.1 537.337  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1110.1,537.337 265.903,537.337 265.903,520.271 1110.1,520.271 1110.1,537.337 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M2089.37 516.004 L265.903 516.004 L265.903 498.938 L2089.37 498.938 L2089.37 516.004 L2089.37 516.004  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2089.37,516.004 265.903,516.004 265.903,498.938 2089.37,498.938 2089.37,516.004 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M1346.47 494.671 L265.903 494.671 L265.903 477.604 L1346.47 477.604 L1346.47 494.671 L1346.47 494.671  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1346.47,494.671 265.903,494.671 265.903,477.604 1346.47,477.604 1346.47,494.671 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M2190.67 473.338 L265.903 473.338 L265.903 456.271 L2190.67 456.271 L2190.67 473.338 L2190.67 473.338  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2190.67,473.338 265.903,473.338 265.903,456.271 2190.67,456.271 2190.67,473.338 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M1076.33 452.004 L265.903 452.004 L265.903 434.938 L1076.33 434.938 L1076.33 452.004 L1076.33 452.004  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1076.33,452.004 265.903,452.004 265.903,434.938 1076.33,434.938 1076.33,452.004 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M907.492 430.671 L265.903 430.671 L265.903 413.605 L907.492 413.605 L907.492 430.671 L907.492 430.671  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"907.492,430.671 265.903,430.671 265.903,413.605 907.492,413.605 907.492,430.671 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M1582.85 409.338 L265.903 409.338 L265.903 392.271 L1582.85 392.271 L1582.85 409.338 L1582.85 409.338  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1582.85,409.338 265.903,409.338 265.903,392.271 1582.85,392.271 1582.85,409.338 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M1008.8 388.005 L265.903 388.005 L265.903 370.938 L1008.8 370.938 L1008.8 388.005 L1008.8 388.005  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"1008.8,388.005 265.903,388.005 265.903,370.938 1008.8,370.938 1008.8,388.005 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M2258.21 366.671 L265.903 366.671 L265.903 349.605 L2258.21 349.605 L2258.21 366.671 L2258.21 366.671  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2258.21,366.671 265.903,366.671 265.903,349.605 2258.21,349.605 2258.21,366.671 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M2156.9 345.338 L265.903 345.338 L265.903 328.271 L2156.9 328.271 L2156.9 345.338 L2156.9 345.338  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2156.9,345.338 265.903,345.338 265.903,328.271 2156.9,328.271 2156.9,345.338 \"/>\n",
       "<path clip-path=\"url(#clip292)\" d=\"M2291.97 324.005 L265.903 324.005 L265.903 306.938 L2291.97 306.938 L2291.97 324.005 L2291.97 324.005  Z\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip292)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"2291.97,324.005 265.903,324.005 265.903,306.938 2291.97,306.938 2291.97,324.005 \"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"299.671\" cy=\"1168.8\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"333.439\" cy=\"1062.14\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"367.207\" cy=\"678.137\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"400.974\" cy=\"742.137\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"434.742\" cy=\"294.138\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"468.51\" cy=\"635.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"502.278\" cy=\"1083.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"536.046\" cy=\"1318.13\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"569.814\" cy=\"443.471\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"603.582\" cy=\"827.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"637.349\" cy=\"1019.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"671.117\" cy=\"1211.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"704.885\" cy=\"912.803\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"738.653\" cy=\"230.138\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"772.421\" cy=\"806.137\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"806.189\" cy=\"550.137\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"839.956\" cy=\"486.138\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"873.724\" cy=\"379.471\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"907.492\" cy=\"400.805\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"941.26\" cy=\"464.804\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"975.028\" cy=\"315.471\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1008.8\" cy=\"614.137\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1042.56\" cy=\"1382.13\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1076.33\" cy=\"1232.8\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1110.1\" cy=\"1339.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1143.87\" cy=\"1360.8\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1177.63\" cy=\"955.469\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1211.4\" cy=\"976.803\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1245.17\" cy=\"592.804\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1278.94\" cy=\"1040.8\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1312.71\" cy=\"1296.8\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1346.47\" cy=\"1254.14\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1380.24\" cy=\"1275.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1414.01\" cy=\"1190.14\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1447.78\" cy=\"763.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1481.55\" cy=\"1147.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1515.31\" cy=\"507.471\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1549.08\" cy=\"848.803\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1582.85\" cy=\"699.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1616.62\" cy=\"422.138\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1650.38\" cy=\"870.136\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1684.15\" cy=\"251.472\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1717.92\" cy=\"720.803\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1751.69\" cy=\"187.472\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1785.46\" cy=\"891.47\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1819.22\" cy=\"998.136\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1852.99\" cy=\"571.471\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1886.76\" cy=\"934.136\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1920.53\" cy=\"144.805\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip292)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1954.3\" cy=\"208.805\" r=\"2\"/>\n",
       "</svg>\n"
      ],
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAlgAAAGQCAIAAAD9V4nPAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nO3dd1wU1/o/8LOVpTeVKmJBsSGiIGLD2K7GeK0YzTVRSaLG2L9RU26isURijC3FYOK1phhjFI1RQSOCosaGBbBhoSt9EbbO/P6Y3927gYUssLuzO/N5/+FrOTs78xwGeZgzZ84joGmaAAAA8JWQ7QAAAADYhEQIAAC8hkQI1mXOnDnSehQXF5v2WDdu3IiPj799+7Zpd9scM2bMkEql27dvZzuQRlCr1fHx8QcOHGA7EIAmErMdAMBfaDQatVrdoUMHf3//Wm9JJBLTHuvkyZPvvPPO1q1bu3btato9NxnTfa1Wy3YgjaBUKmfNmtWtW7eJEyeyHQtAUyARgjWaO3fuwoUL2Y4CAHgBiRBsUkVFxalTpx4/fiyRSHr37t2nTx+BQFBrm4cPH169ejU3N1cgEHTq1Gnw4MFSqVT3bkZGRl5eHiEkJyfnypUrTGPHjh2dnZ0LCgry8/PbtGnTokUL/R3evn1boVCEhYUxx3r+/HlWVpa7u3u7du0KCwtPnjxZVFQ0atQo3fVlQUHB6dOnCwoKXFxcBgwY0Llz5yb0NDs7u6ysrHPnzjKZ7I8//rh165azs/PIkSN9fHyYDe7fv3/mzBm5XB4eHt6/f3/9zxYWFubl5QUEBLRs2fLPP/+8ePEiTdNRUVG9evWqeyCKotLS0q5du6ZWq9u2bTt06FAnJyf9DR4/flxcXMx8i9LS0q5cuaLRaKZMmXL37l1CSE1Nje7byHxPmNelpaVpaWlPnjypqakJCAh44YUXPDw89Hcrl8vv3r3r6ekZGBiYn59/4sSJ0tLSDh06/OMf/7Czs6sb57Nnz/7444+8vDwHB4d27doNHDiw1mZqtTolJSUjI0Oj0XTo0GHo0KEymaxR33PgHRrAmrz++uuEkI0bNzawzZYtW1xcXPR/jPv165efn6/bQKvVdunSpdaPeuvWrc+dO6fbJiQkpO5/h6SkJJqmV69eTQj57rvvah23U6dOhBC1Ws18eeHCBULI5MmTN2/erBu23bp1K03TarV64cKFtcZyp0yZUl1d3XD3//WvfxFCvv76a10LM9547NixyMhI3a7s7e0TEhIoilq6dKlQ+L87/a+99hpFUbrPfvLJJ4SQzZs3jxs3Tj+Sl19+WalU6h/3/v37PXv21N+mRYsWBw4c0N8mNjaWEHLgwIHBgwfrNtu8eXPdb2NMTAzzkTfeeEMkEum/5ejoqN87mqbPnDnDRP7ll1/q/6XSsWPHJ0+e6G+p0WiWLVtWK+25uLjk5ubq702Xgxn+/v5nz55t+NsOPIdECNblbxPh559/TggJDAzcuXPnzZs309LSmI+EhYWpVCpmG41G06VLl88+++zMmTN3795NS0t75513JBKJp6dncXExs01aWtqbb75JCHn77bcT/6ukpIRuZCJs3bq1g4PDhx9+mJiYmJSUdOXKFZqmp0+fTgjp3bv3oUOHsrKyTp06NXz4cELIK6+80nD360uEbdq0GTBgwJEjRy5fvrxixQqhUOjh4bFy5coWLVp88803ly9f/vnnn/38/AghP//8s+6zTCL09fVt167dkSNHnjx5cubMGeZycM6cObrNysrK2rRpw2Sjy5cvZ2Vlbdiwwd7eXigUnjp1SrcZkwgDAgJ69OixY8eO8+fP79mz5+HDhwkJCcwZ0X0bb9y4wXxkwoQJ77zzzu+//56RkXHt2rWtW7e2bNlSIBCcPn1at1smEQYGBjo5Oa1bt+7ixYtJSUkvvPACIeSll17S/+YwAXTo0GHPnj137ty5du3avn37RowY8fDhQ2aDS5cu2dnZOTo6rl69+tKlS+np6evXr7e3t3dycrp//37D33ngMyRCsC5MVmvdunWvv/rhhx9omi4oKJDJZC1atNC//qNp+rXXXiOE7N27t4E9r1y5khCyadMmXcv69evJf6/h9DUqERJCdu7cqb9ZcnIyISQ0NFT/qkur1TIZKD09vYEg60uEPXr00B2XpulJkyYRQkQikf7emHmbkydP1rUwiVAoFGZkZOgaS0pKXF1dhULhgwcPmJYVK1YQQl588UX9SLZt20YI6dmzp66FyUOtWrWqqKjQ31IulxNCunXr1kC/dJi0N3bs2FothJDjx4/rGisrK93d3UUike4a+ty5c8zl3dOnT+vbee/evQUCwbFjx/Qbd+/eTQiZPn26MeEBP+HxCbBGpaWlj/6qsrKSEPLzzz8rFIrY2FjdHTLGnDlzCCHHjh1rYJ///Oc/CSGXLl0ybah+fn5M9tLZs2cPIWTp0qX6A31CoXDWrFmEkN9//70JR5k/f75Y/L87+oMGDSKEDBkyRH+Al2l8+PBhrc+OGjVK//akh4fHjBkzKIo6fPgw0/Lrr78SQpYvX67/qRkzZnh7e1+7di07O1u/fc6cObXGpRtl4MCB7u7udc9CSEjIiBEjdF86OztHRUVptdrHjx8zLd9//z0hZOHChS1btjS458zMzMuXL/fq1WvkyJH67f/6179cXFya9m0HnsBkGbBGq1evNjhr9Nq1a4SQ27dv1/qt/fz5c0KI7pcm8zouLu7s2bP5+fllZWW6dpM/jNipU6dat8GYIJOSktLT0/XbHz16pPu3sTp27Kj/JZMMgoKC9Bs9PT2FQmFRUVGtz4aGhhps0T1AmZGRQQgJCwvT30YqlXbv3r2wsDAjI0P/rlvdm68NeP78+eeff37kyJGcnJyioiL6vws6VldX19qSudrW5+XlRQgpKioKDg4mhFy/fp0QUutGpr6rV68SQpRKZa2fDUKInZ1dUVFRTU2Nvb298cEDfyARgi1hUlpycjIzUKbP3d1dd8107969vn37lpWVRUVFjRw5khlkKysri4uLM/kjerVmlhJCysvLCSEHDx6sO5HV3d29bqMxav0GZ3bi4OBQq1EgENB1Vg+uewnVqlUrQggzpKlUKtVqtZOTU629kf+mImYznbr9rY9SqRw0aNCVK1eCg4MnTZrUokULZp5LXFwcc32vr+7RmUlAFEUxXzIfqTUSoI/5tt+7dy8+Pr7uu+7u7kiEUB8kQrAlzs7OhJCvvvqq1mhkLWvXri0pKdm8efP8+fN1jRcuXIiLizPmKLV+Besw15211E1szFMHycnJBiemWt7Tp09rtTBXjcwIp52dnVQqraqqqq6urpWNCgsLdZs1wQ8//HDlypWJEyfu379f912iKIq5WdtYbm5uhJD8/Pz6nkJhfjYmT568c+fOpgUMvIV7hGBLmJGx8+fPN7wZMyY5ZcoU/UZm6Ewfcw+v7jWit7c3+W+20KmoqGCeOzRVkBZTt+NMS7du3ZgvmQcfL1++rL+NUqm8ceOG/mb1YZ4S0Wg0tdqZwczJkyfr/62QmZlZU1PT+E78/++q7lHF+jY4f/583WtigIYhEYItefnll+3t7Xfv3l13gVCapquqqpjXzPDdkydPdO8+f/78008/rfURX19fQkhOTk6tduaW2PHjx/Ub161bZ+Rv2BkzZhBC1q9fX1paWusttVqtUCiM2YkJnThx4tatW7ovi4uLd+7cKRKJxo4dy7QwE1Pj4uL0O/jtt98+ffo0IiKCebKiAXZ2di1atCgsLKyVC5khWf2zQAj56KOPmtaLadOmEUI2bdrEXKfW1b179/Dw8Hv37u3YsaPuu7qfDYC6kAjBlvj4+GzcuPH58+f9+/dfu3btqVOnbt68efTo0bVr13bu3Fm37jPzxPe0adOOHDly9+7dI0eODBo0qNaUFkJIWFiYUCjcvn37u++++/XXX8fHxzPXfFFRUa1bt05NTZ0xY8bp06ePHTs2c+bM+Ph4I2+PDRw4cM6cOdnZ2b179/7iiy9SU1OvX7/+66+/Ll++PCAggJmZYkn+/v6jRo366aef7t2799tvv73wwgtyufztt9/WZbh58+a1b9/+2LFjU6ZMSU1NvXHjxpo1a5YsWSIWizds2GDMIXr37l1eXj5x4sRNmzbFx8cnJSURQqKjowkhH3/88fbt2zMzM1NTU19++eU//viDGeRsrN69e8+bN6+goKBPnz7bt2+/cePGxYsXd+7cOXjwYN1E2e3btzs5Ob355puzZ89OSEi4efPm6dOnt23b9sILL7z11ltNOCjwBZvPbgDUYczKMj/88EPdJbmDg4NTUlKYDRQKBfOwhE5kZCQzVjlkyBD9XX3zzTf68y+YlWVomj5//rz+HBMfH59z587Vt7JM3Qi1Wu0nn3xS6+6aQCCIiIh4/PhxA12r7znCy5cv62+2f/9+Qsg777xT6+MikSggIED3JfMc4datW19++WX9SGbOnKn/VCJN048fP+7Xr5/+Nr6+vkePHtXfhnmOUP8Re5379+/3799f96eGbmWZNWvW6P/94evre+7cucDAQJFIpPusbmWZWvtkfhL0H73XarUrVqyodSPTy8tL/6HS69ev9+nTp9bPRsuWLbds2VI3bACGgTlmACx6+vRpRUVFy5YtG75uUKvVFy9evHfvnkaj8fb2Dg4OrvUsASEkPT39xo0bWq22c+fOERERWq32yZMn9vb2dWceVlVVMTNKfHx8dBML5XJ5YmLis2fPfH19hw0bJpPJcnJymHU4mZteSqUyLy/PycmJmYRZ1/Pnz8+fP//o0SOxWOzt7d2jRw9mMLYBRUVFlZWVrVq1cnV11bU8f/7cz89Pf2mx58+fFxUVubm51Vq3Mzs7WywWBwQEMF+uW7fu3Xff3bZt26xZs27evHnlyhWKoiIjIw0+AkHT9PXr19PT05VKZfv27fv3719ric5nz57J5XL9b1EtGo2msLBQpVI5OjoyM04JIU+ePLl06VJpaWlgYOCgQYPs7OyePHmi0Wh0j2QoFIr8/HxnZ+das1vrO1x5eXlKSkp+fr6jo2P79u0jIiLqXutnZGRcvXq1qqqqZcuWAQEBYWFhdbcB0EEiBOAs/UTIdiwA1gv3CAEAgNeQCAEAgNcwNArAWRcuXEhOTh4xYkTdVdYAQAeJEAAAeA1DowAAwGtIhAAAwGtmXHQ7Pz9/165dOTk57du3j42NZR4LKy0t3bZtW35+/pAhQ8aNG2e+owMAABjDXFeEWVlZoaGhjx496tatW25uLrNylUajGThwYEZGRkhIyJIlS7Zu3WqmowMAABjJXJNlhg8f3qdPn1WrVuk3Hjx48L333svIyBAKhYmJiTNnznz06BFWfAAAABaZJRGqVCoHB4fU1NRr166pVKqxY8cyy/suXry4pqbm66+/JoSo1Wp7e/usrKwOHTqYPAAAAAAjmeUeYU5ODkVRc+fOHTduXElJSWhoKFOktKCgQJf2JBKJu7t7fn6+wUSYmZn5+uuvBwcH61piYmIGDBhQ3xEpimKKqfIQTdNNK3rODXw+9XzuO+H3Tz6fTz1z8Wb8qZdKpWLx32Q6syRCZmn5t99+mynMplAoPv/88507d0qlUv0iqGq1Wn8dYX3Pnj0rLCxkKpAxgoKC6tuYECKXy5n61DxUU1MjFov/9kxzFZ9PPZ/7TlGUQqGoVYmCP/h86lUqFflvVW1jGPMXg1l+e/r4+IhEIqZmDSGkc+fOCQkJhBA/P7/c3FymsbKysrKy0s/Pz+AepFJpq1atZs+ebeQRRSIRb+81iv6L7UDYgb6zHQU7BAIBn7vP877r/jUVs1xc29nZvfjii0z5N0JIampq165dCSFjxow5fvx4eXk5IWT//v1hYWF1q8oBAABYkrnG01atWjVixIg///yzrKwsNzeXeVIiMjJy2LBhffv2DQ0NTUxM/PHHH810dAAAACOZKxGGhIRkZmampKQ4Ozv37dtXd3tv7969Fy9eLCgo+Pzzz+vWRwUAALAwM86wcHNze+mll2o1CgSCyMhI8x0UAACgUXg6ARcAAICBRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALyGRAgAALwmZjsAE6AoaszElytrVGwHwg6KogQCgUAgYDsQdmi1WpFIxHYU7OBz3wmhKYoWCnn6pzyfTz1N0+8vnDNhwgQT7pMLiVCj0Zw9dZJacITtQAAAwMzO77lx4wYSoQECoZB0HsJ2FAAAYGb3zpl8l+ZKhAUFBTU1NcxriUTSunVr3Vv379/Py8sLCwtzdnY209EBAACMZK5E+Oqrr968edPR0ZEQEhgYeOrUKaZ9wYIFv/zyS+fOnW/cuHH48OHIyEiTHI7Sasnx9SbZFQAAWK/750nP/qbdpRmHRrds2RITE6Pfkp6evmfPnszMTC8vr02bNr3zzjspKSnNP5BUKn1p9IsVxSebvytbRFEUb6cMEH7PGuBz3wm/f/Kt9tT7+voGBASY9RDatmFjxowx7T7NmAgrKioyMzPbtWtnZ2fHtBw4cGDEiBFeXl6EkFdffXXx4sUFBQU+Pj7NPJBKpTpy9DfpyMXNjdhW0YTwdMooIYTf3edz3wm/u2+NfaeePQwrKPn+++/NehSVyvQPCJgxEa5bt04qlebm5q5evXrBggWEkJycnMDAQOZdDw8PZ2fnnJwcg4lQo9FUVFQkJibqWrp168Zk0LooihKKRMqxa0zfBwAAMNLtRPraJoqizHoQZv/GH8WYYQNzJcKffvrJw8ODEJKWljZkyJCoqKjw8HCFQiGVSnXbyGQy3YSaWioqKgoKCj755BNdy7x584YNG2ZwY5VKRWiTRg8AAI1HUVR1dbVZD8FcEWo0GiO3l8lkYvHfZDpzJUImCxJC+vbtGxUVde7cufDwcG9v75KSEqZdq9WWlpbWNy7q6ekZHBx8+vRpY46lUqloQpNnD00SOQAANEVlkVAkcnJyMutBmESof03VfGZ/jlCr1T558qRFixaEkIiIiLi4OKY9LS3Nw8NDN1LaHGKxOCi4a+lXI5q/K1tE0zRvl5Uh/O4+n/tO+N19q+17yJiXysrKTLIrsVhssUfsBDRt+lHFZ8+eLVu2LDo6WiKRfP/991lZWdeuXXNyclIqlZ07dx49enR0dPSHH344ZcqU999/3+AeLly4sGjRorS0NGMOp9Fo7B0cRTJHk3YCbANNaIH1zRqwDD73nef4cOo1NVXyygp7e/ta7TZzRejs7BwcHHzq1CmKogYMGLB3717mYtnOzu7s2bMbN248cODAwoULY2NjTXI4iqJoQpSfF5lkbwAAwDrpAk/jbwQ2k1kSoUwmW7p0qcG3/P39N2zYYI6DAgAANAFPH0cFAABgcGHRbaFQSFOU+3rTrNZmc2iaCIgVPlxrITRNrHLSgCXwue+EJjThb/f5cOorlTUWWz2HI4kwvG//iufmfXjFalEUzeNyhITSUkJR4wY2oqMiX5/xqpnisaTq6moHBwe2o2AHRVEqlUomk7EdCDv4cOpdXFws1kcuJEKNRvNnWirqEYJRsi+2un+lV69ebMdhAnK5nLclXCiKUigUnE8G9eHzqTcHLiRCgnqEYDyVgty7wnYQAGBFMFkGAAB4jSNXhKhHCMYqyCIYUgIAPVxIhKhHyNuqbKQJhdkkpJ1vO91SfzZNqVTqapxxlVQqnTVrFm/vBYJlcCERoh4hfx+eIKQJ3b/wlJCnJWaKxrJoQqrYjsG8BBf29uvXLyIigu1AgMu4kAgJIahHCMBJrvfPsB0CcB9/h9QAAAAIZ64IaRr1CAE4iFIr2Q4BuI8LiRD1CPmwroxAIDTYSYqmhTzovkF86LudnbRVq1ZsRwEcx4VESFHUgzsZqEfIYRrF8317dk+ePLnuW3xeYoPPfQcwIY4kQtQj5Danva8rFAq2owAAbsJkGQAA4DUkQgAA4DUuDI2iHiHn6xHWFD2WSHg6GQoAzI0jiRD1CDk+ebBdh8++3rFh23/qvtPoJdaa5K0Zr8TOmG7uowAAK7iQCFGPEMzr+tHzly4jEQJwFRcSIUE9QjCrwruE3GU7CAAwF0yWAQAAXuPIFSHqEYIZPbhAIgPYDgIAzIULiRD1CFGP0LzHcCV2AsoKSxjyoR5hfWia1mg0EomE7UDYYXOnXiaTzZ4922pj5kIiRD1Cjj888Tcs0f0LTwh5YoUlDLlfj7BBfP7Jt7FTL0j9z9ChQ7t27cp2IIZxIRES1CMEALBiLrd/YzuEhvB3SA0AAIBw5ooQ9QgBAKwWpVazHUJDuJAIUY+Q6+vKNITP3W9S3wXcKGFIE0IILeDrPUKbK0Xp4Cjz9PRkO4p6cSERoh4hn9E8/m3YhL6rqyvz8/K8vLzMFJLFUBSlUCgcHBzYDoQdKEVpWhxJhKhHCGAMxw86KJVKtqMAsC6YLAMAALyGRAgAALzGhaFR1CPkfD3CBtA0salJA6bUhL7LSwt5uxoLQH04kghRj5C3yYDSUkIRTwc2mtB3n5Beoye/aqZ4LIumKJq3iwtapgxnr5Cu8V9sMvdRrAEXEiHqEQIAmFhJTtGJjWwHYSFcSIQE9QgBAEyrIIucYzsGS+HpwAIAAACDI1eEqEcIAGBKlc/YjsByuJAIUY+Qt1MGiKVmDdTH3t6+R48ebB1dpVJJpVK2js4untcjtMipt+s67V0zH8JacCERoh4hfx+eIITd7qt///zw4cNsZSM+r7OFJdZ4e+rNgQuJkKAeIbBEdHIz2yEAQHPxd0gNAACAmPuKsKCgYMKECcOHD1+xYgXTcvLkyWXLluXn5w8ZMuSrr75yc3MzyYFQjxBYQdM02yEAQHOZNxHOnTu3pqbm0aNHzJfFxcWTJk3auXNndHT0m2++uXjx4h07djT/KKhHyN91ZdjuvmfnbnK5nK3JSlVVVRqNhpVDs46iKKVSaZ2VNNzd3dkOARrHjInwxx9/lMlkI0aMKCwsZFr27dvXu3fvcePGEUJWrVoVFha2efPm5t/yRT1CPmO3HmFlRYVf2yC2js7nWoxWS1NTdey3o8OHD2c7EGgEcyXC4uLilStXnjlzZuPG/y3Sk5WVFRoayrwODg4mhDx69Kh79+7NPBbqEQKAlXCNn6BQKNiOAhrHXIlw3rx5y5Ytq1UIu6SkRL/FxcWluLjY4MeLioouX76sP8IQFxc3ZcoUgxurVCqCOzUAYAVomq6pqZHL5WY9SlVVlVn3b81UKhUhxPhnlmQy2d8+b2qWRHjixIm7d++uXr06Ozu7vLxcLpfn5OS0bt3a3d1d//xVVlZ6enoa3IOXl1fPnj1PnDiha2lg2F2lUmF8CACsgUAgsLe3t8BDfrx9jrCxidAYZkmEZWVlAoFg8uTJhJC8vDy1Wj1//vxff/01KCjo1KlTzDbZ2dkajaZNmzb17UQkEhl5zxn1CFGPkJ/43HdCE5pYY/erC7IlkjlsRwGNIzD3/O/ly5cXFhbu3LmTEJKfn9+xY8fjx49HRkbOnj27qqrqxx9/NPipCxcuLFq0KC0tzZhDUBQVNWgI6hHyE+oRmmpvU8aNfnHUKFPtzdwoilKpVDKZjO1AahOJRCEhIeaeSMznlWVs5opQn4eHh1arZV77+vru2LFjypQpZWVl/fr127Vrl0kOgXqEAM115dei4rJevXqxHYexeL7EGpiW2RPh0qVL9b+MiYmJiYkx+VFQjxCgWXJvEZLPdhAA7ODpmBIAAACDI4tuox4hQLPcO0+8O7IdBAA7uJAIUY8Q9QjZjqI2gUDQqVMnUy2lWx9TFqXr0I1Z8gmAh7iQCFGPkL8PTxBind2n7iSPHNmu1g1yk+Pz1EEAE+JCIiSoRwhWRvzr+yhMAWAr+DukBgAAQDhzRYh6hGBV6OpyQgwvHwgA1oYLiRD1CPm7roy1dl8gEPj49C8rKzPrUVCPsLH1CJ2dncViLvzSA9Piws8E6hHymdXW5Hvz7YXk7YVmPYTV9t06aVWK5UvfWfXxSrYDAavDkUSIeoQA8DeOf/a8ppztIMAaYbIMAADwGhIhAADwGheGRlGPEPUI+YnPfW9CPUJFaZG042tmCwhsGEcSYXjf/qhHaDyRgKx8b2lgYKC5YrKg6upq3tbi4XPfm1aPMDg42EzxgE3jQiJEPcLGcjzyb0dHRxsqPtcAPi8zxue+ox4hmBAXEiFBPcJGkpzZxHYIAADWApNlAACA1zhyRYh6hI2iKMxmOwQAAGvBhUSIeoSNrUcoCm59/vz59PR0M4VkSUql0s7Oju0o2EFR1Lx585ycnNgOBMC2cSERoh5hEx6eOHdVQYjCHNFYHE1IFdsxsENw8fsBAwb079+f7UAAbFtDiZCiqMePH8vl8pCQEIsF1DSoRwg85PrwHNshAHCB4SE1iqJWrFjh5ubWrl27UaNGMY3z58+fPXu2BWMDAAAwO8NXhCtWrPj000/nz5/v7u7+5ZdfMo3Dhg2bOnXqli1bpFKpBSM0CuoRAg9RqsYVIQIAgwwkQo1Gs3nz5rVr1y5evDg5OVmXCENDQ6uqqnJyctq3b2/ZIP8G6hEKhULeluOhaFrI13XGpFKxt7c321EA2DwDifDp06eVlZX/+Mc/arW7ubkRQkpLS60tEfK8HqFGUf1t/Lbp06ezHQg7+Ly6Cp/7DmBCBhKhs7OzUCgsKCjo0qWLfvutW7cIIVb4FyjP6xE6fD+nsXW6AQBAx8BkGWdn5379+q1ataqqqkq3lnNFRcXy5ct79OjRunVry0YIAABgRoYny2zZsmXQoEGdO3fu2rVrZWXlzJkzjx8/XlpampiYaOH4AAAAzMpwIgwNDb18+fKKFStOnjwpl8t//vnn6OjolW1QvUIAACAASURBVCtXhoWFWTg+Y/C8HmFN0WPJv6LYjgIAwFYZSIQKhWL37t3R0dH79u0jhNA03bhidxZnzfUIRw974eWYiWY9hFqt7tGjh1kPAQDAYQYSYVlZ2axZs9LS0pgvrTwLEmuuR5hxqmtugbnL/tXU1EgkErMeAgCAwwwkwpYtW3p4eOTn51s+miaz0nqEZflE+YztIAAAoCEGZo2KxeKPPvroo48+ys3NtXxAAAAAlmR4sszNmzefPXvWoUOHsLAwX19f/So/+/fvt1RsjWCl9QgfXyMdefqYPwCArTCcCB8/fuzv7+/v769SqR49emTZkBrNeusROhJPJ4+4uLi677i4uMyePdv6778CAHCe4UR48qT1JZX6WXM9wgt5hOSV1G3XJq6YPHmyh4eH5UMCAAB9XCjMS2ywHqEs5Tu2QwAAAELqS4Rnz55VqVQG3xo6dKg54wEAALAow4kwJiamqMjwGtY0TZszniayuXqENEWxHQIAABDSwD1CtVqt+7K0tDQ5Ofm7777bsmWLpQJrBGuoRygUGHgQpQFugYEODg5mCgYAAIxnOBGGhITUahk2bJi/v/+aNWsmTpxobXMdWa9HqK6uvH3rVnBwMFsBAABAkzVissyYMWPmzJlz584da/uNz3o9Qtd14QqFgq2jAwBAczRiQA8LzQAAAPcYNWuUoqiHDx+uX7/e29s7KCjImP0eP378/PnzxcXFgYGBr732mpeXF9NeWlq6bdu2/Pz8IUOGjBs3rvkdAAAAaI5GzBoNDQ3du3evSCQyZr8JCQmtW7fu0aNHcnLypk2b0tPTW7ZsqdFoBg4cGBoaOnDgwCVLluTm5s6bN6+5PbCCeoRVefdQ/wEAwEYZNWtUKBT6+Ph4e3sbv9+vvvqKeTFr1qygoKCUlJTx48cnJCRoNJrdu3cLhcK2bdvOnDnzrbfeMjKzNoD1eoTeXUJenbOQraNTFCUQCKxtBlPDPvq/eWPGjGE7CgAAQoyfNdpkt27dKikp6dq1KyEkNTV18ODBzBLe0dHRBQUFDx8+7NChQzMPYb31CMEQwfnd6enpSIQAYCUMJ0Jvb++DBw9GRUXpN6alpUVFRRn/QP28efP27t1bVVW1bdu2Tp06EUIKCgp0aU8ikbi7u+fn5xtMhOXl5dnZ2bGxsbqWmJiYAQMGGDyQSqWy0nqEYNDdVLVaXV1tmiv4mpqa5g8q2Cg+952iKD5P1ebzqWfmr2g0GiO3l0qlYvHfPB/RiMcntFrt3+5OX1xc3IcffpiWljZjxowuXbr07dtXKpVqtVrdBmq12s7OzuBnHRwcnJycwsPDdS1BQUH1bWxbo4JACBGJRPWdzcZSqVSm2pXN4XPfKYqiaZq33efzqWd+4UulUiO31y8jWB9jE5tCoUhMTPTx8TFye0KIg4ODg4PDmDFjRo4cmZCQ0LdvXz8/P90zGJWVlZWVlX5+fgY/K5VKW7VqNXv2bGMOJBKJrLQeIRh0/7wwYrCp/p4ViUS8/dOYz30XCAR87j7P+67711T+kgg3bty4ePH/L2bUr1+/ulu/++67xuxUrVZrtVqZTEYIUSgU169fZ0Y1x4wZM2bMmPLycjc3t/3794eFhfn7+ze3B9Zcj9AiKIoy5k8eK+JFSktLDZZpbAKlUsnbP4251PeePXsOHz6c7SiAp/6SCPv3779u3TpCyOrVq6dNm9amTRvdW/b29t27dx88eLAxO3327FlISEhkZKSdnd3Fixe7d+8+ffp0QkhkZOSwYcP69u0bGhqamJj4448/mqQP1lyP0CJoQmxscPjCPULuGSjT2CQ0IVUm2pXN4Ujf6bK8jr8cuYlECCz5SyIMDw9nbsup1epaibBRfH19b926dfXqVbVavXLlym7duune2rt378WLFwsKCj7//PNGDbQ2zObqEQLA/9w/T59+n+0ggL8M3yP84IMPmrlfb2/vUaNG1W0XCASRkaw9+Q4AAFBLvZNlysvLT548mZ2dXV5ert/OjJ1aG5urRwgA/1NewHYEwGuGE+GlS5dGjRpVUlIikUhEIpFSqaRpWiKRODk5WWEitIZ6hCyiaZrXD5DQRk2P5iSKpoVcOfXhI//BdgjAX4YT4dy5c9u3b3/jxo3333/fz8/vgw8++O2335YsWbJx40YLx2cM1usRAltoStuhY8fbV/9kOxB2yOVyZ2dntqMAsHkGEqFKpbp+/fqxY8d8fX0JIRqNRiaTTZgwQSaTvfbaay+++KLxTzJaBuv1CIE1OTeqfprBdhAAYNsMjCmVlpZqNBpmyqiLi0tFRQXTHh0dXVJSkpWVZdEAAQAAzMlAImzZsqVEIiksLCSEBAQEnD9/nllf9N69e4QQzjzACwAAQAwOjYpEoqioqMTExIEDB06ePPm9994bPXp0z5499+7dGxQU1L59e8tH2TDW6xGyi6aJgNjcI/WmoVUqZC74ywwAmsXwZJnNmzeXlJQQQvz9/ffs2bN69erk5OSwsLAvv/yyUetuWwbr9QjZRVG0rZUjNB1HJ6FI0mvgsLrviIVk37dfN7/IFwBwnuGs1qNHD93rmJiYmJgYS8XTFKhHCHU5Hlr+5MkTJEIA+Ft/c3lXWVlZXl4eEBBgmWiaDPUIoRZJkgfbIQCAbaj3SeQtW7YEBAS4urrqyvMuW7Zs6dKllgoMAADAEgxfEa5fv/7dd9+dPn16q1atdu/ezTRGRETExsauWbNGIpFYMEKjoB4h1KJ49pjtEADANhhIhFqtdt26dStWrPjggw+Sk5N1ibB3794VFRW5ublt27a1bJB/o2n1CD08PDp27GimkCxJo9EIhULeLjOmUqkMrvAg6Tw1NDTU8vEAgM0xkAifPn1aWlo6bty4Wu2enp6EkOLiYmtLhE2oR0g/L/O4eubgwYPmi8piampqJBKJFc7mtQwsMwYAzWTgt6eDg4NAIGAen9B3584dQkjLli0tEVcjNboe4dMHdPwZc0UDAAC2w8B4mqura3h4+Lp161Qqle7xNIVC8e9//7tTp06BgYEWDRAAAMCcDI+nbdy4cciQIaGhod26dauqqvq///u/hISE7Ozso0ePWjg+IzW6HmFpjtliAQAAW2I4EUZFRZ07d+69995LSEhQKpWbNm3q06dPfHx8dHS0ZcMzStPqEYb27FlWVmbwLXd3d1PEBQAANqDeGRZhYWHHjx9XqVRyudzBwcHe3t6SYTVK0+oRnjlzxqeNgXVTNTVVvx/7bdgwA6t2AQAA9/wlEYaEhMyYMWPRokWEEIqivvjii1GjRln/IlWmrUfoGj9eoVCYZFcAAGD9/jJZpqKiQpcDNBrNggULrl+/zkZUAAAAFsLTp7ABAAAYXHgK27T1CKvzH0gkc02yKwAAsH4cSYQmrEco6Bj8/trP3l+7wSR7a4L3FsyeMGECW0cHAOCb2onwP//5T3JyMmGezCNkzZo13377rf4Gx48ft1hwRuJUPcILP1xPT0ciBACwmL8kQl9f39zc3Nu3bzNf+vv7FxcXFxcXsxFY43CnHuGDC4RQbAcBAMAjf0mEaWlpbMUBAADACi7cIyRcqkd4/zwJMc2sHwAAMAYXEmED9QgFAkHnzp1dXFwsH1UTdQibMH4820EAAPAIFxJhQ/UIM5ImTer21ltvWTwoAACwDVxIhKT+eoQyZRUz/RUAAMAgrCwDAAC8xpErwvrqEdI1FZYPBgAAbAgXEmED9QiFQmGrVmPrqzvIDQqFQiwWi8VcOJVNUFVVpdFo2I6CHbbYdzc3N4FAwHYUAH/Bhd+eDdcjnBY7y8LxgCXRhBYQnv5itbm+axTPDx38ZfTo0WwHAvAXHEmEJqxHCABm4rJjCop9ghXCZBkAAOA1JEIAAOA1LgyNmrYeoc2haSIgxKZuFZkSTRPezr2wub7XFDyUSKaxHQVAbQLrfN78woULixYtMnIRcIqiogYNMVU9QptDUbRAwN+JeJSWEop4OrBhc30XCAQyBycT/bDSFEULhbbUfRPSarUikcjgW8vnz5o0caKF47EklUpFCJFKpSbcJxeuCDlVjxAAoMku/HD9ejq3E6E5cCEREi7VIwQAaLLsi4TY2KOl1sBcAws0TWdlZZ09e7awsLDWW3fv3k1OTq6srDTToQEAAIxnlivC8vLy7t27y2Qyf3//a9euLViwYOXKlcxbCxYs+OWXX7p06ZKenn748OHISNPMcOFOPUIAgCa7d4507cN2ELbHLIlQKpUeOHCgT58+hJDMzMzu3bu/8sorHTt2TE9P37NnT2ZmppeX16ZNm955552UlBSTHK6+eoR8QFEUb6cMkAZnDXAen/tOLPKTb7UFTVUqleHZIh3CJk5AQdNGM0sidHBwYLIgIaRjx46Ojo4lJSWEkAMHDowYMcLLy4sQ8uqrry5evLigoMDHx6eZh2uoHiEv0Px9eIIQfnefz30nluh+RtLEiV3nzp1r3qM0nlwud3Z2ZjsK7jD7ZJndu3e3atUqLCyMEJKTkxMYGMi0e3h4ODs75+TkGEyEGo2mrKxs//79upYBAwYwGbQuiqLqq0cIANBkMtVziqIoimI7kNqsMyrLYDpufPeNGTYwbyI8d+7c0qVLExIS7OzsCCEKhUL/cl4mk9XU1Bj8YEVFxbNnz3788Uddi1QqHTp0qMGNVSoVscaHIQHAxtG0SqWqrra6Z5Rramp4OyrOPEdofN0VmUz2t8V5zJgI//zzz/Hjx+/du7dv375Mi7e3NzNGSgjRarWlpaX1jYt6enp27Njx4MGDxhxIpVLRxHA9QgCAJqNrKu3s7JycnNgOpDaapq0wKsuwpQfqr1+/PmbMmO3bt48Y8b8ygREREevWrWNep6WleXh46EZKm6OBeoR8QNM0f9eV4Xf3+dx3YpHuC4VCL69/mqqgqb29vUwmM8muwLTMkgiLi4uHDh0aEhKSmZmZmZlJCPnnP/8ZHBw8YcKEDz74YP78+dHR0R9++OGCBQtMktUbrkcI3GZzNflMiM99txhTFTSltJq+/fonnzxmkr2BaZnrivD1118nhOj+kmIuZu3s7M6ePbtx48YDBw4sXLgwNjbWJMdCPUIAsHZZZ+QXPmE7CDDMLImwRYsWuiHQWvz9/Tds2GCOgwIAADQBfx/EBgAAINxYdBv1CFGPkJ/43HdCE5rYUvc11VX2QW3YjgIM40giDO/bH/UI+cnmavKZEJ/7TgihKFootJ2fe0e3qhplr4HD6r5jJxb+sm9n89fYgibjQiJEPUIAsF2OP84rLCxEImQRFxIhQT1CALBZYgeePhpvPfg7rgIAAEA4c0WIeoQAYKOUZU/ZDoHvuJAIUY8Q9QjZjoIdfO47MelPvqOjY/fu3U2yqyaw6/p6UFAQW0cHwo1EiHqE/H14ghB+d5/PfScm6z5NaU5u+e2330ywK7BNXEiEhBDUIwSAJtKqRSe3sB0EsIm/Q2oAAACEM1eENE2Tx1fZjgIAbBBlbIlX4CouJEKxWNwttFflwdlsB8IOmqYJfxeWIRRNC/naez73nbk9KBSaZq6QV/9BJtkP2CguJEKKojLSr6IeIT/xuSYfn/uuVavGjZ+wf98utgMBLuBIIkQ9QgB+ufRTVTFWVQTTwGQZAADgNSRCAADgNS4MjaIeIeoR8hOf+66qLLUb2JftKIAjOJIIUY+Qt78QOVOTTyggyxe+3aVLF+M/Ul1d7eDgYL6QrBlFUX5+fmxHARzBhUSIeoTAAQ4nPxWJRL169TL+I3K53NnZ2XwhWTOKohQKBdtRAEdwIRES1CME2ye6tJvtEAB4igtjSgAAAE3GkStC1CMEW6fKzSBkNNtRAPARFxIh6hGiHiHbUZiAsK3HrVu34uLijP+IUqm0s7MzX0jWjKZpjUYjkUjYDoQd7J56sVgcGxvr5ubGVgAmx4VEiHqE/H14ghAudf/8DQ0hJY35BE1IlbmisQHcOfWNx+apF1450KVLl5EjR7IVgMlxIRES1CMEALAU14KbbIdgYvwdUgMAACCcuSJEPUIAAMvQVlewHYKJcSERoh4h6hGaZl8CgVBgS2MkFE3ZVsAmRdM0LeBr99k99WIp8ff3Z+vo5sCFRIh6hHxmwpp86urKRw8ftm7d2iR7swCsLMPbFeb4fOrNgSOJEPUIofmcV3RRKpVsRwEAlsbTgQUAAAAGEiEAAPAaF4ZGUY8Q9QhNQv4sh7crlQDwGUcSIR/qEXq18Njwyeq67UqlUiwWc2OZsSYwYU0+e3v7Nm3amGRXAGBDuJAIeVGPUKV4uGOawWJ1NTU1EolELObCqWwCTJ8DgGbiyG9P7tcjVPJ5SUkAADPCZBkAAOA1jlwRcr8eoUbFdgQAANzEhUTIk3qErkNfMFisTq1Wi0QibpQkbN269dSpU9mOAgD4hQuJkC/1CLXkRIrBYnVcqcqmVogvfopECAAWxoVESFCPkBuel4ou7mM7CADgHS6MpwEAADSZua4IDx06dOTIkdu3b0+bNm3u3Lm69pMnTy5btiw/P3/IkCFfffWVm5ubSQ6HeoRcUFPJdgQAwEfmSoQPHz7s0qVLdnZ2fn6+rrG4uHjSpEk7d+6Mjo5+8803Fy9evGPHjuYfC/UIOVOPMCCiT1lZWaM+UlVVpdFozBSPOUilUkdHlAwDsCLmSoSLFi0ihNy4cUO/cd++fb179x43bhwhZNWqVWFhYZs3b27+siCoR8gZBTmPfdq0b9RHTFiP0AJoinJ3dyvMecR2IADwPxadLJOVlRUaGsq8Dg4OJoQ8evSoe/fuzdwt6hGCzagsqlnbm+0gAOAvLJoIS0pKvLy8dF+6uLgUFxcb3LKoqOjy5cvu7u66lri4uClTphjcWKVSEdq0kQKYC01ouVxukl1VVfF34T2KopRKpVarZTsQdvD51KtUKkKIVCo1cnuZTPa3VWUsmgjd3d31z19lZaWnp6fBLb28vHr27HnixAn9z9a3W5VKZTtjY8B3AiIw4SrhvF1wnKIoiURiqsIjtoi3p76xidAYFk2EQUFBp06dYl5nZ2drNJoGqt6IRKIGkp8+1CNEPUJbQWnUEilKHgJYF3MlwtLS0vLycrlcXlZWlp2d7enp6erqOnXq1BUrVqSmpkZGRq5du3b8+PGurq7NPxZP6hHWh6JogYAr00Ybj9JSQpENPQ4rFrdw7TVwmEn2pdVqeVuHkhCaomgWVxYM6dzxP998ydbRwbTMlQjj4+O3b99OCElPTz9x4sS///3v6dOn+/r67tixY8qUKWVlZf369du1a5dJjsWLeoQAYD3KC3KPYykr7hDQtDXOM7lw4cKiRYvS0tKM2VilUjk4OWu/4u/dYwCwqGfZrba9WPT4PlvH53M9anPcI7ShMSUAAADT48ii29yvRwgA1qOqlO0IwJS4kAgtVo/Qx8engWmubNFoNEKhkBv1CJtApVKZdpDEhvC57zRNazSav30+zGzEwS9/wNKhwfS4kAgtU4+QLnncPffpDz/8YNajNEFNTY1EIhGLuXAqm4DPN0v43HeKohQKBZ+fIwQT4shvT0vUI8z6g75ooEA8AADYNJ6OpwEAADA4ckVoiXqERffMu38AAGADFxKhxeoRhg0a0NhqeRagUCjEYjGL9widnJzYm7MAANBcXEiEFqtHuGfv4z1795r7KLZFq1bOnBn7zZdb2A4EAKCJOJIIUY+QNSnfVdWYeVAaAMCcMFkGAAB4DYkQAAB4jQtDo6hHyGI9QmVFieyfo9g5NgCAKXAkEaIeIWv1CB29rtzKNFWBvSbgc00+q+17l6B2e777hu0oAIzFhUSIeoQAVkRe/Ojoe2wHAdAIXEiEhBCBUEg6D2E7CgAgpDSX7QgAGgeTZQAAgNc4ckWIeoQA1qKmku0IABqHC4nQYvUIrRNFUbwtRkiseMKIBVht31tERcTFmbdUC9v1CFmmVCrt7Owa9RFfX99p06aZKR5bx4VEaJl6hFaMZu3hCavA5+5ba98V5GhKifkPY63dtwSakKpGbK5V08mrkAjrw4VESCxTjxAAwEYpn0uTt7MdhPXi75AaAAAA4cwVoSXqEQIA2Ch1DdsRWDUuJEKxWNwlpGfFgTfZDoQdNE0LiIC390poihYIedr5BvouFokEAm6P99A0TXO9j/WiaErYyL779x9opmA4gAuJkKKoO7fSZS382A6EJewuNso6miasrS/Htnr6rlE8/8ewoQd/5HLtTIqiFAqFg4MD24GwQy6XOzs7sx0Fd3AkEVI0LV+ZxXYgANbh6q/y3P1sBwFgM3g6sAAAAMBAIgQAAF7jwtAo6hHiFiE/1dd3lbzcPirc4uEA2CqOJELUI+RtMqC0lFDE04GNevvu2OJxfqGpikR6ujqdPPKrSXYFYJ24kAhRjxDAjDaOommav39qAQ9wIRES1CMEAICm4umYEgAAAIMjV4SoRwgAAE3DhUSIeoTWWY9QJpOFhoaa+ygqlUoqlZr7KNbJMn1vuX49bhACt3EhEaIeoXU+PKE5ufnAgQNOTk5mPQqf15ric98BTIgLiZCgHqFVkpz+mu0QAAD+njUOqQEAAFgMR64IUY/QGtEU2xEAAPw9LiRC1COsrx6hSChicR6NS4+ednZ2bB0dAMBIXEiEqEdocLFRSq1q7dMqM/0yGzEBANgMjiRC1CM0ID+zas8UtoMAALB2mCwDAAC8ZukrwtLS0m3btuXn5w8ZMmTcuHEWPjoAAEAtFk2EGo1m4MCBoaGhAwcOXLJkSW5u7rx585q/W9QjNFiPUKtSyBy4MPQNAGBWFv1FmZCQoNFodu/eLRQK27ZtO3PmzDlz5ojFzY1BLBZfvXJZo9GYJEibo1QqxWKxSCSq+5aPj4/l4wEAsC0WTYSpqamDBw9mJvRHR0cXFBQ8evSoQ4cOzd9zu3bteLvWVE1NjUQiaf7fEwAA/GTR354FBQW6tCeRSNzd3fPz8w0mwvLy8uzs7NjYWF1LTEzMgAED6ttzQkICb+84Xrp0ydvbOyAggO1AWKBQKE6fPj1q1Ci2A2HHqVOnoqKizL2aq3UqLCy8f/9+//792Q6EHQkJCWPHjuXnYui3b98WCoWdO3c2cnupVPq31wkWTYRSqVSr1eq+VKvV9T1w7eDg4OTkFB4ermsJCgqqb2OVSjVz5syXX37ZtNHail27dvXp0+eNN95gOxAW3LlzZ+XKlbz9GyguLi4uLq5fv35sB8KCtLS0Q4cODRnC03Lc8+fPHzZsmKenJ9uBsODQoUMikcj4yjbGLCpi0UTo5+eXm5vLvK6srKysrPTzM/wUvFQqbdWq1ezZs43ZLXN7zOBNMj4QCAQCgYCf3RcKhbztOyFEIBAIhUJ+dp/PfWeIRCJ+dt8cv/Es+hzhmDFjjh8/Xl5eTgjZv39/WFiYv7+/JQMAAACoxaJXhJGRkcOHD+/bt29oaGhiYuJPP/1kyaMDAADUJaBp2pLHo2n64sWLBQUFkZGRDUzu//3331955ZVevXoZuc8//vjjhRdeMF2YtiQjI8PV1bW+QWZuq6qqunXrVmQkTx8hvXz5clBQkKurK9uBsKCwsPDZs2fdu3dnOxB2JCcnR0VFSSQStgNhwYMHDwQCQbt27Yzcfty4cW+99VbD21g6ERqpurr6p59+at26tZHbP3z4sG3btmYNyWo9ffrU0dHR0dGR7UBYQFFUTk5OmzZt2A6EHTk5OT4+Pvx8cqampqaiosLb25vtQNjB5994ZWVlAoHAzc3NyO3btm3bvn37hrex0kQIAABgGVh0GwAAeA2JEAAAeA2JEAAAeA2JEAAAeE20YsUKtmNorosXLyYmJtI0zZNiC5WVlX/++adcLm/VqpV+e0pKyunTpyUSSa12LsnLy0tMTLxx44ajo6O7u7uuvaqqKiEh4caNG/7+/jKZjMUIzae4uDg5OTk1NbWwsDAgIEB/smhaWlpSUpJQKOT8LMqqqqqzZ88ySzAyLXK5PCEh4datWxw+9VeuXMnIyMjOzs7Ozi4qKtJNp9dqtUlJSSkpKa6urvr/HbjnwYMHR48evXv3roeHh66+wqNHjw4fPlxUVNS2bVtj1lFrCG3jPvzwwzZt2syaNcvf3/+zzz5jOxyzW7p0qVQqdXNze/XVV/Xb33777aCgoFmzZnl5eX377bdshWdWCQkJHh4e48aNmzp1qouLS3x8PNP+7Nmz9u3bjx49etKkSb6+vo8fP2Y3TjN55ZVXRo0a9cYbb0RGRrZr166goIBpX7p0abt27WbNmuXj47N161Z2gzS3N998UywW79mzh/mS+SU4ZsyYiRMn+vv75+TksBuemURHR4eEhAwdOnTo0KGxsbFMI0VRo0ePDgsLe/311z09PY8fP85ukOazcePGFi1aTJ48OSYmZvbs2UzjyZMnPTw8YmNje/fuPWrUKIqimnMI206Ez549s7e3v3v3Lk3T169fd3Z2rqysZDso88rNza2urn7vvff0E+GDBw/s7e2Z34ynT5/29vZWqVTsxWguBQUFcrmcef3zzz97enoyP/0ff/zx6NGjmfYZM2YsWLCAtRAtgqKoqKioDRs20DSdl5cnk8kePXpE0/SFCxc8PDyqq6vZDtBcTp8+/cILL/Ts2VOXCD/66KNx48Yxr6dNm7ZkyRL2ojOj6OjoX375pVbjH3/84efnV1VVRdP0d99917t3bzZCM7tr1645OTndu3evVntERMQ333xD03R1dXVAQEBSUlJzjmLb9wgTExODg4ODgoIIIT169PDy8kpOTmY7KPPy8/Ozt7ev1Xjs2LGoqChmWCw6Olqj0fz5559sRGde3t7eugExHx8fJtkTQo4ePTpx4kSmfeLEiUePHmUtRIugKEqhULRo0YIQcvz48Z49ezJLCvTp08fBweHcuXNsB2gW1dXVCxYsiI+P1689dOTIkQkTJjCvuX3q7969e+LEiSdPnuhajh49OnLkSGYljQkTJly+bkr/0wAACKBJREFUfLmgoIC9AM1l//7948ePd3JySkpK0tVsKCoqunTpEnPq7e3tR40a1cxTb9uJMDc3V3/Zbj8/v7y8PBbjYUteXp7u+yAQCHx8fLj9faBpetWqVTNnzmRuDOTl5elWmGN+BmiOLhPx448/Dhs2rEOHDgMHDnzllVfIX0894fR/gXffffe1116rtURI3VPPRmhmZ29vn5SUtHHjxm7dui1dupRp1O+7q6urk5MTJ7v/4MGDBw8eDB8+fPv27aGhofHx8YSQ/Px8mUymq0LV/FNv24szabVa/T8PxWKxRqNhMR628O37sGTJkvLy8jVr1jBfarVa3a1ykUikX/OSYyIjIz08PG7evLl+/fqJEyf269ePJ6c+LS0tNTX14sWLtdprnXpO9p0QcuTIEabq0P3798PCwsaMGdO/f3/9vhPunnqFQpGTk5OVlWVvb5+amjp8+PCpU6fW6nvzT71tJ0JfX9+nT5/qviwqKvL19WUxHrb4+Pjcvn1b9yW3vw/vvffemTNnTp06pVte1cfHR/djUFRU5OPjw9XK3YGBgYGBgcOHDy8pKdmyZUu/fv18fHz0bwdw9dR/+umnrq6uc+fOJYQ8efJkx44dAoHglVdeqXXqOdl3oldstUOHDr1797527Vr//v31+65QKCoqKjjZfR8fH6lUytwP6tevn0ajefDggbe3d3V1dVVVFXOvhPlf35yj2PbQ6MCBA69fv15cXEwIyc3NvX//Pj+rdUdHR6empj5//pwQcvPmzaqqKiMLd9icjz766OjRoydPntSfLD548OATJ04wr0+ePBkdHc1OcBZUXFzMFJ0YNGjQpUuXKioqCCEPHjzIy8vjZC2OJUuWzJkzh5k26ezs3K1bt86dOxNCBg8efPLkSWYbPpz658+f37lzJyAggBASHR2dlJTEjH8kJia2b9/e+CoFNmTIkCH3799nXj98+FCj0fj5+fn5+XXs2DExMZEQQlFUUlLS4MGDm3MUm190+9VXX7179+6UKVN27drVr1+/rVu3sh2ReZ0+ffqnn376888/q6qqBg8ePHz4cOaO8ejRo6urq8eMGRMfHz9x4sSPP/6Y7UhN79dffx0/fvzYsWN1D0quX7/excXl4cOHvXr1io2NdXBw2Lx589mzZ0NCQtgN1Rz69+8fHR3t5uZ25cqV48ePp6SkdOvWjRASExOTn58/adKkHTt2DBs27LPPPmM7UvPq1avXokWL/vWvfxFCHjx4EB4e/sYbb0il0q1bt547d65r165sB2hiubm506dPHzBggEQi2b9/v5OT05kzZ5iB0N69e7dv375fv34bNmxYvXr1jBkz2A7W9DQaTXh4eLdu3fr27bt9+/b+/fszv+R37969fPnyJUuWXLx4MSsr68qVK80pSmXziVCr1e7bty8jIyM0NDQmJqa5j1VavVu3bp0/f173ZY8ePfr06UMIUalUu3btys7OjoiIGDduHHsBmlFmZmZKSop+y7Rp05gxk4cPH+7bt0+r1U6ePDk4OJilAM3rzJkzaWlpcrm8devWkyZNYmaNEkLUavXevXvv3LnTq1eviRMncnVYWOfgwYPdu3dn5ooTQrKzs7///nuKoiZPntypUyd2YzMHlUp16NAh5t5H165dx48fr1tLobKy8j//+c/Tp0+HDBnC4YKscrl8165dRUVFERERL730kq79jz/+SEpKatWq1fTp05tZldPmEyEAAEBzcPz6CQAAoGFIhAAAwGtIhAAAwGtIhAAAwGtIhAAAwGtIhAAAwGtIhADWqLq6mlkvplHOnz9/6NAhc8QDwGFIhABWJDs7+/XXX/fy8nJ0dHRzc3N1dR01ahRTi86Yj3/77bfLli0zd5AAHGPbi24DcElKSspLL71kb2//1ltvhYeHSySS7Ozsw4cPT5o06cKFCxEREX+7h/Hjx4eHh1sgVAAuwcoyAFahoqKiY8eOrq6uKSkpXl5e+m+dOXPGz89Pt6gYIUSj0ZSWlrZo0cLINQVramqUSqWbm5uJgwbgBAyNAlgFZtHIzz77rFYWJIRER0frsuCxY8ciIiLs7OyY4dORI0c+fvxYt+XixYsHDBjAvM7Ozvbw8Pj++++nTZvm4uLi7u7eqVOntLQ0y3QHwIYgEQJYhVOnTtnZ2Y0YMaLhzQoLCydOnHj27NnMzMzdu3dnZWWNHTtWN65TXl6uq1Gn1WrLysqWLFni6uqanJx87NgxrVY7ffp0DhcuBmga3CMEsAq5ubk+Pj52dna6lgcPHpSXlzOvvb29/fz8CCEzZ87UbRAcHOzu7j5s2LCMjIz6yg/17dv3iy++YF5/8sknMTExt2/f5mSZKoAmQyIEsAparbbWDb8lS5YcPnyYef3uu++uXbuWeZ2VlXXo0KH8/HylUslUY75//359iXDkyJG61126dCGE5OTkIBEC6MPQKIBV8Pb2Liws1B+33LZt24MHD65evaq/2dq1a7t27XrkyBG1Wu3u7s7Mf2ngiUN3d3fda6lUSghRKpWmjx7AluGKEMAqDBgwIDExMTU1ddCgQUyLt7c3IaSsrEy3jUajWbVq1YIFCz7//HOm5fr1619++aXlowXgElwRAliF2NhYBweH5cuXKxSK+rYpKChQKBS9evXStRw7dswi0QFwGRIhgFXw9fXdvn375cuXo6KifvrppwcPHuTn51+6dGndunWEEIFAwGzTsmXLr7766vHjx3K5fNeuXVu2bGE7cACbh6FRAGsxdepUPz+/999/f+rUqRRFMY1t2rRZs2bNwoULCSEikWj37t3Tpk0LDAxk3vrmm2/Gjh3LYswAHICVZQCsTllZ2aNHj9Rqtb+/v6+vb613a2pq7t69KxaLO3fuXGuiKU3TNE0budwMADCQCAEAgNfwlyMAAPAaEiEAAPAaEiEAAPAaEiEAAPAaEiEAAPAaEiEAAPDa/wOIlP5f9onByQAAAABJRU5ErkJggg==\" />"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_gain =  [(first(x),last(x)) for x in importance(model)]\n",
    "feature, gain = first.(feature_gain), last.(feature_gain)\n",
    "\n",
    "using Plots;\n",
    "\n",
    "p = bar(feature, y=gain, orientation=\"h\", legend=false)\n",
    "xlabel!(p,\"Gain\")\n",
    "ylabel!(p,\"Feature\")\n",
    "title!(\"Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d533db36",
   "metadata": {},
   "source": [
    "As you can see, not all features has the same importance. It should be notice that the Feature axis identifies the position in the Vector  feature which is ordered by the gain value by default.\n",
    "\n",
    "### Integration with MLJ\n",
    "\n",
    "As mentioned previously, **XGBoost** can be seamlessly integrated into the **MLJ framework**, allowing it to be used consistently with other models, pipelines, and tuning workflows.  \n",
    "This integration is provided through the [`MLJXGBoostInterface.jl`](https://github.com/JuliaAI/MLJXGBoostInterface.jl) package, which acts as a bridge between MLJ and the native XGBoost implementation.\n",
    "\n",
    "To load the model, simply use the `@load` macro from MLJ:\n",
    "\n",
    "```julia\n",
    "# Load data and ensure correct scientific types\n",
    "using DataFrames, MLJ\n",
    "\n",
    "# Load XGBoost classifier\n",
    "XGBoostClassifier = @load XGBoostClassifier pkg=XGBoost\n",
    "```\n",
    "Once loaded, the classifier can be configured, trained, and evaluated like any other MLJ model.\n",
    "\n",
    "```julia\n",
    "# Define model and parameters\n",
    "xgb_model = XGBoostClassifier(\n",
    "    num_round = 100,\n",
    "    eta = 0.1, \n",
    "    max_depth = 6,\n",
    "    objective = \"multi:softprob\"       # suitable for multi-class problems\n",
    ")\n",
    "\n",
    "# Bind model and data\n",
    "mach = machine(xgb_model, X, y)\n",
    "\n",
    "# Train the model\n",
    "fit!(mach, verbosity = 1)\n",
    "\n",
    "# Evaluate using cross-validation\n",
    "cv_result = evaluate!(\n",
    "    mach,\n",
    "    resampling = CV(nfolds = 5, shuffle = true),\n",
    "    measure = [accuracy, cross_entropy],\n",
    "    verbosity = 0\n",
    ")\n",
    "\n",
    "cv_result.measurement\n",
    "\n",
    "# ==============================\n",
    "# Example of Tuning of the model\n",
    "# ==============================\n",
    "\n",
    "# Define a parameter range to explore\n",
    "r_eta = range(xgb_model, :eta, lower=0.01, upper=0.3)\n",
    "r_depth = range(xgb_model, :max_depth, lower=3, upper=10)\n",
    "\n",
    "# Define tuning strategy\n",
    "tuned_xgb = TunedModel(\n",
    "    model = xgb_model,\n",
    "    resampling = CV(nfolds=5, shuffle=true),\n",
    "    range = [r_eta, r_depth],\n",
    "    measure = accuracy,\n",
    "    tuning = Grid(resolution=5),\n",
    "    operation = predict_mode\n",
    ")\n",
    "\n",
    "# Train tuned model\n",
    "mach_tuned = machine(tuned_xgb, X, y)\n",
    "fit!(mach_tuned)\n",
    "\n",
    "```\n",
    "\n",
    "## Julia Notes\n",
    "\n",
    "### Understanding `coerce` in MLJ\n",
    "\n",
    "When working with datasets in MLJ, it’s important to understand that MLJ distinguishes between **machine types** (e.g., `Int64`, `Float64`, `String`) and **scientific types** (or `scitypes`), which describe how data should be *interpreted* for modeling.\n",
    "\n",
    "### Why `coerce` is Needed\n",
    "\n",
    "MLJ models don’t rely on the raw Julia types — instead, they expect variables to have *scientific meanings*:\n",
    "- A numeric column can represent a **continuous** feature.\n",
    "- A string column can represent a **categorical** feature.\n",
    "- A boolean or integer can be **ordered** or **unordered**, depending on context.\n",
    "\n",
    "Since Julia doesn’t know this automatically, we use the `coerce()` function to explicitly tell MLJ how to interpret each column.  \n",
    "This ensures compatibility between your data and the MLJ model you plan to use.\n",
    "\n",
    "---\n",
    "\n",
    "### Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9470a0d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "ArgumentError: \"sonar.csv\" is not a valid file or doesn't exist",
     "output_type": "error",
     "traceback": [
      "ArgumentError: \"sonar.csv\" is not a valid file or doesn't exist",
      "",
      "Stacktrace:",
      " [1] \u001b[0m\u001b[1mCSV.Context\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90msource\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mheader\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mnormalizenames\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mdatarow\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mskipto\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mfooterskip\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mtranspose\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mcomment\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mignoreemptyrows\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mignoreemptylines\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mselect\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mdrop\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mlimit\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mbuffer_in_memory\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mthreaded\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mntasks\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mtasks\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mrows_to_check\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mlines_to_check\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mmissingstrings\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mmissingstring\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mdelim\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mignorerepeated\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mquoted\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mquotechar\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mopenquotechar\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mclosequotechar\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mescapechar\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mdateformat\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mdateformats\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mdecimal\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mgroupmark\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mtruestrings\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mfalsestrings\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mstripwhitespace\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mtype\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mtypes\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mtypemap\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mpool\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mdowncast\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mlazystrings\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mstringtype\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mstrict\u001b[39m::\u001b[0mCSV.Arg, \u001b[90msilencewarnings\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mmaxwarnings\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mdebug\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mparsingdebug\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mvalidate\u001b[39m::\u001b[0mCSV.Arg, \u001b[90mstreaming\u001b[39m::\u001b[0mCSV.Arg\u001b[0m\u001b[1m)\u001b[22m",
      "\u001b[90m   @\u001b[39m \u001b[35mCSV\u001b[39m \u001b[90m~/.julia/packages/CSV/XLcqT/src/\u001b[39m\u001b[90m\u001b[4mcontext.jl:314\u001b[24m\u001b[39m",
      " [2] \u001b[0m\u001b[1m#File#32\u001b[22m",
      "\u001b[90m   @\u001b[39m \u001b[90m~/.julia/packages/CSV/XLcqT/src/\u001b[39m\u001b[90m\u001b[4mfile.jl:222\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m",
      " [3] \u001b[0m\u001b[1mCSV.File\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90msource\u001b[39m::\u001b[0mString\u001b[0m\u001b[1m)\u001b[22m",
      "\u001b[90m   @\u001b[39m \u001b[35mCSV\u001b[39m \u001b[90m~/.julia/packages/CSV/XLcqT/src/\u001b[39m\u001b[90m\u001b[4mfile.jl:162\u001b[24m\u001b[39m",
      " [4] \u001b[0m\u001b[1mread\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90msource\u001b[39m::\u001b[0mString, \u001b[90msink\u001b[39m::\u001b[0mType; \u001b[90mcopycols\u001b[39m::\u001b[0mBool, \u001b[90mkwargs\u001b[39m::\u001b[0m@Kwargs\u001b[90m{}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m",
      "\u001b[90m   @\u001b[39m \u001b[35mCSV\u001b[39m \u001b[90m~/.julia/packages/CSV/XLcqT/src/\u001b[39m\u001b[90m\u001b[4mCSV.jl:117\u001b[24m\u001b[39m",
      " [5] \u001b[0m\u001b[1mread\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90msource\u001b[39m::\u001b[0mString, \u001b[90msink\u001b[39m::\u001b[0mType\u001b[0m\u001b[1m)\u001b[22m",
      "\u001b[90m   @\u001b[39m \u001b[35mCSV\u001b[39m \u001b[90m~/.julia/packages/CSV/XLcqT/src/\u001b[39m\u001b[90m\u001b[4mCSV.jl:113\u001b[24m\u001b[39m",
      " [6] top-level scope",
      "\u001b[90m   @\u001b[39m \u001b[90m\u001b[4mIn[35]:4\u001b[24m\u001b[39m"
     ]
    }
   ],
   "source": [
    "using MLJ, DataFrames, CSV\n",
    "\n",
    "# Load the dataset\n",
    "data = CSV.read(\"sonar.csv\", DataFrame)\n",
    "\n",
    "# Inspect current column types\n",
    "schema(data)\n",
    "\n",
    "# Suppose the last column is the target ('Rock' or 'Mine')\n",
    "y, X = unpack(data, ==(:Target), rng=123)\n",
    "\n",
    "# Convert the target variable to categorical\n",
    "y = coerce(y, Multiclass)\n",
    "\n",
    "# Convert all feature columns to continuous\n",
    "X = coerce(X, autotype(X, rules = (:discrete_to_continuous,)))\n",
    "\n",
    "# Verify the new scientific types\n",
    "schema(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e39a8f6",
   "metadata": {},
   "source": [
    "\n",
    "### Common examples\n",
    "\n",
    "| Situation                            | What to Do              | Example                              |\n",
    "| ------------------------------------ | ----------------------- | ------------------------------------ |\n",
    "| Categorical labels stored as strings | Convert to `Multiclass` | `y = coerce(y, Multiclass)`          |\n",
    "| Numerical features                   | Convert to `Continuous` | `X = coerce(X, :var1 => Continuous)` |\n",
    "| Automatic inference                  | Use `autotype()`        | `autotype(X)`                        |\n",
    "| Check current scientific types       | Use `schema()`          | `schema(X)`                          |\n",
    "\n",
    "### What Happens If You Skip coerce\n",
    "\n",
    "If you skip the coercion step:\n",
    "\n",
    "* MLJ might misinterpret your target as continuous, blocking classification models.\n",
    "* Some algorithms (e.g., DecisionTree, NaiveBayes) may fail or produce wrong predictions.\n",
    "* The machine() function might refuse to bind the model and data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b376d4",
   "metadata": {},
   "source": [
    "## Voting Classifier\n",
    "\n",
    "Following this lines there are several examples of how to use the new Voting Classifier that we have implemented.\n",
    "\n",
    "In previous examples, we created the ensemble as follows:\n",
    "\n",
    "```julia\n",
    "voting_hard = VotingClassifier(models=base_models_list, voting=:hard)\n",
    "voting_soft = VotingClassifier(models=base_models_list, voting=:soft)\n",
    "\n",
    "mach_hard = machine(voting_hard, train_input, train_output) |> fit!\n",
    "mach_soft = machine(voting_soft, train_input, train_output) |> fit!\n",
    "```\n",
    "\n",
    "* `voting=:hard` → uses majority voting (based on class labels).\n",
    "\n",
    "* `voting=:soft` → uses average probability voting (requires models that output probabilities).\n",
    "\n",
    "The following examples show how to modify and integrate the VotingClassifier dynamically within your workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f45bc191",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `base_models_list` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `base_models_list` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "\u001b[90m   @\u001b[39m \u001b[90m\u001b[4mIn[36]:2\u001b[24m\u001b[39m"
     ]
    }
   ],
   "source": [
    "# Example 1: Changing the Voting Type Dynamically\n",
    "ensemble = VotingClassifier(models=base_models_list, voting=:hard)\n",
    "println(\"\\nCurrent voting type: $(ensemble.voting)\")\n",
    "\n",
    "ensemble.voting = :soft\n",
    "println(\"Voting type changed to: $(ensemble.voting)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39a28c86",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: `@pipeline` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.\nin expression starting at In[37]:2",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: `@pipeline` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.\nin expression starting at In[37]:2",
      ""
     ]
    }
   ],
   "source": [
    "# Example 2: Using the VotingClassifier in a Pipeline\n",
    "pipe_hard = @pipeline(\n",
    "    Standardizer(),\n",
    "    VotingClassifier(models=base_models_list, voting=:hard)\n",
    ")\n",
    "\n",
    "pipe_soft = @pipeline(\n",
    "    Standardizer(),\n",
    "    VotingClassifier(models=base_models_list, voting=:soft)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ade52dec",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `voting_hard` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `voting_hard` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "\u001b[90m   @\u001b[39m \u001b[90m\u001b[4mIn[38]:2\u001b[24m\u001b[39m"
     ]
    }
   ],
   "source": [
    "# Example 3: Cross-Validation Comparing Both Voting Strategies\n",
    "cv_hard = evaluate!(\n",
    "    machine(voting_hard, train_input, train_output),\n",
    "    resampling=CV(nfolds=5),\n",
    "    measure=accuracy\n",
    ")\n",
    "println(\"Hard Voting CV accuracy: $(round(mean(cv_hard.measurement)*100, digits=2)) %\")\n",
    "\n",
    "cv_soft = evaluate!(\n",
    "    machine(voting_soft, train_input, train_output),\n",
    "    resampling=CV(nfolds=5),\n",
    "    measure=accuracy\n",
    ")\n",
    "println(\"Soft Voting CV accuracy: $(round(mean(cv_soft.measurement)*100, digits=2)) %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
