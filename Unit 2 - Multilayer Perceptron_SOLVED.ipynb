{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f0c2c02",
   "metadata": {},
   "source": [
    "# Training Multilayer Perceptrons\n",
    "\n",
    "The aim of this tutorial is to learn how to train multilayer perceptrons in Julia. To do so, we will make use of the `Flux` library, whose documentation can be consulted at https://fluxml.ai/Flux.jl/.\n",
    "\n",
    "If not previously, the packege has to be installed on the system by executing the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a530ceb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `C:\\Users\\Brais\\.julia\\registries\\General.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m RealDot ────────────── v0.1.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Adapt ──────────────── v3.6.2\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m GPUArraysCore ──────── v0.1.5\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DiffRules ──────────── v1.15.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m IrrationalConstants ── v0.2.2\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m IRTools ────────────── v0.4.10\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Functors ───────────── v0.4.5\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ShowCases ──────────── v0.1.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ArgCheck ───────────── v2.3.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ContextVariablesX ──── v0.1.3\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m FLoopsBase ─────────── v0.1.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Transducers ────────── v0.4.78\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DiffResults ────────── v1.1.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ProgressLogging ────── v0.1.4\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m NNlib ──────────────── v0.9.7\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m SpecialFunctions ───── v2.3.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Flux ───────────────── v0.14.6\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Setfield ───────────── v1.1.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Baselet ────────────── v0.1.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Zygote ─────────────── v0.6.65\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m StaticArrays ───────── v1.6.5\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m AbstractFFTs ───────── v1.5.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Optimisers ─────────── v0.3.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m NaNMath ────────────── v1.0.2\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m StaticArraysCore ───── v1.4.2\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m OneHotArrays ───────── v0.2.4\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ConstructionBase ───── v1.5.4\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m MicroCollections ───── v0.1.4\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DefineSingletons ───── v0.1.2\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m LLVMExtra_jll ──────── v0.0.26+0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m UnsafeAtomicsLLVM ──── v0.1.3\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m StructArrays ───────── v0.6.16\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m NameResolution ─────── v0.1.5\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m CEnum ──────────────── v0.4.2\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ChainRulesCore ─────── v1.16.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m InitialValues ──────── v0.3.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DelimitedFiles ─────── v1.9.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Reexport ───────────── v1.2.2\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m FillArrays ─────────── v1.6.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m GPUArrays ──────────── v9.0.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ForwardDiff ────────── v0.10.36\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m SplittablesBase ────── v0.1.15\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DataStructures ─────── v0.18.15\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m LogExpFunctions ────── v0.3.26\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ChainRules ─────────── v1.55.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Requires ───────────── v1.3.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ZygoteRules ────────── v0.2.3\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m CommonSubexpressions ─ v0.3.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m MacroTools ─────────── v0.5.11\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m KernelAbstractions ─── v0.9.8\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m PrettyPrint ────────── v0.2.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m BangBang ───────────── v0.3.39\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m StatsAPI ───────────── v1.7.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m MLStyle ────────────── v0.4.17\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m CompositionsBase ───── v0.1.2\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m OpenSpecFun_jll ────── v0.5.5+0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m SparseInverseSubset ── v0.1.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Compat ─────────────── v4.10.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m UnsafeAtomics ──────── v0.2.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m JuliaVariables ─────── v0.2.4\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m MLUtils ────────────── v0.4.3\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m SortingAlgorithms ──── v1.1.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m FLoops ─────────────── v0.2.1\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Missings ───────────── v1.1.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m SimpleTraits ───────── v0.9.4\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Atomix ─────────────── v0.1.0\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m StatsBase ──────────── v0.34.2\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DocStringExtensions ── v0.9.3\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m LLVM ───────────────── v6.3.0\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `C:\\Users\\Brais\\.julia\\environments\\v1.9\\Project.toml`\n",
      "  \u001b[90m[587475ba] \u001b[39m\u001b[92m+ Flux v0.14.6\u001b[39m\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `C:\\Users\\Brais\\.julia\\environments\\v1.9\\Manifest.toml`\n",
      "  \u001b[90m[621f4979] \u001b[39m\u001b[92m+ AbstractFFTs v1.5.0\u001b[39m\n",
      "  \u001b[90m[79e6a3ab] \u001b[39m\u001b[92m+ Adapt v3.6.2\u001b[39m\n",
      "  \u001b[90m[dce04be8] \u001b[39m\u001b[92m+ ArgCheck v2.3.0\u001b[39m\n",
      "  \u001b[90m[a9b6321e] \u001b[39m\u001b[92m+ Atomix v0.1.0\u001b[39m\n",
      "  \u001b[90m[198e06fe] \u001b[39m\u001b[92m+ BangBang v0.3.39\u001b[39m\n",
      "  \u001b[90m[9718e550] \u001b[39m\u001b[92m+ Baselet v0.1.1\u001b[39m\n",
      "  \u001b[90m[fa961155] \u001b[39m\u001b[92m+ CEnum v0.4.2\u001b[39m\n",
      "  \u001b[90m[082447d4] \u001b[39m\u001b[92m+ ChainRules v1.55.0\u001b[39m\n",
      "  \u001b[90m[d360d2e6] \u001b[39m\u001b[92m+ ChainRulesCore v1.16.0\u001b[39m\n",
      "  \u001b[90m[bbf7d656] \u001b[39m\u001b[92m+ CommonSubexpressions v0.3.0\u001b[39m\n",
      "  \u001b[90m[34da2185] \u001b[39m\u001b[92m+ Compat v4.10.0\u001b[39m\n",
      "  \u001b[90m[a33af91c] \u001b[39m\u001b[92m+ CompositionsBase v0.1.2\u001b[39m\n",
      "  \u001b[90m[187b0558] \u001b[39m\u001b[92m+ ConstructionBase v1.5.4\u001b[39m\n",
      "  \u001b[90m[6add18c4] \u001b[39m\u001b[92m+ ContextVariablesX v0.1.3\u001b[39m\n",
      "  \u001b[90m[864edb3b] \u001b[39m\u001b[92m+ DataStructures v0.18.15\u001b[39m\n",
      "  \u001b[90m[244e2a9f] \u001b[39m\u001b[92m+ DefineSingletons v0.1.2\u001b[39m\n",
      "  \u001b[90m[8bb1440f] \u001b[39m\u001b[92m+ DelimitedFiles v1.9.1\u001b[39m\n",
      "  \u001b[90m[163ba53b] \u001b[39m\u001b[92m+ DiffResults v1.1.0\u001b[39m\n",
      "  \u001b[90m[b552c78f] \u001b[39m\u001b[92m+ DiffRules v1.15.1\u001b[39m\n",
      "  \u001b[90m[ffbed154] \u001b[39m\u001b[92m+ DocStringExtensions v0.9.3\u001b[39m\n",
      "  \u001b[90m[cc61a311] \u001b[39m\u001b[92m+ FLoops v0.2.1\u001b[39m\n",
      "  \u001b[90m[b9860ae5] \u001b[39m\u001b[92m+ FLoopsBase v0.1.1\u001b[39m\n",
      "  \u001b[90m[1a297f60] \u001b[39m\u001b[92m+ FillArrays v1.6.1\u001b[39m\n",
      "  \u001b[90m[587475ba] \u001b[39m\u001b[92m+ Flux v0.14.6\u001b[39m\n",
      "  \u001b[90m[f6369f11] \u001b[39m\u001b[92m+ ForwardDiff v0.10.36\u001b[39m\n",
      "  \u001b[90m[d9f16b24] \u001b[39m\u001b[92m+ Functors v0.4.5\u001b[39m\n",
      "  \u001b[90m[0c68f7d7] \u001b[39m\u001b[92m+ GPUArrays v9.0.0\u001b[39m\n",
      "  \u001b[90m[46192b85] \u001b[39m\u001b[92m+ GPUArraysCore v0.1.5\u001b[39m\n",
      "  \u001b[90m[7869d1d1] \u001b[39m\u001b[92m+ IRTools v0.4.10\u001b[39m\n",
      "  \u001b[90m[22cec73e] \u001b[39m\u001b[92m+ InitialValues v0.3.1\u001b[39m\n",
      "  \u001b[90m[92d709cd] \u001b[39m\u001b[92m+ IrrationalConstants v0.2.2\u001b[39m\n",
      "  \u001b[90m[b14d175d] \u001b[39m\u001b[92m+ JuliaVariables v0.2.4\u001b[39m\n",
      "  \u001b[90m[63c18a36] \u001b[39m\u001b[92m+ KernelAbstractions v0.9.8\u001b[39m\n",
      "  \u001b[90m[929cbde3] \u001b[39m\u001b[92m+ LLVM v6.3.0\u001b[39m\n",
      "  \u001b[90m[2ab3a3ac] \u001b[39m\u001b[92m+ LogExpFunctions v0.3.26\u001b[39m\n",
      "  \u001b[90m[d8e11817] \u001b[39m\u001b[92m+ MLStyle v0.4.17\u001b[39m\n",
      "  \u001b[90m[f1d291b0] \u001b[39m\u001b[92m+ MLUtils v0.4.3\u001b[39m\n",
      "  \u001b[90m[1914dd2f] \u001b[39m\u001b[92m+ MacroTools v0.5.11\u001b[39m\n",
      "  \u001b[90m[128add7d] \u001b[39m\u001b[92m+ MicroCollections v0.1.4\u001b[39m\n",
      "  \u001b[90m[e1d29d7a] \u001b[39m\u001b[92m+ Missings v1.1.0\u001b[39m\n",
      "  \u001b[90m[872c559c] \u001b[39m\u001b[92m+ NNlib v0.9.7\u001b[39m\n",
      "  \u001b[90m[77ba4419] \u001b[39m\u001b[92m+ NaNMath v1.0.2\u001b[39m\n",
      "  \u001b[90m[71a1bf82] \u001b[39m\u001b[92m+ NameResolution v0.1.5\u001b[39m\n",
      "  \u001b[90m[0b1bfda6] \u001b[39m\u001b[92m+ OneHotArrays v0.2.4\u001b[39m\n",
      "  \u001b[90m[3bd65402] \u001b[39m\u001b[92m+ Optimisers v0.3.1\u001b[39m\n",
      "  \u001b[90m[8162dcfd] \u001b[39m\u001b[92m+ PrettyPrint v0.2.0\u001b[39m\n",
      "  \u001b[90m[33c8b6b6] \u001b[39m\u001b[92m+ ProgressLogging v0.1.4\u001b[39m\n",
      "  \u001b[90m[c1ae055f] \u001b[39m\u001b[92m+ RealDot v0.1.0\u001b[39m\n",
      "  \u001b[90m[189a3867] \u001b[39m\u001b[92m+ Reexport v1.2.2\u001b[39m\n",
      "  \u001b[90m[ae029012] \u001b[39m\u001b[92m+ Requires v1.3.0\u001b[39m\n",
      "  \u001b[90m[efcf1570] \u001b[39m\u001b[92m+ Setfield v1.1.1\u001b[39m\n",
      "  \u001b[90m[605ecd9f] \u001b[39m\u001b[92m+ ShowCases v0.1.0\u001b[39m\n",
      "  \u001b[90m[699a6c99] \u001b[39m\u001b[92m+ SimpleTraits v0.9.4\u001b[39m\n",
      "  \u001b[90m[a2af1166] \u001b[39m\u001b[92m+ SortingAlgorithms v1.1.1\u001b[39m\n",
      "  \u001b[90m[dc90abb0] \u001b[39m\u001b[92m+ SparseInverseSubset v0.1.1\u001b[39m\n",
      "  \u001b[90m[276daf66] \u001b[39m\u001b[92m+ SpecialFunctions v2.3.1\u001b[39m\n",
      "  \u001b[90m[171d559e] \u001b[39m\u001b[92m+ SplittablesBase v0.1.15\u001b[39m\n",
      "  \u001b[90m[90137ffa] \u001b[39m\u001b[92m+ StaticArrays v1.6.5\u001b[39m\n",
      "  \u001b[90m[1e83bf80] \u001b[39m\u001b[92m+ StaticArraysCore v1.4.2\u001b[39m\n",
      "  \u001b[90m[82ae8749] \u001b[39m\u001b[92m+ StatsAPI v1.7.0\u001b[39m\n",
      "  \u001b[90m[2913bbd2] \u001b[39m\u001b[92m+ StatsBase v0.34.2\u001b[39m\n",
      "  \u001b[90m[09ab397b] \u001b[39m\u001b[92m+ StructArrays v0.6.16\u001b[39m\n",
      "  \u001b[90m[28d57a85] \u001b[39m\u001b[92m+ Transducers v0.4.78\u001b[39m\n",
      "  \u001b[90m[013be700] \u001b[39m\u001b[92m+ UnsafeAtomics v0.2.1\u001b[39m\n",
      "  \u001b[90m[d80eeb9a] \u001b[39m\u001b[92m+ UnsafeAtomicsLLVM v0.1.3\u001b[39m\n",
      "  \u001b[90m[e88e6eb3] \u001b[39m\u001b[92m+ Zygote v0.6.65\u001b[39m\n",
      "  \u001b[90m[700de1a5] \u001b[39m\u001b[92m+ ZygoteRules v0.2.3\u001b[39m\n",
      "  \u001b[90m[dad2f222] \u001b[39m\u001b[92m+ LLVMExtra_jll v0.0.26+0\u001b[39m\n",
      "  \u001b[90m[efe28fd5] \u001b[39m\u001b[92m+ OpenSpecFun_jll v0.5.5+0\u001b[39m\n",
      "  \u001b[90m[8ba89e20] \u001b[39m\u001b[92m+ Distributed\u001b[39m\n",
      "  \u001b[90m[9fa8497b] \u001b[39m\u001b[92m+ Future\u001b[39m\n",
      "  \u001b[90m[4af54fe1] \u001b[39m\u001b[92m+ LazyArtifacts\u001b[39m\n",
      "  \u001b[90m[2f01184e] \u001b[39m\u001b[92m+ SparseArrays\u001b[39m\n",
      "  \u001b[90m[10745b16] \u001b[39m\u001b[92m+ Statistics v1.9.0\u001b[39m\n",
      "  \u001b[90m[4607b0f0] \u001b[39m\u001b[92m+ SuiteSparse\u001b[39m\n",
      "  \u001b[90m[05823500] \u001b[39m\u001b[92m+ OpenLibm_jll v0.8.1+0\u001b[39m\n",
      "  \u001b[90m[bea87d4a] \u001b[39m\u001b[92m+ SuiteSparse_jll v5.10.1+6\u001b[39m\n",
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDefineSingletons\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mRealDot\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDelimitedFiles\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mReexport\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mCompat\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mAbstractFFTs\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDocStringExtensions\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mRequires\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mPrettyPrint\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mFillArrays\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStatsAPI\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mArgCheck\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mShowCases\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mProgressLogging\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mCompositionsBase\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mConstructionBase\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mIrrationalConstants\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mCEnum\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStaticArraysCore\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mSparseInverseSubset\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mNaNMath\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mInitialValues\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMissings\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mUnsafeAtomics\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mCompat → CompatLinearAlgebraExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mFunctors\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mAdapt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mNameResolution\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mLLVMExtra_jll\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mOpenSpecFun_jll\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mFillArrays → FillArraysSparseArraysExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDiffResults\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mBaselet\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mContextVariablesX\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mLogExpFunctions\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mAtomix\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mAbstractFFTs → AbstractFFTsTestExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mFillArrays → FillArraysStatisticsExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mFLoopsBase\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMacroTools\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mChainRulesCore\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mCommonSubexpressions\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDataStructures\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mAbstractFFTs → AbstractFFTsChainRulesCoreExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mSimpleTraits\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mSortingAlgorithms\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mZygoteRules\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mOptimisers\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mLogExpFunctions → LogExpFunctionsChainRulesCoreExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mIRTools\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStatsBase\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mLLVM\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mSpecialFunctions\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStaticArrays\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mUnsafeAtomicsLLVM\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStaticArrays → StaticArraysStatisticsExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mConstructionBase → ConstructionBaseStaticArraysExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mAdapt → AdaptStaticArraysExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mSpecialFunctions → SpecialFunctionsChainRulesCoreExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mGPUArraysCore\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDiffRules\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mSetfield\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStructArrays\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mBangBang\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mGPUArrays\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mSplittablesBase\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mBangBang → BangBangChainRulesCoreExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mBangBang → BangBangStructArraysExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mKernelAbstractions\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mBangBang → BangBangStaticArraysExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMicroCollections\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mForwardDiff\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mForwardDiff → ForwardDiffStaticArraysExt\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mChainRules\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mTransducers\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mNNlib\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mOneHotArrays\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLStyle\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mJuliaVariables\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mFLoops\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLUtils\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mZygote\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39mFlux\n",
      "  83 dependencies successfully precompiled in 77 seconds. 32 already precompiled.\n"
     ]
    }
   ],
   "source": [
    "using Pkg; Pkg.add(\"Flux\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe81995",
   "metadata": {},
   "source": [
    "Flux is a library which provides a set of functions to create neural networks with an arbitrary number of layers. This is a library designed to develop Deep Learning projects, whose ANNs usually have a large number of layers of different types, for example, convolutional or maxpooling layers.  In these exercises, only multilayer perceptrons will be developed, with fully-connected (dense) layers, with a maximum of two hidden layers. \n",
    "\n",
    "In order to implement an ANN in Julia, there is a function called `Chain`. This function receives as parameters the layers that the network will have (excluding the input layer, which does not perform any processing), which can be of different types.This function receives as parameters the layers that the network will have (excluding the input layer, which does not perform any processing), which can be of different types. Therefore, it is a function with a variable number of parameters.  Depending on the type of layer desired, there are different functions to create each one of them. A couple of examples are the functions `Conv`, which allows for the creation of convolutional layers, or `MaxPool`, which allows for the creation of MaxPooling layers. These layers are used in more advanced models that will be seen in other subjects of the programme and that are beyond the syllabus of this subject.  In this subject, as only multilayer perceptrons will be covered, the layers will always be created fully connected with the function `Dense`. This function accepts as parameters the number of inputs, outputs, and the transfer function of the neurons in the layer. In this sense, two different cases can be distinguished when creating ANNs, depending on the problem to be solved: \n",
    "\n",
    "- ***Regression problems***. In this type of problem, the output layer usually has a linear transfer function, while the hidden layers have a non-linear transfer function.  In the following examples, a sigmoidal transfer function is used as the transfer function in the hidden layers; for further information on other supported transfer functions, please refer to the library documentation.  \n",
    "\n",
    "  In the first example, an ANN is implemented with 10 inputs, a hidden layer with 5 neurons, and an output layer of 1 neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "013ea6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(10 => 5, σ),                    \u001b[90m# 55 parameters\u001b[39m\n",
       "  Dense(5 => 1),                        \u001b[90m# 6 parameters\u001b[39m\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m61 parameters, 500 bytes."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux;\n",
    "\n",
    "ann = Chain(    \n",
    "    Dense(10, 5, σ),    \n",
    "    Dense(5, 1, identity) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466c7de9",
   "metadata": {},
   "source": [
    "* The second example builds an ANN with 15 inputs, two hidden layers with 12 and 5 neurons, and an output layer with 2 neurons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66502f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(15 => 12, σ),                   \u001b[90m# 192 parameters\u001b[39m\n",
       "  Dense(12 => 5, σ),                    \u001b[90m# 65 parameters\u001b[39m\n",
       "  Dense(5 => 2),                        \u001b[90m# 12 parameters\u001b[39m\n",
       ") \u001b[90m                  # Total: 6 arrays, \u001b[39m269 parameters, 1.426 KiB."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann = Chain(    \n",
    "    Dense(15, 12, σ),    \n",
    "    Dense(12, 5, σ),    \n",
    "    Dense(5, 2, identity) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6a9bb0",
   "metadata": {},
   "source": [
    "**Warning:** Be aware that the number of neurons between the different layers has to match\n",
    "\n",
    "### Question\n",
    "What would happen if all the layers of the ANN have a linear transfer function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d892b3",
   "metadata": {},
   "source": [
    "***ANSWER***\n",
    "\n",
    "If all the layers of an artificial neural network (ANN) have a linear transfer function, the network would essentially become a linear model, regardless of its depth or the number of layers. This means that the network would not be able to capture complex non-linear patterns in the data, and its expressiveness and modeling capabilities would be severely limited. \n",
    "\n",
    "This is the mathematical proof: Being a neuron the aplkication of  $ f_1(x) = m_1 x + b_1 $, and similarly, for a second on the second layer $ f_2(x) = m_2 x + b_2 $, then if the result from the first neuron is transfered to the second one without a non-lineal conversion:\n",
    "\n",
    "$f_2(f_1(x)) = m_2 (m_1 x + b_1) + b_2 = m_2m_1x + m_2b_1 + b_2 = m_3x + b_3$\n",
    "\n",
    "the result is another expression that multiplies the input by a weight $m_3$ and adds a bias $b_3$, as a simple neuron does.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976afc9b",
   "metadata": {},
   "source": [
    "- ***Classification problems***.  In this case, as explained in theory class, two different situations are considered, depending on whether there are two classes or more than two classes, whereas not belonging to any class is treated as another class:\n",
    "1. When there are only two classes, this is usually referred to as positives and negatives. In this case, the desired outputs will be either 1 or 0, and a single hidden neuron is, therefore, present.  Thus, such neuron is desired to return values between 0 and 1, which will be interpreted as the degree of certainty the system has that the output is positive.To ensure it returns a bounded value between 0 and 1, a sigmoidal function is applied to the output layer.In the following example, an ANN is defined with a hidden layer and an output neuron; in this example, the sigmoid function has also been used in the hidden layer, but this could be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f813d6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(8 => 4, σ),                     \u001b[90m# 36 parameters\u001b[39m\n",
       "  Dense(4 => 1, σ),                     \u001b[90m# 5 parameters\u001b[39m\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m41 parameters, 420 bytes."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann = Chain(    \n",
    "    Dense(8, 4, σ),    \n",
    "    Dense(4, 1, σ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5660d303",
   "metadata": {},
   "source": [
    "2. With more than two classes, you have one output neuron per class. The desired output of a pattern is 1 for the neuron of the class it belongs to, and 0 for the rest. This kind of encoding is known as one-hot-encoding.  In this way, the output of a neuron for a pattern can be interpreted as the degree of certainty that the pattern belongs to the class corresponding to that neuron.  Unlike the previous case, a sigmoidal transfer function is not applied to the outputs of each neuron in the output layer to bound the output between 0 and 1, but no function (identity function) is applied.  Instead, a `softmax` function is applied to the outputs of all the neurons, which takes unbounded numeric values and returns numeric values between 0 and 1 such that the sum of all values is 1.  Even though this function does not constitute a layer of neurons, sometimes this last `softmax` function is considered as an additional layer, and in fact, from the point of view of programming with the `Flux` library, it is effectively performed as if it were a last layer, as can be seen in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f162550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(4 => 5, σ),                     \u001b[90m# 25 parameters\u001b[39m\n",
       "  Dense(5 => 3),                        \u001b[90m# 18 parameters\u001b[39m\n",
       "  NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m43 parameters, 428 bytes."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann = Chain(    \n",
    "    Dense(4, 5, σ),    \n",
    "    Dense(5, 3, identity),    \n",
    "    softmax )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fe51cb",
   "metadata": {},
   "source": [
    "Another possibility to use the `Chain` function is by successive calls, adding layers to an already created network.  To do this, the ellipsis operator is used when specifying arguments to a function. In the following example, an equivalent ANN is created by first creating an empty ANN and successively adding layers to it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1d83bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(4 => 5, σ),                     \u001b[90m# 25 parameters\u001b[39m\n",
       "  Dense(5 => 3),                        \u001b[90m# 18 parameters\u001b[39m\n",
       "  NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m43 parameters, 428 bytes."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann = Chain(); \n",
    "ann = Chain(ann...,  Dense(4, 5, σ) );\n",
    "ann = Chain(ann...,  Dense(5, 3, identity) ); \n",
    "ann = Chain(ann...,  softmax )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c810a352",
   "metadata": {},
   "source": [
    "This variable, obtained in either form, can be used as a function.  For example, the matrix inputs created in the previous tutorial can be taken and passed to the ANN resulting in the outputs of the network by simply writing the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c627c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×150 Matrix{Float32}:\n",
       " 0.841465   0.837456   0.840843   …  0.806846   0.809847  0.811619\n",
       " 0.118501   0.121424   0.118535      0.148933   0.146391  0.144728\n",
       " 0.0400349  0.0411195  0.0406225     0.0442215  0.043762  0.0436533"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "using DelimitedFiles;\n",
    "dataset = readdlm(\"iris.data\",',');\n",
    "\n",
    "# Prepare the data\n",
    "inputs = convert(Array{Float32,2}, dataset[:,1:4]);\n",
    "\n",
    "targets = dataset[:,5];\n",
    "classes = unique(targets);\n",
    "numClasses = length(classes);\n",
    "oneHot = Array{Bool,2}(undef, length(targets), numClasses);\n",
    "for numClass = 1:numClasses\n",
    "    oneHot[:,numClass] .= (targets.==classes[numClass]);\n",
    "end;\n",
    "targets = oneHot;\n",
    "\n",
    "outputs = ann(inputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d85b7c",
   "metadata": {},
   "source": [
    "***Warning***: due to the formulation used in the world of ANNs, the input and output matrices for the ANN have each pattern in each column, not in each row. Therefore, each row of the input matrix will represent one of the input attributes, while in the output matrix, each row will correspond to one of the desired outputs of the ANN. Consequently, a transpose of the matrices created in the previous practice is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1758a307",
   "metadata": {},
   "source": [
    "Thus, even if the ANN is not properly trained, it may be interesting to make a call similar to this one to verify that the ANN has been created correctly. Once it has been verified that the ANN has been created correctly, it is time to train it. \n",
    "\n",
    "To train an ANN, following the workflow described in the theory class, the patterns are presented to the network, then the output is compared with the desired output, and finally a loss value is calculated.  This loss value will be used to modify the weights of the connections and bias. Therefore, a key point is to define this loss function, which will be different for regression and classification problems. The Flux library includes the Losses module with a large number of loss functions used to train NRs. In this subject only the most common ones will be used and, therefore, the first step is to load this module with: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee788074",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux.Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c53217a",
   "metadata": {},
   "source": [
    "For any kind of problem, the usage of the `loss` functions is the same. The first argument is the outputs of the model, the second argument is the desired outputs (in both cases with a pattern in each column), and the third argument is the optional keyword `agg`, which indicates how to aggregate the loss values for each pattern. If no value is specified for this keyword, by default an average of all loss values for all patterns will be performed.\n",
    "\n",
    "* For a regression problem, the most commonly loss function is the **Mean Square Error** (MSE) between the model outputs and the desired targets.  This MSE function is already defined in the Losses module of Flux, although it is very simple to define. It can be used as follows, where `x` are the model inputs and `y` are the targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "275883f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(m, x, y) = Losses.mse(m(x), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fa6eac",
   "metadata": {},
   "source": [
    "  Other loss functions that may be of interest for regression problems are `Flux.Loss.mae` or `Flux.Loss.msle`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e93cb7d",
   "metadata": {},
   "source": [
    "* For a classification problem the loss function is different. As it was appointed during theory sessions, the binary cross-entropy function is the common choice for a  2-classes problem (only one output neuron)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "def3bcb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(m, x, y) = Losses.binarycrossentropy(m(x), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7266f73",
   "metadata": {},
   "source": [
    "  Whereas, the cross-entropy function is main used for problems with more than 2  classes (one output neuron per class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9c1d20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(m,x, y) = Losses.crossentropy(m,(x), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df78f212",
   "metadata": {},
   "source": [
    "Although, each of these functions could be declared in the various branches of an if statement, Julia has problems when making such function declarations. For this reason, these two declarations can be merged into one, with the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c79bcd17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(m, x, y) = (size(y,1) == 1) ? Losses.binarycrossentropy(m(x),y) : Losses.crossentropy(m(x),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1333f6f",
   "metadata": {},
   "source": [
    "### Question\n",
    "As it can be seen, first of all the number of rows of the desired output matrix (`y`) is checked, why is the number of rows checked and not the number of columns?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95b6608",
   "metadata": {},
   "source": [
    "***ANSWER***\n",
    "\n",
    "According to the convention used in Julia, input samples are store in columns so each column represent a sample of the data set. However, for the output when we have more than two classes such as this one, each partiticular sample process outputs as much outputs as clases in columns and the result for each sample is store in a row.\n",
    "Therefore, we need to chech if the number of input samples aligns to the number of outputs of our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6dd73c",
   "metadata": {},
   "source": [
    "It is important to remember that in both `x` (inputs) and `y` (targets) each pattern must be in a column, contrary to the usual practice. For this reason, as will be shown below, the matrices of inputs and desired outputs will be transposed.\n",
    "\n",
    "These functions use the variable `ann`, which is used as a function as it was previously described.  Therefore, it needs to be defined within the environment in which the function is defined.\n",
    "\n",
    "Once the loss function has been defined, it is necessary to indicate the optimizer to be used during training. The optimizer is nothing more than a specific implementation of one of the alternative backpropagation algorithms. Flux has a large number of those implementations, from the classical one based on gradient descent (`Descent`) or adding also the momentum (`Momentum`) to more advanced ones: `Adam`, `RAdam`, `AdaMax`, `AdaGrad`, `AdaDelta`, `AMSGrad`, `RMSProp`, etc.  Possibly, nowadays, the most widely used optimizer is ADAM, to which you have to indicate the learning rate. This value is usualy a small amount, however, you can find more information in the documentation of the library. Therefore, we have to setup the optimizer we want to use, for example a definitiion of an `Adam` would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c08e278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(layers = ((weight = \u001b[32mLeaf(Adam(0.1, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; … ; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0], Float32[0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0; … ; 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(0.1, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), (weight = \u001b[32mLeaf(Adam(0.1, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(0.1, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), ()),)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learningRate = 0.1;\n",
    "opt_state = Flux.setup(Adam(learningRate), ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90b2fa9",
   "metadata": {},
   "source": [
    "For `learningRate`parameter, a value is defined beforehand in the range $(0.01, 0.1)$ which would control the updates of the mdoel. A common value is 0.01, although you should try different values until you find one that gives good results for the specific problem. Additionaly, the model is required in this case represented by `ann`.  \n",
    "\n",
    "Thus, once the optimizer is defined, training **an epoch** of the previous ANN can be done with the `train!` function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce614f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.train!(loss, ann, [(inputs', targets')], opt_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bdc91f",
   "metadata": {},
   "source": [
    "**Important**. It is worth mentioning that in Julia, by convention, when a function is defined by adding `!` (bang) as the last symbol, it is understood taht it modifies the contents of one or more of its arguments, which has therefore been passed by reference.\n",
    "\n",
    "That is the case of the function `train!`, which has four arguments:\n",
    "\n",
    "1. The `loss` function, which has been previously defined\n",
    "2. The `ann` parameter contains the model to be trained. \n",
    "3. A set of patterns, inputs and targets. As you can see in the example, an array with only one element is being passed. This element is a tuple with two elements: the arrays of inputs and desired outputs.  This way of passing the patterns, which may seem cumbersome, has its motivation, since when the set of patterns is very large, calculating the modifications to the weights with all the patterns can be very costly.  For this reason, the patterns are usually divided into batches so that each update is done with only one of these batches. If this were done, the array passed as a parameter, instead of having one tuple, would have several, one per batch.  However, in the exercises to be carried out in this subject, this will not be done, and all the patterns will be passed together. \n",
    "  * **Important**: As indicated above, these matrices of inputs and targets have each pattern in a column, contrary to what is usual.  For this reason, the input and target matrices passed as parameters are transposed, i.e. instead of passing `inputs` and `targets`, `inputs'` and `targets'` are passed. If the input and/or target matrices already had a pattern in each column, there would be no need to transpose the corresponding matrix.\n",
    "  * **Important**: The matrices given in this parameter are used for training the ANN. Therefore, they have to be completely different from those used for testing.\n",
    "4. Optimizer.  Defined previously"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10f9c4f",
   "metadata": {},
   "source": [
    "In this way only one loop is trained.  Therefore, to train an ANN it is necessary to create a loop that executes this function as long as some stop criterion is not met. Some of the most common criteria can be:\n",
    "\n",
    "* The loss in training is good enough.\n",
    "* The number of training cycles has reached a predefined maximum. \n",
    "* The change in training error is less than a predefined value. \n",
    "* etc.\n",
    "\n",
    "#### Question\n",
    "Could a similar stopping criterion to first one be made but with the test error? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bef25d",
   "metadata": {},
   "source": [
    "***ANSWER***\n",
    "\n",
    "The main reason for not using a similar stopping criterion based on the test error is that the test set must remain entirely independent of the training process. Introducing the test error as a stopping criterion could lead to biasing the training process toward the specific characteristics of the test set, making it less effective in evaluating the model's generalization capability to new, unseen data. To accurately assess a model's ability to generalize, it's essential to maintain the complete independence of the test set from the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d5c172",
   "metadata": {},
   "source": [
    "This way, it is possible to train an ANN so that the error or loss in the training set is minimised.   However, the training of ANNs is not deterministic, but has a random component, which is the random initialisation of the weights.  When this happens, to minimise the random component, the ANN is created and trained several times and the results are averaged.  If you want to train several ANNs, it will be necessary to nest two loops, where the outer loop will iterate through the different networks, and the inner loop will execute the different training cycles of each network.\n",
    "\n",
    "#### Question\n",
    "Where in the code (outside both loops, inside the first loop or inside the second loop) will it be necessary to put the call to the Chain function to create each network? Why shouldn't it be put elsewhere?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0520901a",
   "metadata": {},
   "source": [
    "***ANSWER***\n",
    "\n",
    "The call to the Chain function to create each network should be placed inside the first loop. This is because each iteration of the outer loop corresponds to a specific network configuration or setup. Placing it outside both loops would mean that the same network would be trained in all iterations, essentially using a single network for all experiments. Conversely, putting the Chain function inside the second loop would lead to the network changing with every cycle of the inner loop, making it challenging to compare or analyze the performance of different network configurations. By placing it inside the first loop, you ensure that the second loop trains a specific network configuration, while the outer loop allows you to train and evaluate multiple networks with different settings or architectures. This structured approach facilitates systematic experimentation and comparison of various network setups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9389a2b",
   "metadata": {},
   "source": [
    "Through this process, the weights and biases, starting from initially random values, will take on different values until one of the stop criteria is met. \n",
    "\n",
    "In this sense, it is necessary to bear in mind that the weights corresponding to the connection of inputs with a very high absolute value will take on a low absolute value; on the other hand, the weights of those connections that connect inputs with a low absolute value will take on a high absolute value.  In this way, the ANN is able to combine inputs that are in very different ranges, passing them to values of similar scale.  similar scale values. In other words, the ANN is able to \"learn\" the relationship between the different scales on which the inputs move.\n",
    "\n",
    "When dealing with classification problems, the value of loss is often not easy to interpret.  For classification problems there are different metrics that will be the focus of later sessions.  For now, to assess the goodness of output of ANN, we will only use classification accuracy, defined as the ratio of well-classified patterns (number of well-classified patterns divided by the total number of patterns). Therefore, Two different cases can be identified:\n",
    "1. When there are two classes, the ANN has a single output neuron. As described above, a sigmoidal function, which returns a value between 0 and 1, is typically used as a transfer function. By simply passing a threshold (usually 0.5), the pattern can be classified as \"positive\" or \"negative\" depending on whether the output is greater or less than the threshold respectively.\n",
    "2. When there are more than two classes, applying the softmax function will result in different values of certainty, certainty or probability of belonging to each class.  Therefore, a pattern is classified as the class with the highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084554cc",
   "metadata": {},
   "source": [
    "**Important**: This process of normalisation, although it is applied here to ANNs, is common to the rest of the Machine Learning techniques. Therefore, the code to be developed regarding normalisation will also be used in the other models.\n",
    "\n",
    "On the following paragraph the requirements of the different exercises to devop are explained with the signature of the methods that has to be developed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a324e1",
   "metadata": {},
   "source": [
    "1. Develop a function called oneHotEncoding, containing the code developed in the previous session regarding the encoding of a categorical input or output.  That is, to receive a vector of values and encode it as explained in the previous tutorial.  This function will receive two parameters, called `feature` and `classes`, both of type `AbstractArray{<:Any,1}`, that is, they are vectors containing any type of value. The first one has the values of that attribute or desired output for each pattern, and the second one has the values of the categories. This function should perform the following tasks:\n",
    "  * When the number of values in the class vector is equal to 2, the attribute vector is compared with one of the two classes by broadcasting the `==` operator to generate a vector of Boolean values.   This vector is then transformed into a two-dimensional matrix of one column and returned. To do the latter, see the reshape function.\n",
    "  * When the number of classes is greater than 2, first a matrix of boolean values is created (of type Array{Bool,2} or BitArray{2}) with as many rows as patterns and as many columns as categories (one column per category). Subsequently, iterate over each column/category, and assign the values of that column as the result of comparing the vector `feature` with the corresponding category by performing a broadcast as in the previous point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9c757c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "function oneHotEncoding(feature::AbstractArray{<:Any,1},      \n",
    "        classes::AbstractArray{<:Any,1})\n",
    "    # First we are going to set a line as defensive to check values\n",
    "    @assert(all([in(value, classes) for value in feature]));\n",
    "    \n",
    "    # Second defensive statement, check the number of classes\n",
    "    numClasses = length(classes);\n",
    "    @assert(numClasses>1)\n",
    "    \n",
    "    if (numClasses==2)\n",
    "        # Case with only two classes\n",
    "        oneHot = reshape(feature.==classes[1], :, 1);\n",
    "    else\n",
    "        #Case with more than two clases\n",
    "        oneHot =  BitArray{2}(undef, length(feature), numClasses);\n",
    "        for numClass = 1:numClasses\n",
    "            oneHot[:,numClass] .= (feature.==classes[numClass]);\n",
    "        end;\n",
    "    end;\n",
    "    return oneHot;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45faa4ff",
   "metadata": {},
   "source": [
    "2. Overload this function called oneHotEncoding in these two ways: \n",
    "  * One that receives a single parameter called `feature`, of type AbstractArray{<:Any,1}, which takes the categories and makes a call to the previous function. The unique function can be used to extract the categories. Develop this function without explicitly declaring it using the word function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ff8cb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function equivalent to \n",
    "# function oneHotEncoding(feature::AbstractArray{<:Any,1})\n",
    "\n",
    "oneHotEncoding(feature::AbstractArray{<:Any,1}) = oneHotEncoding(feature, unique(feature));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1b3f83",
   "metadata": {},
   "source": [
    "  * Another function that receives a single parameter called `feature`, of type AbstractArray{Bool,1}, and therefore it will be a vector that contains for each pattern only two possibilities. As there are two categories, the output array must have a single column, which has the same elements. A simple way to do this function is to use the reshape function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613ffa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function oneHotEncoding(feature::AbstractArray{Bool,1})\n",
    "#    return reshape(feature, (length(feature), 1))\n",
    "#end;\n",
    "\n",
    "# It is prefirable an overload of the method\n",
    "oneHotEncoding(feature::AbstractArray{Bool,1}) = reshape(feature, :, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7a436a",
   "metadata": {},
   "source": [
    "3. Develop a set of functions to normalise the data using the code developed in the previous practice.  For this, develop the following functions: \n",
    "  * Two functions called `calculateMinMaxNormalizationParameters` and `calculateZeroMeanNormalizationParameters` that receive a parameter of type `AbstractArray{<:Real,2}` and return a tuple with two values, each of them being a matrix with one row with the minimum and maximum values for each column (first function) or means and standard deviations for each column (second function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537abc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function calculateMinMaxNormalizationParameters(dataset::AbstractArray{<:Real,2})\n",
    "    return minimum(dataset, dims=1), maximum(dataset, dims=1)\n",
    "end;\n",
    "\n",
    "# Alternative more compact definition\n",
    "#calculateMinMaxNormalizationParameters(dataset::AbstractArray{<:Real,2}) = ( minimum(dataset, dims=1), maximum(dataset, dims=1) );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cc0f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "function calculateZeroMeanNormalizationParameters(dataset::AbstractArray{<:Real,2})\n",
    "    return mean(dataset, dims=1), std(dataset, dims=1)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29062af",
   "metadata": {},
   "source": [
    "  * Develop four functions to normalise between maximum and minimum. \n",
    "    * The first one, called normalizeMinMax! receives two parameters, an array of values to normalise (of type `AbstractArray{<:Real,2})` and the normalisation parameters (of type `NTuple{2, AbstractArray{<:Real,2}}`), executes the code of the previous tutorial referred to normalise between maximum and minimum, and returns the same array with the normalised data. Subsequently, make another function with the same name (overloaded) but with a single parameter that is the data matrix, and what it will do is to calculate the normalisation parameters with the function developed in the previous point and call the function normalizeMinMax! These two functions end in `!` because the array of values passed as a parameter is modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7376c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "function normalizeMinMax!(dataset::AbstractArray{<:Real,2},      \n",
    "        normalizationParameters::NTuple{2, AbstractArray{<:Real,2}})\n",
    "    minValues = normalizationParameters[1];\n",
    "    maxValues = normalizationParameters[2];\n",
    "    dataset .-= minValues;\n",
    "    dataset ./= (maxValues .- minValues);\n",
    "    # eliminate any atribute that do not add information\n",
    "    dataset[:, vec(minValues.==maxValues)] .= 0;\n",
    "    return dataset;\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c52803",
   "metadata": {},
   "outputs": [],
   "source": [
    "function normalizeMinMax!(dataset::AbstractArray{<:Real,2})\n",
    "    normalizeMinMax!(dataset , calculateMinMaxNormalizationParameters(dataset));\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63822b75",
   "metadata": {},
   "source": [
    "  Make two other functions with the name normalizeMinMax that do the same, but do not modify the data matrix, i.e. create a new one, modify it and return it.  To do this, see the copy function.  These two functions should make calls to the previous functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090c6221",
   "metadata": {},
   "outputs": [],
   "source": [
    "function normalizeMinMax( dataset::AbstractArray{<:Real,2},      \n",
    "                normalizationParameters::NTuple{2, AbstractArray{<:Real,2}}) \n",
    "    normalizeMinMax!(copy(dataset), normalizationParameters);\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa814c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "function normalizeMinMax( dataset::AbstractArray{<:Real,2})\n",
    "      normalizeMinMax!(copy(dataset), calculateMinMaxNormalizationParameters(dataset));\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25f1866",
   "metadata": {},
   "source": [
    "- Develop four similar functions for the case of performing a 0-mean normalisation, whose names are `normalizeZeroMean!` and  `normalizeZeroMean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6e6c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "function normalizeZeroMean!(dataset::AbstractArray{<:Real,2},      \n",
    "                        normalizationParameters::NTuple{2, AbstractArray{<:Real,2}}) \n",
    "    avgValues = normalizationParameters[1];\n",
    "    stdValues = normalizationParameters[2];\n",
    "    dataset .-= avgValues;\n",
    "    dataset ./= stdValues;\n",
    "    # Remove any atribute that do not have information\n",
    "    dataset[:, vec(stdValues.==0)] .= 0;\n",
    "    return dataset; \n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbe49ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "function normalizeZeroMean!(dataset::AbstractArray{<:Real,2})\n",
    "    normalizeZeroMean!(dataset , calculateZeroMeanNormalizationParameters(dataset));   \n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdf7502",
   "metadata": {},
   "outputs": [],
   "source": [
    "function normalizeZeroMean( dataset::AbstractArray{<:Real,2},      \n",
    "                            normalizationParameters::NTuple{2, AbstractArray{<:Real,2}})\n",
    "    normalizeZeroMean!(copy(dataset), normalizationParameters);\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c164721",
   "metadata": {},
   "outputs": [],
   "source": [
    "function normalizeZeroMean( dataset::AbstractArray{<:Real,2}) \n",
    "    normalizeZeroMean!(copy(dataset), calculateZeroMeanNormalizationParameters(dataset));\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50d4550",
   "metadata": {},
   "source": [
    "4. Develop a function called `classifyOutputs`, which receives a parameter called outputs of type `AbstractArray{<:Real,2}`. It will contain the outputs of a model (not necessarily for an ANN) with `a single sample/patten in each row` and convert it to an array of boolean values that each row only has a value of true, indicating the class to which that pattern is classified.  To do this, first look at the number of columns the outputs matrix has, and then do the following: \n",
    "\n",
    "   * If you have one column, compare the matrix with a threshold value by broadcasting the >= operator to generate a matrix of boolean values of a column to be returned. To do this, the function needs to receive the threshold value as an optional parameter, with a default value of 0.5. \n",
    "\n",
    "   * If you have more than one column, you must create an array of boolean values of the same size, and, for each row, set the column with a larger value to true. This can be done without writing any loops. \n",
    "\n",
    "  This aim of this exercise, together with the next one, is to develop skills in  vector programming. Below this lines are the steps that could be taken to write the code for this second scenario, when you have more than one column (assuming that each pattern is in each row):\n",
    "  \n",
    "   * First, for each row, it needs to be found out in which column is the maximum output value for each pattern. This can be done with the `findmax` function, which returns a tuple with two values: the maximum in each row or column, and the coordinates in the matrix in which that maximum was found, which is what we are really interested in. With the keyword `dims` you can indicate whether you want to search for the maximums in the rows or in the columns.  The line of code would be as follows:\n",
    "   ```julia\n",
    "        (_,indicesMaxEachInstance) = findmax(outputs, dims=2);\n",
    "    ```\n",
    "  * Once you have it, you can create a boolean array of the same dimensionality as the output array, where each value indicates the membership of the corresponding class of that pattern. This matrix is initialised to false, so it can be easily created with the function `false`. Such as:\n",
    " ```julia\n",
    "        outputs = falses(size(outputs));\n",
    "    ``` \n",
    "  * Finally, the values of the indices containing the largest values of each row, which are collected in the `indicesMaxEachInstance` variable created earlier, are set to true in this array.  This can be done by comparing with the `outputs` array as follows:\n",
    " ```julia\n",
    "        outputs[indicesMaxEachInstance] .= true;\n",
    "    ``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85e30ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "function classifyOutputs(outputs::AbstractArray{<:Real,2}; \n",
    "                        threshold::Real=0.5) \n",
    "   numOutputs = size(outputs, 2);\n",
    "    @assert(numOutputs!=2)\n",
    "    if numOutputs==1\n",
    "        return outputs.>=threshold;\n",
    "    else\n",
    "        # Look for the maximum value using the findmax funtion\n",
    "        (_,indicesMaxEachInstance) = findmax(outputs, dims=2);\n",
    "        # Set up then boolean matrix to everything false while max values aretrue.\n",
    "        outputs = falses(size(outputs));\n",
    "        outputs[indicesMaxEachInstance] .= true;\n",
    "        # Defensive check if all patterns are in a single class\n",
    "        @assert(all(sum(outputs, dims=2).==1));\n",
    "        return outputs;\n",
    "    end;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3bce71",
   "metadata": {},
   "source": [
    "5.  Develop a function called `accuracy` given a matrix of desired outputs (`targets`) and a matrix of outputs emitted by a model (not necessarily for an ANN), calculate the accuracy in a classification problem.  Both matrices should have a number of rows equal to the number of patterns, i.e. each pattern will be placed in each row. Develop this function in such a way that it works for the cases of having 2 classes (one output neuron) as well as more than two classes (one output neuron per class).\n",
    "\n",
    "    To develop this function, four functions with the same name must be carried out:\n",
    "    \n",
    "    * In the first one, `targets` and `outputs` are of type `AbstractArray{Bool,1}`, i.e. vectors of boolean values.  The precision will simply be the average value of the comparison of both vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388a808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics;\n",
    "\n",
    "function accuracy(outputs::AbstractArray{Bool,1}, targets::AbstractArray{Bool,1}) \n",
    "    mean(outputs.==targets);\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e942f1e",
   "metadata": {},
   "source": [
    "  * In the second of the functions, `targets` and `outputs` are of type `AbstractArray{Bool,2}`, i.e. two-dimensional arrays of Boolean values.  In this case, it will be necessary to examine the number of columns.  If they have only one column, a call to the above function will be made taking the first column of targets and outputs as vectors. If the number of columns is greater than 2, it will be necessary to compare both matrices to see in which rows the values do not match.\n",
    "  \n",
    "### Question\n",
    " What would happen if the number of columns is equal to 2? (put your answer into a comment in the code below)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88daf403",
   "metadata": {},
   "outputs": [],
   "source": [
    "function accuracy(outputs::AbstractArray{Bool,2}, targets::AbstractArray{Bool,2}) \n",
    "    @assert(all(size(outputs).==size(targets)));\n",
    "    if (size(targets,2)==1)\n",
    "        return accuracy(outputs[:,1], targets[:,1]);\n",
    "    else\n",
    "        return mean(all(targets .== outputs, dims=2));\n",
    "    end;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff28ee46",
   "metadata": {},
   "source": [
    "  * In the third of the functions, `targets` will be of type `AbstractArray{Bool,1}` (a vector of boolean values) while `outputs` will be of type `AbstractArray{<:Real,1}`, i.e. real outputs that have not yet been interpreted as \"positive\"/\"negative\" class membership values.  In this case, the function could accept an optional `threshold` parameter with a default value of 0.5, and what it would do would be to pass the threshold to the outputs vector, and call the first `accuracy` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2642d2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "function accuracy(outputs::AbstractArray{<:Real,1}, targets::AbstractArray{Bool,1};      \n",
    "                threshold::Real=0.5)\n",
    "    accuracy(outputs.>=threshold, targets);\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3079a0a5",
   "metadata": {},
   "source": [
    "  * Finally, in the last of the functions, `targets` will be of type `AbstractArray{Bool,2}` (an array of boolean values) while `outputs` will be of type `AbstractArray{<:Real,2}`, i.e. be real outputs that have not yet been interpreted as values belonging to N classes (N: number of columns). In this case, it is again necessary to distinguish whether you have 1 or more than 2 columns. In the first case, a call to `accuracy` should be made with the vectors corresponding to the first column of outputs and targets. In the second case, a call should be made to the function classifyOutputs to convert outputs into a variable of type AbstractArray{Bool,2}, and then make a call to `accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b032c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "function accuracy(outputs::AbstractArray{<:Real,2}, targets::AbstractArray{Bool,2};\n",
    "                threshold::Real=0.5)\n",
    "    @assert(all(size(outputs).==size(targets)));\n",
    "    if (size(targets,2)==1)\n",
    "        return accuracy(outputs[:,1], targets[:,1]);\n",
    "    else\n",
    "        return accuracy(classifyOutputs(outputs; threshold=threshold), targets);\n",
    "    end;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e728ae87",
   "metadata": {},
   "source": [
    "As in the previous exercise, due to the aim of this one is to develop the vectorial programming skills of the students, the main steps will be pointed out considering that we want ot calculate the accurry of more than 2 classes, the samples are in the rows of the matrices, and both outputs and targgets matrices are `AbstractArray{Bool, 2}`.\n",
    "\n",
    "  * Once the outputs variable has the desired form (array of Boolean values), it is compared with the targets array as follows: \n",
    "   ```julia\n",
    "        classComparison = targets .== outputs\n",
    "    ```\n",
    "  * In this new array, for each pattern, when the class matches, all elements of that row will be true. On the other hand, when the class does not match, more than one element in that row will be false. Therefore, one way to know for a pattern if it is well sorted is to look if in its row all elements are true.  This can be checked with the function `all`, which receives an array and returns true if all elements are true, but also accepts the keyword `dims`, with which it applies this same function across the given dimension. To do this on rows, you would have to do like this:\n",
    "   ```julia\n",
    "        correctClassifications = all(classComparison, dims=2)\n",
    "    ```\n",
    "  * Finally, the only thing left to do is to average this matrix. Remember that you can operate with boolean values in the same way as with real values, in this case they will be treated as 0 or 1. \n",
    "     ```julia\n",
    "        accuracy = mean(correctClassifications)\n",
    "    ```\n",
    "  * These last steps could have been performed by looking, instead of the matches between the two arrays, at the patterns where there are no matches.  For that you can use the function `any`, which receives an array of boolean values and returns true if there is any value equal to true, and also accepts the keyword `dims`. This would calculate the error rate instead of the accuracy, but the accuracy can be calculated from the error rate by simply subtracting it from 1. The code would look like:\n",
    "      ```julia\n",
    "        classComparison = targets .!= outputs \n",
    "        incorrectClassifications = any(classComparison, dims=2)\n",
    "        accuracy = 1 - mean(incorrectClassifications)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c517c72",
   "metadata": {},
   "source": [
    "6. Develop a function to create ANNs to solve classification problems.  This function must receive the `topology` (number of hidden layers and neurons in each, and optionally activation functions in each hidden layer, the number of input neurons and the number of output neurons).\n",
    "   **Important** It is worth pointing out that the transfer function of the output layer is not given by the user but by the problem itself (regression/classification).  Similarly, the number of neurons in the input and output layers is given by the problem to be solved.\n",
    "   A straightforward way to create this ANN is to receive the topology as a parameter called topology of type AbstractArray{<:Int,1}, which contains the number of neurons in each hidden layer (empty for networks without hidden layers), and create the ANN as follows:\n",
    "   * Create an empty ANN\n",
    "   ```julia\n",
    "            ann = Chain();\n",
    "   ```\n",
    "   * Create a variable called numInputsLayer, initialized to the number of inputs of the ANN.\n",
    "   * If there are hidden layers, i.e. if the `topology` vector is not empty, iterate through this vector (the value of the loop will be equal to the number of neurons in each layer) and in each iteration create a hidden layer where the number of inputs will be equal to the value of the numInputsLayer variable and the number of outputs equal to the current value of the loop. If the transfer function of each hidden layer has not been specified, use the same transfer function σ in all hidden layers. After this, update the value of numInputsLayer to the value used in that iteration.\n",
    "   ```julia\n",
    "            for numOutputsLayer = topology      \n",
    "                ann = Chain(ann..., Dense(numInputsLayer, numOutputsLayer, σ) );      \n",
    "                numInputsLayer = numOutputsLayer; \n",
    "            end;\n",
    "   ```\n",
    "   If these lines are written in the script without being inside a loop or function, compiling the code will give several warnings and the code will not properly work. This is automatically corrected by using this code inside a function.  To use it in the main body, you should write `global ann, numInputsLayer;` at the beginning of the loop.\n",
    "   * Finally, add the final layer, with the number of neurons and transfer function appropriate to the number of classes as described above, adding the `softmax` function if there are more than two output classes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686ad280",
   "metadata": {},
   "outputs": [],
   "source": [
    "function buildClassANN(numInputs::Int, topology::AbstractArray{<:Int,1}, numOutputs::Int;\n",
    "                    transferFunctions::AbstractArray{<:Function,1}=fill(σ, length(topology))) \n",
    "    ann=Chain();\n",
    "    numInputsLayer = numInputs;\n",
    "    for numHiddenLayer in 1:length(topology)\n",
    "        numNeurons = topology[numHiddenLayer];\n",
    "        ann = Chain(ann..., Dense(numInputsLayer, numNeurons, transferFunctions[numHiddenLayer]));\n",
    "        numInputsLayer = numNeurons;\n",
    "    end;\n",
    "    if (numOutputs == 1)\n",
    "        ann = Chain(ann..., Dense(numInputsLayer, 1, σ));\n",
    "    else\n",
    "        ann = Chain(ann..., Dense(numInputsLayer, numOutputs, identity));\n",
    "        ann = Chain(ann..., softmax);\n",
    "    end;\n",
    "    return ann;\n",
    "end;                                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d4e6a2",
   "metadata": {},
   "source": [
    "7. Develop a function that creates an ANN to perform classification (via a call to the above function) and trains it.   To do this, this function should implement a loop in which the ANN is trained with the training set passed as a parameter until one of the stop criteria passed as parameters is met.  The function should return the trained ANN.  The argumentes shopuld be:\n",
    "  * **topology**, of type `AbstractArray{<:Int,1}` with the topology (hidden layers) of the ANN. \n",
    "  * **dataset**, of type `Tuple{AbstractArray{<:Real,2},  AbstractArray{Bool,2}} with the  matrix of inputs and targets.  From these matrices, the number of input and output neurons necessary to call the previous function can be obtained by means of the size function.\n",
    "  * Optional parameters controlling other aspects of the algorithm such as the stopping criteria of the algorithm, with default values:\n",
    "    * **maxEpochs**, of type Int, with a default value of 1000\n",
    "    * **minLoss**, of type Real, with a default value of 0\n",
    "    * **learningRate**, of type Real, with a default value of 0.01.\n",
    "    \n",
    "  It is important to highlight that the set of patterns used here will have each pattern in a row, while the Flux library (in general, most Genetic Algorithms libraries do) expect matrices in which each pattern is in a column.  This is not a problem, matrices only need to be transposed on certain occasions, such as when supplying the function to train an ANN cycle, when calculating the loss value, or when taking the output matrices obtained by running the ANN.\n",
    "  \n",
    "  Bear in mind that a call to the train! function trains the ANN for one loop only. Therefore, it is necessary, first, to create the ANN by calling the above function, and then to run a loop that calls the `train!` function only once in each iteration.  That is, if you want to train for n cycles, this loop will have to perform `n` iterations.\n",
    "  \n",
    "  The output of this function should be at least the trained ANN and a vector with the loss values for each training cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed65c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function trainClassANN(topology::AbstractArray{<:Int,1},      \n",
    "                    dataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,2}};\n",
    "                    transferFunctions::AbstractArray{<:Function,1}=fill(σ, length(topology)),\n",
    "                    maxEpochs::Int=1000, minLoss::Real=0.0, learningRate::Real=0.01) \n",
    "\n",
    "    (inputs, targets) = dataset;\n",
    "    \n",
    "    # This function assumes that each sumple is in a row\n",
    "    # we are going to check the numeber of samples to have same inputs and targets\n",
    "    @assert(size(inputs,1)==size(targets,1));\n",
    "\n",
    "    # We define the ANN\n",
    "    ann = buildClassANN(size(inputs,2), topology, size(targets,2));\n",
    "\n",
    "    # Setting up the loss funtion to reduce the error\n",
    "    loss(model,x,y) = (size(y,1) == 1) ? Losses.binarycrossentropy(model(x),y) : Losses.crossentropy(model(x),y);\n",
    "\n",
    "    # This vectos is going to contain the losses and precission on each training epoch\n",
    "    trainingLosses = Float32[];\n",
    "\n",
    "    # Inicialize the counter to 0\n",
    "    numEpoch = 0;\n",
    "    # Calcualte the loss without training\n",
    "    trainingLoss = loss(ann, inputs', targets');\n",
    "    #  Store this one for checking the evolution.\n",
    "    push!(trainingLosses, trainingLoss);\n",
    "    #  and give some feedback on the screen\n",
    "    println(\"Epoch \", numEpoch, \": loss: \", trainingLoss);\n",
    "\n",
    "    # Define the optimazer for the network\n",
    "    opt_state = Flux.setup(Adam(learningRate), ann);\n",
    "\n",
    "    # Start the training until it reaches one of the stop critteria\n",
    "    while (numEpoch<maxEpochs) && (trainingLoss>minLoss)\n",
    "\n",
    "        # For each epoch, we habve to train and consequently traspose the pattern to have then in columns\n",
    "        Flux.train!(loss, ann, [(inputs', targets')], opt_state);\n",
    "\n",
    "        numEpoch += 1;\n",
    "        # calculate the loss for this epoch\n",
    "        trainingLoss = loss(ann, inputs', targets');\n",
    "        # store it\n",
    "        push!(trainingLosses, trainingLoss);\n",
    "        # shown it\n",
    "        println(\"Epoch \", numEpoch, \": loss: \", trainingLoss);\n",
    "\n",
    "    end;\n",
    "\n",
    "    # return the network and the evolution of the error\n",
    "    return (ann, trainingLosses);\n",
    "end;                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867622f0",
   "metadata": {},
   "source": [
    "8. Occasionally, when the classification problem is of two classes, instead of having the desired outputs as a single column matrix, it will be as a vector.  To deal with these cases, a function of the same name as above and accepting the same arguments is requested, except that the second argument, dataset, is of type `Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,1}}`. This function should only convert the desired output vector into an array with one column and call the above function.  To do this conversion, see the reshape function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7b93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "function trainClassANN(topology::AbstractArray{<:Int,1},      \n",
    "                    (inputs, targets)::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,1}};      \n",
    "                    transferFunctions::AbstractArray{<:Function,1}=fill(σ, length(topology)),      \n",
    "                    maxEpochs::Int=1000, minLoss::Real=0.0, learningRate::Real=0.01)\n",
    "    \n",
    "     trainClassANN(topology, (inputs, reshape(targets, length(targets), 1)); \n",
    "        maxEpochs=maxEpochs, minLoss=minLoss, learningRate=learningRate);\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ffa64",
   "metadata": {},
   "source": [
    "#### Warning\n",
    "Remember to integrate this functions with the code developed on the previous tutorial in a separate file.\n",
    "\n",
    "Once the training function returns a trained ANN, it can be used to simulated on different problems by passing it a set of inputs (with the patterns in columns, i.e. transposed) and will return the outputs for that set (again, the patterns will be in columns, so the outputs of the ANN will have to be transposed).  These outputs, along with the desired outputs for that set of inputs, can be applied to the accuracy function to calculate the accuracy on that set. \n",
    "\n",
    "Run multiple times to train different networks with different architectures - which one will give the highest accuracy on the training set? Also test different learning rate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2977e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "using DelimitedFiles;\n",
    "using Flux;\n",
    "\n",
    "#Define the parameters\n",
    "topology = [4, 3]; \n",
    "learningRate = 0.01;\n",
    "numMaxEpochs = 1000; \n",
    "\n",
    "# Load the dataset\n",
    "dataset = readdlm(\"iris.data\",',');\n",
    "# Prepare the data\n",
    "inputs = convert(Array{Float32,2}, dataset[:,1:4]);\n",
    "targets = oneHotEncoding(dataset[:,5]);\n",
    "\n",
    "# Normalization \n",
    "newInputs = normalizeMinMax(inputs);\n",
    "@assert(all(minimum(newInputs, dims=1) .== 0));\n",
    "@assert(all(maximum(newInputs, dims=1) .== 1));\n",
    "# Standarization. Beware of the error done for using the meand and std they are not going to hit perfect 0 or 1\n",
    "#newInputs = normalizeZeroMean(inputs);\n",
    "#@assert(all(abs.(mean(newInputs, dims=1))    .<= 1e-4));\n",
    "#@assert(all(abs.(std( newInputs, dims=1)).-1 .<= 1e-4));\n",
    "\n",
    "# In this example, we use the MAX-MIN normalization \n",
    "normalizeMinMax!(inputs);\n",
    "\n",
    "(ann, trainingLosses) = trainClassANN(topology, (inputs, targets); maxEpochs=numMaxEpochs, learningRate=learningRate);\n",
    "outputs = ann(inputs')';\n",
    "trainingAccuracy = accuracy(outputs, targets);\n",
    "\n",
    "println(\"Accuracy: \", 100*trainingAccuracy, \" %\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a675f36",
   "metadata": {},
   "source": [
    "***ANSWER***\n",
    "\n",
    "repeat above code several times in order to perform an study with at least 3 architectures and 3 learning rates, this can be done with a functiona an a couple of loops in a very easy way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8659529c",
   "metadata": {},
   "source": [
    "Repeat the experiments performed above, with the unstandardised data, and compare the results with the standardised data. Are there important differences when using standardised data? Which one is the best topology?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f2d622",
   "metadata": {},
   "source": [
    "***ANSWER***\n",
    "\n",
    "\n",
    "same code execute multiple times but without normalization the instructionn to normalize\n",
    "\n",
    "This test should get a poorer performance than the normalized one due to the difference in the scales which makes the them generalize less than the normalized counterpart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b1a6e1",
   "metadata": {},
   "source": [
    "# Julia Notes\n",
    "\n",
    "As any other language, Julia allows the creation of functions. There are several ways to create functions, the most common are these two:\n",
    "\n",
    "1. When the operation to be performed is simple, the function can be declared on one line without the need for the reserved word function.  This is the case for operations that can be performed in one or a few lines of code.  In the latter case, parentheses can be used to enclose the actions to be performed, and `;` to separate them.  The value to be returned by the function will be the last thing to be evaluated, although the reserved word `return` can also be used.  Here are a couple of examples of this way of declaring functions:\n",
    "    ```julia\n",
    "         add(x::Float32, y::Float32) = x+x; \n",
    "         mse(outputs::Array{Float32,1}, targets:: Array{Float32,1}) = mean((targets.-outputs).^2);\n",
    "         avgGreaterThan0(valores::Array{Float32,1}) = mean(values[values.>0]); \n",
    "         avgGreaterThan0(valores::Array{Float32,1}) = ( positives=values.>0; mean(values[positives]); )   \n",
    "    ```\n",
    "2. In many other cases, a function may perform many complex operations that are impractical to write on a single line.  In this case, you can declare the function with the reserved word `function`, and return the result with the reserved word `return`.  As is common in programming languages, evaluating `return` immediately exits the function.  If `return` is not used, the result returned by function will be the last one evaluated.  The example above is shown below: \n",
    "   ```julia\n",
    "         function avgGreaterThan0(Values::Array{Float32,1})    \n",
    "            positives=values.>0;    \n",
    "            return mean(Values[positives]); \n",
    "         end;  \n",
    "    ```\n",
    "  When passing parameters, it is not mandatory, but recommended, to indicate the type of the parameters.  If this is not done, they are assumed to be of type `Any`.  However, a common practice in Julia is to overload functions, i.e. to define functions with the same name but different parameters or of different types.  When calling a function, the correct function will be executed, if any of the ones defined match the parameters passed. This allows different behaviours to be defined. When a function call is made, Julia checks that there exists in memory a definition with that name in which the types of the passed arguments match the defined ones and, if it exists, it will execute it with those parameters.\n",
    "  \n",
    "  In the examples above, the parameters have been defined as `Float32` or `Array{Float32,2}`, which means that the functions are defined for those specific types, and therefore, if calls are made with parameters of type `Float64` or `Array{Float64,2}`, a suitable function definition will not be found, and an error will be raised.  To solve this, it is worthy to apply the subtyping properties as seen in the previous tutorial to define the most generic types that the function can work with. For example, instead of using `Float32`, you can use `AbstractFloat`, `Real` or `Number` types (be aware that the latter type includes complex numbers). Instead of using `Array{Float32,2}`, you could use `Array{<:AbstractFloat}`, `Array{<:Real}` or `Array{<:Number}`. Additionaly, it is needed to take into remember that there are other types that behave like arrays but are not arrays. An important example that will be used in this subject is when transposing arrays, which creates an object that is not of type `Array`, but of type `LinearAlgebra.Adjoint`, but that can be used as if it were an array because its operations are defined as such.  Both `Array` and `LinearAlgebra.Adjoint` are subtypes of `AbstractArray`, as can be seen in the following example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a04b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = [1. 2.; 3. 4.]; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d3ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "isa(m, Array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa17fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "isa(m, AbstractArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7a1ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(m') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7249074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "isa(m', Array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a7cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "isa(m', AbstractArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43600581",
   "metadata": {},
   "source": [
    "Since both types, `Array` and `LinearAlgebra.Adjoint` are subtypes of `AbstractArray`, to allow an argument to be of one type or the other, this will be the type to be used in the function arguments. Therefore, the above functions will be as follows: \n",
    "\n",
    "   ```julia\n",
    "        add(x::Real, y:: Real) = x+x;\n",
    "        mse(outputs::AbstractArray{<:Real,1},targets::AbstractArray{<:Real,1})=mean((targets.-outputs).^2)\n",
    "        avgGreaterThan0(values::AbstractArray{<:Real,1}) = mean(values[values.>0]); \n",
    "        avgGreaterThan0(values:: AbstractArray{<:Real,1})=(positives=values.>0; mean(values[poitives]);) \n",
    "        \n",
    "        function avgGreaterThan0(values::AbstractArray{<:Real,1})    \n",
    "            positives=values.>0;    \n",
    "            return mean(Values[positives]); \n",
    "        end;  \n",
    "   ```\n",
    "When an argument is a vector or array of boolean values, as is the case of the desired outputs in a classification problem, the type to use will be `AbstractArray{Bool,N}`, where N is the dimensionality of the array.  As mentioned in the previous tutorial, this type is a supertype of both `Array{Bool,N}` and `BitArray{N}`. \n",
    "\n",
    "An interesting feature of Julia functions is that they can return more than one value. This is done by using the type `Tuple{...}`, which designates tuples where each element has a certain type, for example `Tuple{Float32,Float32}`, `Tuple{Array{Float32,2}, Int64}` or `Tuple{Float32, Tuple{Int64, Int4}}`. In general, if you want to return more than one element, you return a tuple with the elements you want to return, for example:\n",
    "\n",
    "   ```julia\n",
    "        function avgGreaterThan0(values::AbstractArray{<:Real,1})    \n",
    "            positives=valores.>0;    \n",
    "            return (positives, mean(values[positives])) \n",
    "        end; \n",
    "\n",
    "        (positives, average) = avgGreaterThan0([1.2 , -1.3 , 5.5 , -3.8 , -2.1])\n",
    "   ```\n",
    "\n",
    "When creating tuples where all elements have the same type, a simplified way to define the type is to use `NTuple`, specifying the number of elements and the type. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e6a43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tuple{Float64,Float64}==NTuple{2,Float64}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8072b588",
   "metadata": {},
   "source": [
    "As was aforementioned, the names of those functions that modify the values of one of their arguments are usually terminated with `!` by convention."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
